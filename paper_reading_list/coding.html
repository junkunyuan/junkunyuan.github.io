
<!DOCTYPE html>
<html>
<head>
<link rel="stylesheet" href="resource/html.css" type="text/css">
<link rel="shortcut icon" href="resource/my_photo.jpg">
<title>Paper Reading List</title>
<meta name="description" content="Paper Reading List">
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<div id="layout-content" style="margin-top:25px">
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.24.1/prism.min.js"></script>
            <link href="https://cdn.jsdelivr.net/npm/prismjs@1.24.1/themes/prism.css" rel="stylesheet">
            <script src="https://cdn.jsdelivr.net/npm/prismjs@1.24.1/components/prism-bash.min.js"></script>
            <script src="https://cdn.jsdelivr.net/npm/prismjs@1.24.1/components/prism-python.min.js"></script>
            <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
            <body>

<h1 id="top">Coding and Engineering</h1>
<p class="larger"><b>Tools used to build AI systems.</b></p>
<p class="larger"><b></b></p>
<p>Curated by <a href="https://junkunyuan.github.io/">Junkun Yuan</a>.</p>
<p>Click <a href="paper_reading_list.html">here</a> to go back to main contents.</p>
<p><font color=#B0B0B0>Last updated on August 11, 2025 at 23:38 (UTC+8).</font></p>
<hr><p id='table' class="huger"><b>Table of contents:</b></p><ul><li><a class="no_dec larger" id="PyTorch" href="#PyTorch-table"><b>PyTorch</b></a></li><p><a class="no_dec" href="#data transformspytorch"><font color=#404040>data transforms</font></a> <font color=#B0B0B0>&nbsp;|&nbsp;</font> <a class="no_dec" href="#data loaderpytorch"><font color=#404040>data loader</font></a> <font color=#B0B0B0>&nbsp;|&nbsp;</font> <a class="no_dec" href="#operationpytorch"><font color=#404040>operation</font></a> <font color=#B0B0B0>&nbsp;|&nbsp;</font> <a class="no_dec" href="#modulepytorch"><font color=#404040>module</font></a> <font color=#B0B0B0>&nbsp;|&nbsp;</font> <a class="no_dec" href="#activation functionpytorch"><font color=#404040>activation function</font></a> <font color=#B0B0B0>&nbsp;|&nbsp;</font> <a class="no_dec" href="#optimizerpytorch"><font color=#404040>optimizer</font></a> <font color=#B0B0B0>&nbsp;|&nbsp;</font> <a class="no_dec" href="#huggingfacepytorch"><font color=#404040>huggingface</font></a></p><li><a class="no_dec larger" id="Distributed Training" href="#Distributed Training-table"><b>Distributed Training</b></a></li><p><a class="no_dec" href="#deepspeeddistributed training"><font color=#404040>deepspeed</font></a></p></ul><h2 id="PyTorch-table"><a class="no_dec" href="#PyTorch">PyTorch</a></h2>
            <p class="little_split" id='data transformspytorch'></p>
            <div style="border-left: 14px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('data transforms-PyTorch-details')"><i>Data Transforms</i></p>
            
            
            <p class="paper_detail"><b> <font color=#404040>data transforms</font></b>  &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://docs.pytorch.org/vision/stable/transforms.html">docs</a> &nbsp; <font color=#D0D0D0></font></p>
            
            
            <div id='data transforms-PyTorch-details' class="info_detail">
                <hr class="dashed">
                <p><font color=#202020>It includes tools to transform and augment data.</font></p>
                
                <p>
<table class="center">
<tr>
<th>category</th>
<th>class / function (alphabetical)</th>
</tr>

<tr><td>geometry</td><td>
<a href="#RandomHorizontalFlip">RandomHorizontalFlip</a>
</td></tr>

<tr><td>resizing</td><td>
<a href="#Resize">Resize</a>
</td></tr>

<tr><td>conversion</td><td>
<a href="#ToTensor">ToTensor</a> &nbsp;&nbsp;
<a href="#Compose">Compose</a> &nbsp;&nbsp;
<a href="#Normalize">Normalize</a>
</td></tr>

</table>
<pre>
<code class="language-python" style="font-size: 14px;">
from torchvision import transforms
from torchvision.transforms.InterpolationMode import BILINEAR, NEAREST, BICUBIC 
</code>
</pre>

<p class="larger" id="RandomHorizontalFlip">
<b><a href="https://docs.pytorch.org/vision/stable/generated/torchvision.transforms.Resize.html">RandomHorizontalFlip</a>: </b>
horizontally flip the image randomly with a given probability. 
<pre>
<code class="language-python" style="font-size: 14px;">
p = 0.5  # *** float. Probability to flip image
trans = <b>transforms.RandomHorizontalFlip</b>(p)
image_trans = trans(image)  # PIL Image => PIL Image or Tensor => Tensor
</code>
</pre>

<p class="larger" id="Resize">
<b><a href="https://docs.pytorch.org/vision/stable/generated/torchvision.transforms.Resize.html">Resize</a>: </b>
resize the image to the given size.
<pre>
<code class="language-python" style="font-size: 14px;">
size = /  # *** sequence or int. For example (512, 768)
interpolation = BILINEAR  # InterpolationMode
max_size = None  # int. Maximum allowed for the longer edge, supported if `size` is int
antialias = True  # bool. Apply antialiasing, only under bilinear or bicubic modes
trans = transforms.Resize(size, interpolation, max_size, antialias)
image_trans = trans(image)  # PIL Image => PIL Image or Tensor => Tensor
</code>
</pre>

<p class="larger" id="ToTensor">
<b><a href="https://docs.pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html">ToTensor</a>: </b>
convert a PIL Image or ndarray to tensor and scale the values accordingly.
<pre>
<code class="language-python" style="font-size: 14px;">
## Input: PIL Image / numpy.ndarray (np.uint8) of shape (HxWxC) in the range [0, 255]
## Output: torch.FloatTensor of shape (CxHxW) in the range (0.0, 1.0)
## Other inputs: only apply type transform
trans = <b>transforms.ToTensor</b>()
image_trans = trans(image)  # PIL Image / ndarray => Tensor
</code>
</pre>

<p class="larger" id="Compose">
<b><a href="https://docs.pytorch.org/vision/stable/generated/torchvision.transforms.Compose.html">Compose</a>: </b>
compose several transforms.
<pre>
<code class="language-python" style="font-size: 14px;">
transforms = /  # *** list of Transform objects
trans = <b>transforms.Compose</b>(transforms)
image_trans = trans(image)  # PIL Image / ndarray / Tensor => Tensor
</code>
</pre>

<p class="larger" id="Normalize">
<b><a href="https://docs.pytorch.org/vision/stable/generated/torchvision.transforms.Normalize.html">Normalize</a>: </b>
normalize a tensor image with mean and standard deviation.
<pre>
<code class="language-python" style="font-size: 14px;">
mean = /  # *** sequence. Means for each channel
std = /  # *** sequence. Standard deviations for each channel
inplace = False  # bool. Bool to make this operation in-place
trans = <b>transforms.Normalize</b>(mean, std, inplace)
image_trans = trans(image)  # Tensor => Tensor
</code>
</pre>
</p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split" id='data loaderpytorch'></p>
            <div style="border-left: 14px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('data loader-PyTorch-details')"><i>Data Loader</i></p>
            
            
            <p class="paper_detail"><b> <font color=#404040>data loader</font></b>  &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader">docs</a> &nbsp; <font color=#D0D0D0></font></p>
            
            
            <div id='data loader-PyTorch-details' class="info_detail">
                <hr class="dashed">
                <p><font color=#202020>It includes tools for data loading: <a href="https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader">DataLoader</a>.</font></p>
                
                <p>
<pre>
<code class="language-python" style="font-size: 14px;">
import torch
from torch.utils.data import DataLoader, Dataset, Sampler

## --------------------------------------------------------------------------------
## DataLoader
## --------------------------------------------------------------------------------
dataset = /  # *** Dataset
batch_size = 1  # *** int. Number of samples per batch.
shuffle = False  # *** bool. If True, have the data shuffled at every epoch
sampler = None  # Sampler or Iterable. Define how to draw samples
num_workers = 0  # *** int. Number of subprocesses to use for data loading
collate_fn = None  # Callable. Merge a list of samples to form a batch of tensors
pin_memory = False  # *** bool. If True, copy Tensors into CUDA pinned memory
drop_last = False  # *** bool. If True, drop the last incomplete batch
timeout = 0  # numeric. If positive, set timeout for collecting a batch from workers
prefetch_factor = None  # int. Default = None if num_workers == 0 else 2
# ...
data_loader = DataLoader(
    dataset, batch_size, shuffle, sampler, num_workers, collate_fn, 
    pin_memory, drop_last, timeout, prefetch_factor
)
## --------------------------------------------------------------------------------
</code>
</pre>
</p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split" id='operationpytorch'></p>
            <div style="border-left: 14px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('operation-PyTorch-details')"><i>Operation</i></p>
            
            
            <p class="paper_detail"><b> <font color=#404040>operation</font></b>  &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://docs.pytorch.org/docs/stable/index.html">docs</a> &nbsp; <font color=#D0D0D0></font></p>
            
            
            <div id='operation-PyTorch-details' class="info_detail">
                <hr class="dashed">
                <p><font color=#202020>It includes operations: 
<a href="#operations">operations</a>,
<a href="#data generation">data generation</a>, 
<a href="#size & reshape">size & reshape</a>.</font></p>
                
                <p>
<table class="center">
<tr>
<th>category</th>
<th>class / function (alphabetical)</th>
</tr>

<tr>
<td>operations</td>
<td>
<a href="#basic operations">basic operations</a> &nbsp;&nbsp;
<a href="#einsum">einsum</a> &nbsp;&nbsp;
<a href="#isclose & allclose">isclose & allclose</a> &nbsp;&nbsp;
<a href="#matmul">matmul</a> &nbsp;&nbsp;
<a href="#mean & var">mean & var</a> &nbsp;&nbsp;
<a href="#softmax">softmax</a> &nbsp;&nbsp;
</td>
</tr>

</table>

<pre>
<code class="language-python" style="font-size: 14px;">
import torch
</code>
</pre>

<p class="larger" id="basic operations">
<b><a href="https://docs.pytorch.org/docs/stable/torch.html">basic operations</a>:</b> 
exp, sin, cos, sqrt.
<pre>
<code class="language-python" style="font-size: 14px;">
y = torch.function(x)  # function: exp, sin, cos, sqrt
</code>
</pre>

<p class="larger" id="mean & var">
<b><a href="https://docs.pytorch.org/docs/stable/mean.html">mean & var</a></b>. 
<pre>
<code class="language-python" style="font-size: 14px;">
dim = /  # *** int or tuple of ints. The dims to reduce
keepdim = False # *** bool. If True, return tensor with the same dims
mean = x.mean(dim, keepdim)

## In version>=2.0, `correction=1` equals to `unbiased=True`, `correction=0` equals to `unbiased=False` 
correction = 1  # *** int. 
var = x.var(dim, keepdim, correction)
</code>
</pre>

<p class="larger" id="softmax">
<b><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html">softmax</a></b>. 
<pre>
<code class="language-python" style="font-size: 14px;">
dim = None  # *** int. The dimension to apply softmax
y = x.softmax(dim)
</code>
</pre>

<p class="larger" id="matmul">
<b><a href="https://docs.pytorch.org/docs/stable/generated/torch.matmul.html">matmul</a>:</b> 
matrix multiplication.
<pre>
<code class="language-python" style="font-size: 14px;">
other = /  # *** tensor
y = x.matmul(other)
</code>
</pre>

<p class="larger" id="einsum">
<b><a href="https://docs.pytorch.org/docs/stable/generated/torch.einsum.html">einsum</a>:</b> 
Einstein summation convention. 
<pre>
<code class="language-python" style="font-size: 14px;">
equation = /  # *** str. The subscript for the Einstein summation
operands = /  # *** list of tensor. The tensor to be computed
## torch.einsum("ii", tensor)  # trace
## torch.einsum("ii->i", tensor)  # diagonal
## torch.einsum("i,j->ij", tensor1, tensor2)  # outer product
## torch.einsum("bij,bjk->bik", tensor1, tensor2)  # batch matrix multiplication
## torch.einsum("...ij->...jk", tensor)  # batch permute
y = torch.einsum(equation, operands)
</code>
</pre>

<p class="larger" id="isclose & allclose">
<b><a href="https://docs.pytorch.org/docs/stable/generated/torch.isclose.html">isclose & allclose</a>:</b> 
check whether two tensors are close. 
<pre>
<code class="language-python" style="font-size: 14px;">
other = /  # *** tensor. The second tensor to compare
rtol = 1e-5  # float. Relative tolerance
atol = 1e-8  # float. Absolute tolerance
equal_nan = False  # bool. If True, then two NaN will be considered equal
## Check if elements satisfy: |input - other| <= atol + rtol * other
x.isclose(other, rtol, atol, equal_nan)  # return a tensor of bool
x.allclose(other, rtol, atol, equal_nan)  # return True or False
## --------------------------------------------------------------------------------
</code>
</pre>

<p class="larger" id="data generation"><b>Data generation:</b> 
<a href="https://docs.pytorch.org/docs/stable/generated/torch.zeros.html">zeros</a>, 
<a href="https://docs.pytorch.org/docs/stable/generated/torch.ones.html">ones</a>, 
<a href="https://docs.pytorch.org/docs/stable/generated/torch.rand.html">uniform distribution</a>,
<a href="https://docs.pytorch.org/docs/stable/generated/torch.randn.html">normal distribution</a>,
<a href="https://docs.pytorch.org/docs/stable/generated/torch.arange.html">arange</a>.</p>
<pre>
<code class="language-python" style="font-size: 14px;">
## --------------------------------------------------------------------------------
## Data generation
## --------------------------------------------------------------------------------
## Zeros & Ones
size = /  # *** sequence of int. The shape of output
y = torch.ones(size)
y = torch.zeros(size)

## Uniform & normal distribution
size = /  # *** sequence of int. The shape of output
generator = None  # torch.Generator. A pseudorandom number generator for sampling
requires_grad = False  # bool. If use autograd
dtype = None  # torch.dtype. The desired data type
device = None  # torch.device. The desired device 
y = torch.rand(size, generator, requires_grad, dytpe, device)  # uniform distribution U(0, 1)
y = torch.randn(size, generator, requires_grad, dytpe, device)  # standard normal distribution N(0, 1)

## Arange
start = 0  # *** number. The starting value
end = /  # *** number. The ending value
step = 1  # *** number. The gap between adjacent points
arange = torch.arange(start, end, step)
## --------------------------------------------------------------------------------
</code>
</pre>

<p class="larger" id="size & reshape"><b>Resize & reshape:</b> 
<a href="https://docs.pytorch.org/docs/stable/generated/torch.Tensor.size.html">size</a>,
<a href="https://docs.pytorch.org/docs/stable/generated/torch.reshape.html">reshape & view</a>,
<a href="https://docs.pytorch.org/docs/stable/generated/torch.flatten.html">flatten</a>,
<a href="https://docs.pytorch.org/docs/stable/generated/torch.tranpose.html">tranpose</a>,
<a href="https://docs.pytorch.org/docs/stable/generated/torch.permute.html">permute</a>,
<a href="https://docs.pytorch.org/docs/stable/generated/torch.permute.html">permute</a>,
<a href="https://docs.pytorch.org/docs/stable/generated/torch.unsqueeze.html">unsqueeze</a>,
<a href="https://docs.pytorch.org/docs/stable/generated/torch.cat.html">cat</a>,
<a href="https://docs.pytorch.org/docs/stable/generated/torch.squeeze.html">squeeze</a>,
<a href="https://docs.pytorch.org/docs/stable/generated/torch.unbind.html">unbind</a>,
<a href="https://docs.pytorch.org/docs/stable/generated/torch.chunk.html">chunk</a>,
<a href="https://docs.pytorch.org/docs/stable/generated/torch.split.html">split</a>,
<a href="https://docs.pytorch.org/docs/stable/generated/torch.where.html">where</a>.</p>

<pre>
<code class="language-python" style="font-size: 14px;">
## --------------------------------------------------------------------------------
## Size & reshape
## --------------------------------------------------------------------------------
## Get size
dim = None  # int. The dimension to retrieve the size
size = x.size(dim)  # => torch.Size or int
size = x.shape  #  => torch.Size

## Reshape & View
shape = /  # sequence of int. The new shape. Note: a single dimension could be -1
y = x.reshape(shape)  # recommend since it could call .contiguous() if needed
y = x.view(shape)

## Flatten: flatten along the given dimensions
start_dim = 0  # *** int. The first dimension to flatten
end_dim = -1  # *** int. The last dimension to flatten
y = x.flatten(start_dim, end_dim)

## Transpose: swap two dimensions
dim0 = /  # *** int. The first dimension to be tranposed
dim1 = /  # *** int. The second dimension to be tranposed
y = x.tranpose(dim0, dim1)

## Permute
dims = /  # *** sequence of int. The desired ordering of dims
y = x.permute(dims)

## Unsqueeze: insert one dimension
dim = /  # *** int. The index at which to insert the singleton dim
y = x.unsqueeze(dim)  # Eqaul to y = x[:, :, None, :] when dim = 3

## Concat: concatenate some tensors along a dimension
tensors = /  # *** tuple of tensors. Tensors with the same shape except in the cat dim
dim = 0  # *** int. The concatenation dim
y = torch.cat(tensors, dim)

## Squeeze: remove all dim with size 1
dim = None  # *** int or tuple of ints. If given, only the dim will be squeezed
y = x.squeeze(dim)

## Unbind
dim = 0  # *** int. Dim to remove
y = x.unbind(0)

## Chunk: split a tensor into the spicific number of chunks
chunks = /  # *** int
dim = 0  # *** int
## If the given dim is divisible by chunks, all returned chunks will be the same size
## If the given dim is not divisible by chunks, the last one will not be the same size
## If such division is not possible, it returns fewer than the specified number of chunks
y = x.chunk(chunks, dim)

## Split
indices_or_sections = /  # *** tensor, int, list, tuple of ints
dim = 0  # *** int. Dimension along which to split the tensor
## If split_size_or_sections is an integer type, split into equally sized chunks
## If split_size_or_sections is a list, split into len(split_size_or_sections) chunks
y = x.split(indices_or_sections, dim)

## Where: select elements
condition = /  # *** bool. When True, yield input, otherwise yield other
input = /  # *** tensor or scalar
other = /  # *** tensor or scalar
y = torch.where(condition, input, output)
## --------------------------------------------------------------------------------
</code>
</pre>
</p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split" id='modulepytorch'></p>
            <div style="border-left: 14px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('module-PyTorch-details')"><i>Module</i></p>
            
            
            <p class="paper_detail"><b> <font color=#404040>module</font></b>  &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://docs.pytorch.org/docs/stable/nn.html">docs</a> &nbsp; <font color=#D0D0D0></font></p>
            
            
            <div id='module-PyTorch-details' class="info_detail">
                <hr class="dashed">
                <p><font color=#202020>
It includes tools to build neural networks: 
<a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html">Parameter</a>,
<a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.parameter.Buffer.html">Buffer</a>,
<a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html">Linear</a>,
<a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#conv2d">Conv2d</a>, 
<a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv3d.html#conv3d">Conv3d</a>,
<a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Dropout.html">Dropout</a>.
</font></p>
                
                <p>
<pre>
<code class="language-python" style="font-size: 14px;">
import torch
from torch import nn 

## --------------------------------------------------------------------------------
## Parameter
## --------------------------------------------------------------------------------
data = /  # *** tensor. Parameter tensor
requires_grad = True
gamma = torch.Parameter(data, requires_grad)
## --------------------------------------------------------------------------------

## --------------------------------------------------------------------------------
## Buffer
## --------------------------------------------------------------------------------
data = /  # *** tensor. Buffer tensor
persistent = True  # whether the buffer is part of the module's state_dict
gamma = self.register_buffer(data, persistent)  # used in the module class
gamma = nn.paramter.Buffer(data, persistent)  # not usually used
## --------------------------------------------------------------------------------

## --------------------------------------------------------------------------------
## Linear
## --------------------------------------------------------------------------------
in_features = /  # *** int. Size of each input sample
out_features = /  # *** int. Size of each output sample
bias = True  # *** bool. If set to False, the layer will not learn an additive bias
device = None  # torch.device or int 
dtype = None  # torch.dtype
## [..., H_in] => [..., H_out]
linear = Linear(in_features, out_features, bias, device, dtype)
## --------------------------------------------------------------------------------

## --------------------------------------------------------------------------------
## Conv2d
## --------------------------------------------------------------------------------
in_channels = /  # *** int. Number of channels in the input
out_channels = /  # *** int. Number of channels in the output
kernel_size = /  # *** int, tuple. Size of convolving kernel
stride = 1  # *** int, tuple. Stride of convolution
padding = 0  # int, tuple, str. Padding added to all four sides of the input
dilation = 1  # int, tuple. Spacing between kernel elements
groups = 1  # int. Number of blocked connections from input channels to output
bias = True  # bool. If True, add a learnable bias to the output
padding_mode = "zeros"  # str. "zeros", "reflect", "replicate", or "circular"
device = None  # torch.device or int
dtype = None  # torch.dtype
## Weight. Shape: [out_channels, in_channels/groups, k_size[0], k_size[1]]
## Bias. Shape: [out_channels,]
conv2d = nn.Conv2d(
    in_channels, out_channels, kernel_size, stride, padding, dilation, groups, 
    bias, padding_mode, device, dtype
)
## [B, C, H_in, W_in] => [B, C, H_out, W_out]
## H_out = [(H_in + 2*padding[0] - dilation[0]*(kernel[0]-1)-1) / stride[0] + 1]
## W_out = [(W_in + 2*padding[1] - dilation[1]*(kernel[1]-1)-1) / stride[1] + 1]
y = conv2d(x)
## --------------------------------------------------------------------------------

## --------------------------------------------------------------------------------
## Conv3d
## --------------------------------------------------------------------------------
in_channels = /  # *** int. Number of channels in the input
out_channels = /  # *** int. Number of channels in the output
kernel_size = /  # *** int, tuple. Size of convolving kernel
stride = 1  # *** int, tuple. Stride of convolution
padding = 0  # int, tuple, str. Padding added to all six sides of the input
dilation = 1  # int, tuple. Spacing between kernel elements
groups = 1  # int. Number of blocked connections from input channels to output
bias = True  # bool. If True, add a learnable bias to the output
padding_mode = "zeros"  # str. "zeros", "reflect", "replicate", or "circular"
device = None  # torch.device or int
dtype = None  # torch.dtype
## Weight. Shape: [out_channels, in_channels/groups, k_size[0], k_size[1], k_size[2]]
## Bias. Shape: [out_channels,]
conv3d = nn.Conv3d(
    in_channels, out_channels, kernel_size, stride, padding, dilation, groups, 
    bias, padding_mode, device, dtype
)
## [B, C, D_in, H_in, W_in] => [B, C, D_out, H_out, W_out]
## D_out = [(D_in + 2*padding[0] - dilation[0]*(kernel[0]-1)-1) / stride[0] + 1]
## H_out = [(H_in + 2*padding[1] - dilation[1]*(kernel[1]-1)-1) / stride[1] + 1]
## W_out = [(W_in + 2*padding[2] - dilation[2]*(kernel[2]-1)-2) / stride[2] + 1]
y = conv3d(x)
## --------------------------------------------------------------------------------

## --------------------------------------------------------------------------------
## Dropout
## --------------------------------------------------------------------------------
p = 0.5  # *** float. Probability of an element to be zeroed
inplace = False  # bool
dropout = nn.Dropout(p, inplace)
y = dropout(x)
## --------------------------------------------------------------------------------
</code>
</pre>
</p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split" id='activation functionpytorch'></p>
            <div style="border-left: 14px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('activation function-PyTorch-details')"><i>Activation Function</i></p>
            
            
            <p class="paper_detail"><b> <font color=#404040>activation function</font></b>  &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://docs.pytorch.org/docs/stable/nn.html">docs</a> &nbsp; <font color=#D0D0D0></font></p>
            
            
            <div id='activation function-PyTorch-details' class="info_detail">
                <hr class="dashed">
                <p><font color=#202020>
It includes activation functions: 
<a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.GELU.html">GeLU</a>,
<a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.SiLU.html">SiLU / swish</a>.
</font></p>
                
                <p>
<pre>
<code class="language-python" style="font-size: 14px;">
from torch import nn

## --------------------------------------------------------------------------------
## GeLU (Gaussian Error Linear Units)
## --------------------------------------------------------------------------------
## GeLU(x) = x * phi(x)
gelu = nn.GeLU()
y = gelu(x)
## --------------------------------------------------------------------------------

## --------------------------------------------------------------------------------
## SiLU / swish (Sigmoid Linear Unit)
## --------------------------------------------------------------------------------
inplace = False
## silu(x) = x * sigmoid(x)
silu = nn.SiLU(inplace)
y = silu(x)
## --------------------------------------------------------------------------------
</code>
</pre>
</p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split" id='optimizerpytorch'></p>
            <div style="border-left: 14px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('optimizer-PyTorch-details')"><i>Optimizer</i></p>
            
            
            <p class="paper_detail"><b> <font color=#404040>optimizer</font></b>  &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://docs.pytorch.org/docs/stable/optim.html">docs</a> &nbsp; <font color=#D0D0D0></font></p>
            
            
            <div id='optimizer-PyTorch-details' class="info_detail">
                <hr class="dashed">
                <p><font color=#202020>It includes tools for building optimization algorithms: <a href="https://docs.pytorch.org/docs/stable/generated/torch.optim.AdamW.html#torch.optim.AdamW.step">AdamW</a>.</font></p>
                
                <p>
<pre>
<code class="language-python" style="font-size: 14px;">
from torch.optim import AdamW

## --------------------------------------------------------------------------------
## AdamW
## --------------------------------------------------------------------------------
params = /  # *** iterable. Parameters / named_parameters / parameter groups to optimize
lr = 0.001  # *** float, Tensor. Learning rate
betas = (0.9, 0.999)  # tuple. For computing running averages of gradients & squares
weight_decay = 0.01  # float. Weight decay coefficient
# ...
adam_optim = AdamW(params, lr, betas, weight_decay)
## --------------------------------------------------------------------------------
</code>
</pre>
</p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split" id='huggingfacepytorch'></p>
            <div style="border-left: 14px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('huggingface-PyTorch-details')"><i>huggingface</i></p>
            
            
            <p class="paper_detail"><b> <font color=#404040>huggingface</font></b>  &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://huggingface.co/docs">docs</a> &nbsp; <font color=#D0D0D0></font></p>
            
            
            <div id='huggingface-PyTorch-details' class="info_detail">
                <hr class="dashed">
                <p><font color=#202020>It includes tools from huggingface: <a href="https://huggingface.co/docs/huggingface_hub/main/en/package_reference/file_download#huggingface_hub.snapshot_download">snapshot_download</a></font></p>
                
                <p>
<pre>
<code class="language-python" style="font-size: 14px;">
from huggingface_hub import snapshot_download

## --------------------------------------------------------------------------------
## Download checkpoints from huggingface
## --------------------------------------------------------------------------------
repo_id = /  # *** str. A user name and a repo name, e.g., "Qwen/Qwen-VL-Chat"
repo_type = None  # *** str. "dataset", "space", or "model"
local_dir = None  # *** str or Path. If provided, directory to place the downloaded files
token = None  # str, bool. User token
max_workers = 8  # int. Number of concurrent threads to download files
# ...

snapshot_download(repo_id, repo_type, local_dir, token, max_workers)
</code>
</pre>
</p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            <h2 id="Distributed Training-table"><a class="no_dec" href="#Distributed Training">Distributed Training</a></h2>
            <p class="little_split" id='deepspeeddistributed training'></p>
            <div style="border-left: 14px solid #FFD6AD; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('deepspeed-Distributed Training-details')"><i>DeepSpeed</i></p>
            
            
            <p class="paper_detail"><b> <font color=#404040>deepspeed</font></b>  &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://deepspeed.readthedocs.io/en/latest/">docs</a> &nbsp; <font color=#D0D0D0></font></p>
            
            
            <div id='deepspeed-Distributed Training-details' class="info_detail">
                <hr class="dashed">
                <p><font color=#202020>DeepSpeed is an open-sourced deep learning optimization library developed by <b>Microsoft Research</b>, designed to simplify and accelerate the training and deployment of <b>large-scale</b> deep learning models.</font></p>
                
                <p>
<table class="center">
    <tr>
        <th>stage</th>
        <th>partition</th>
        <th>memory saving</th>
        <th>complexity</th>
    </tr>
    <tr>
        <td>stage 1</td>
        <td>optimizer states</td>
        <td>~40% - 60% (for Adam)</td>
        <td>low</td>
    </tr>
    <tr>
        <td>stage 2</td>
        <td>optimizer states & gradients</td>
        <td> additional ~15% - 25%</td>
        <td>medium</td>
    </tr>
    <tr>
        <td>stage 3</td>
        <td>optimizer states & gradients & model parameters</td>
        <td> up to 80% - 90%</td>
        <td>high</td>
    </tr>
</table>
</p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
    <script>
        function toggleTable(tableId) {
            const container = document.getElementById(tableId);
            const button = container.previousElementSibling;
            const isVisible = window.getComputedStyle(container).display !== 'none';
            if (!isVisible) {
                const images = container.querySelectorAll('img');
                images.forEach(img => {
                    if (img.dataset.src !== '') {img.src = img.dataset.src;}
                });
                container.style.display = 'block';
            } else {container.style.display = 'none';}
        }
    </script>
    
<button id="backToTop" title="back to top">↑</button>
<script>
    const button = document.getElementById("backToTop");
    window.addEventListener("scroll", () => {
    if (document.documentElement.scrollTop > 300) {
        button.style.display = "block";
    } else {
        button.style.display = "none";
    }
    });

    function updateButtonPosition() {
    const bodyRect = document.body.getBoundingClientRect();
    const windowWidth = window.innerWidth;
    const rightOffset = Math.max((windowWidth - bodyRect.width) / 2, 10);
    button.style.right = rightOffset + "px";
    }

    window.addEventListener("resize", updateButtonPosition);
    window.addEventListener("load", updateButtonPosition);

    button.addEventListener("click", () => {
    window.scrollTo({ top: 0, behavior: 'smooth' });
    });
</script>

</body>
</html>
