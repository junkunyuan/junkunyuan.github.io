
<!DOCTYPE html>
<html>
<head>
    <link rel="stylesheet" href="resource/html.css" type="text/css">
    <link rel="shortcut icon" href="resource/my_photo.jpg">
    <title>Paper Reading List</title>
    <meta name="description" content="Paper Reading List">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <div id="layout-content" style="margin-top:25px">
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.24.1/prism.min.js"></script>
    <link href="https://cdn.jsdelivr.net/npm/prismjs@1.24.1/themes/prism.css" rel="stylesheet">
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.24.1/components/prism-python.min.js"></script>
<body>

<h1 id="top">Coding and Engineering</h1>
<p class="larger"><b> Coding and Engineering.</b></p>
<p>Curated by <a href="https://junkunyuan.github.io/">Junkun Yuan</a>.</p>
<p>Click <a href="paper_reading_list.html">here</a> to go back to main contents.</p>
<p><font color=#B0B0B0>Last updated on July 16, 2025 at 17:14 (UTC+8).</font></p>
<hr><p id='table' class="larger"><b>Table of contents:</b></p><ul><li><a class="no_dec" id="torch & torchvision" href="#torch & torchvision-table"><b>torch & torchvision:</b></a> <a class="no_dec" href="#data transforms"><font color=#B0B0B0>data transforms</font></a> <font color=#B0B0B0>&nbsp;|&nbsp;</font> <a class="no_dec" href="#data loader"><font color=#B0B0B0>data loader</font></a></li></ul><h2 id="torch & torchvision-table"><a class="no_dec" href="#torch & torchvision">torch & torchvision</a></h2>
            <p class="little_split" id='data transforms'></p>
            <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('data transforms-torch & torchvision-details')"><i>Data Transforms</i></p>
            
            
            <p class="paper_detail"><b> data transforms</b>  &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://docs.pytorch.org/vision/stable/transforms.html">docs</a> &nbsp; <font color=#D0D0D0></font></p>
            
            
            <div id='data transforms-torch & torchvision-details' class="info_detail">
                <hr class="dashed">
                <p><font color=#202020>It includes tools to transform and augment data.</font></p>
                
                <p>
<pre>
<code class="language-python">
from torchvision import transforms
from torchvision.transforms.InterpolationMode import BILINEAR, NEAREST, NEAREST_EXACT, BILINEAR, BICUBIC 

## --------------------------------------------------------------------------------
## <a class="no_dec a_black" href="https://docs.pytorch.org/vision/stable/generated/torchvision.transforms.Resize.html?highlight=transforms+resize#torchvision.transforms.Resize"><b>Geometry: Resize</b></a>
## --------------------------------------------------------------------------------
size = /  # sequence or int. For example (512, 768)
interpolation = BILINEAR  # InterpolationMode
max_size = /  # int. Maximum allowed for the longer image edge, only supported if `size` is an int
antialias = /  # bool. Apply antialiasing, only under bilinear or bicubic modes

trans = <b>transforms.Resize</b>(size, interpolation=interpolation, max_size=max_size, antialias=antialias)
image_trans = trans(image)  # PIL Image => PIL Image or Tensor => Tensor
## --------------------------------------------------------------------------------

## --------------------------------------------------------------------------------
## <a class="no_dec a_black" href="https://docs.pytorch.org/vision/stable/generated/torchvision.transforms.RandomHorizontalFlip.html#torchvision.transforms.RandomHorizontalFlip"><b>Geometry: RandomHorizontalFlip</b></a>
## --------------------------------------------------------------------------------
p = 0.5  # float. Probability to flip image

trans = <b>transforms.RandomHorizontalFlip</b>(p=p)
image_trans = trans(image)  # PIL Image => PIL Image or Tensor => Tensor
## --------------------------------------------------------------------------------

## --------------------------------------------------------------------------------
## <a class="no_dec a_black" href="https://docs.pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html?highlight=totensor#torchvision.transforms.ToTensor"><b>Conversion: ToTensor</b></a>
## --------------------------------------------------------------------------------
## Input: PIL Image / numpy.ndarray (np.uint8) of shape (HxWxC) in the range [0, 255]
## Output: torch.FloatTensor of shape (CxHxW) in the range (0.0, 1.0)
## Other inputs: only apply type transform
trans = <b>transforms.ToTensor</b>()
image_trans = trans(image)  # PIL Image / ndarray => Tensor
## --------------------------------------------------------------------------------

## --------------------------------------------------------------------------------
## <a class="no_dec a_black" href="https://docs.pytorch.org/vision/stable/generated/torchvision.transforms.Compose.html#torchvision.transforms.Compose"><b>Composition: Compose</b></a>
## --------------------------------------------------------------------------------
transforms = /  # list of Transform objects

trans = <b>transforms.Compose</b>(transforms)
image_trans = trans(image)  # PIL Image / ndarray / Tensor => Tensor
## --------------------------------------------------------------------------------

## --------------------------------------------------------------------------------
## <a class="no_dec a_black" href="https://docs.pytorch.org/vision/stable/generated/torchvision.transforms.Normalize.html#torchvision.transforms.Normalize"><b>Miscellaneous: Normalize</b></a>
## --------------------------------------------------------------------------------
mean = /  # sequence. Means for each channel.
std = /  # sequence. Standard deviations for each channel.
inplace = False  # bool. Bool to make this operation in-place.

trans = <b>transforms.Normalize</b>(mean, std, inplace=inplace)
image_trans = trans(image)  # Tensor => Tensor
## --------------------------------------------------------------------------------
</code>
</pre>
</p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split" id='data loader'></p>
            <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('data loader-torch & torchvision-details')"><i>new paper Data Loader</i></p>
            
            
            <p class="paper_detail"><b> data loader</b>  &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader">docs</a> &nbsp; <font color=#D0D0D0></font></p>
            
            <p></p><p>
<pre>
<code class="language-python">
from torch.utils.data import DataLoader, Dataset, Sampler

dataset = /  # Dataset
batch_size = 1  # int. Number of samples per batch.
shuffle = False  # bool. If True, have the data shuffled at every epoch
sampler = None  # Sampler or Iterable. Define how to draw samples
batch_sampler = None  # Sampler or Iterable. customize sampling by giving indices
num_workers = 0  # int. Number of subprocesses to use for data loading
collate_fn = None  # Callable. Merge a list of samples to form a batch of tensors.
pin_memory = False  # bool. If True, copy Tensors into device/CUDA pinned memory before ruturning.
drop_last = False  # bool. If True, drop the last incomplete batch
timeout = 0  # numeric. If positive, set the timeout value for collecting a batch from workers.
worker_init_fn = None  # Callable. If not None, this will be called with the worker id as input
multiprocessing_context = None  # str or multiprocessing.context.BaseContext. If None, use the default multiprocessing context  If None, use the default multiprocessing context


data_loader = DataLoader(
    dataset, batch_size=batch_size, shuffle=shuffle, sampler=sampler, batch_sampler=batch_sampler, 
    num_workers=num_workers, collate_fn=collate_fn, pin_memory=pin_memory, drop_last=drop_last, 
    timeout=timeout, worker_init_fn=worker_init_fn, multiprocessing_context=multiprocessing_context, 
    generator=None, *, prefetch_factor=None, persistent_workers=False, pin_memory_device='', in_order=True)
</code>
</pre>
</p>
            <div id='data loader-torch & torchvision-details' class="info_detail">
                <hr class="dashed">
                <p><font color=#202020></font></p>
                
                <p>
<pre>
<code class="language-python">
from torch.utils.data import DataLoader, Dataset, Sampler

dataset = /  # Dataset
batch_size = 1  # int. Number of samples per batch.
shuffle = False  # bool. If True, have the data shuffled at every epoch
sampler = None  # Sampler or Iterable. Define how to draw samples
batch_sampler = None  # Sampler or Iterable. customize sampling by giving indices
num_workers = 0  # int. Number of subprocesses to use for data loading
collate_fn = None  # Callable. Merge a list of samples to form a batch of tensors.
pin_memory = False  # bool. If True, copy Tensors into device/CUDA pinned memory before ruturning.
drop_last = False  # bool. If True, drop the last incomplete batch
timeout = 0  # numeric. If positive, set the timeout value for collecting a batch from workers.
worker_init_fn = None  # Callable. If not None, this will be called with the worker id as input
multiprocessing_context = None  # str or multiprocessing.context.BaseContext. If None, use the default multiprocessing context  If None, use the default multiprocessing context


data_loader = DataLoader(
    dataset, batch_size=batch_size, shuffle=shuffle, sampler=sampler, batch_sampler=batch_sampler, 
    num_workers=num_workers, collate_fn=collate_fn, pin_memory=pin_memory, drop_last=drop_last, 
    timeout=timeout, worker_init_fn=worker_init_fn, multiprocessing_context=multiprocessing_context, 
    generator=None, *, prefetch_factor=None, persistent_workers=False, pin_memory_device='', in_order=True)
</code>
</pre>
</p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
    <script>
        function toggleTable(tableId) {
            const container = document.getElementById(tableId);
            const button = container.previousElementSibling;
            const isVisible = window.getComputedStyle(container).display !== 'none';
            if (!isVisible) {
                const images = container.querySelectorAll('img');
                images.forEach(img => {
                    if (img.dataset.src !== '') {
                        img.src = img.dataset.src;
                    }
                });
                container.style.display = 'block';
            } else {
                container.style.display = 'none';
                
            }
        }
    </script>

    <button id="backToTop" title="back to top">↑</button>
    <script>
        const button = document.getElementById("backToTop");
        window.addEventListener("scroll", () => {
        if (document.documentElement.scrollTop > 300) {
            button.style.display = "block";
        } else {
            button.style.display = "none";
        }
        });

        function updateButtonPosition() {
        const bodyRect = document.body.getBoundingClientRect();
        const windowWidth = window.innerWidth;
        const rightOffset = Math.max((windowWidth - bodyRect.width) / 2, 10);
        button.style.right = rightOffset + "px";
        }

        window.addEventListener("resize", updateButtonPosition);
        window.addEventListener("load", updateButtonPosition);

        button.addEventListener("click", () => {
        window.scrollTo({ top: 0, behavior: 'smooth' });
        });
    </script>
    
</body>
</html>
