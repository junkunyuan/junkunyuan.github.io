LIST = dict()
LIST["file"] = "paper_reading_list/native_multimodal_generation.html"
LIST["title"] = "Native Multimodal Generation"
LIST["description"] = "Integrate and generate multiple modalities (e.g., text, images, and videos) within a unified model."
LIST["categories"] = ["Foundation Algorithms & Models"]
LIST["papers"] = [
# {
# "title": "",
# "author": "",
# "organization": "",
# "date": "",
# "venue": "",
# "pdf_url": "",
# "code_url": "",
# "name": "",
# "comment": "",
# "category": "",
# "jupyter_notes": "",
# "info": "",
# "summary": """""",
# "details": 
# """
# <ul>
#     <li>
# </ul>
# <figure>
#     <img src='' width=500>
#     <figcaption>
#     <b>Figure 1.</b> 
#     </figcaption>
# </figure>
# """,
# },
{
"title": "BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture, Training and Dataset",
"author": "Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, Le Xue, Caiming Xiong, Ran Xu",
"organization": "Salesforce Research, University of Maryland, Virginia Tech, New York University, University of Washington, UC Davis",
"date": "20250514",
"venue": "arXiv 2025",
"pdf_url": "https://arxiv.org/pdf/2505.09568",
"code_url": "https://github.com/JiuhaiChen/BLIP3o",
"name": "BLIP3-o",
"comment": "",
"category": "Foundation Algorithms & Models",
"jupyter_notes": "",
"info": "",
"summary": """It finds it is beneficial to generate CLIP features by employing flow matching loss, and use sequential training of understanding and generation.""",
"details": 
"""
<ul>
    <li> <b>Structure.</b> Use Qwen 2.5-VL-7B-Instruct and freeze it, and train a 1.4B diffusion transformer (Lumina-Next) on it.
    <li> <b>Data.</b> Pre-training data: 25M open-source data and 30M proprietary data, with captions generated by Qwen 2.5-VL. Instruction tuning data: 60K.
    </ul>
fig: fig1.png 550
cap: <b>Structure.</b> It unifies the visual understanding and generation by using CLIP encoder.
fig: fig2.png 550
cap: <b>Design choices</b> on image generation in unified multimodal model.
fig: fig3.png 600
cap: Performance on different <b>design choices</b>. CLIP + Flow Matching is a better choice.
fig: fig4.png 600
cap: <b>Joint training vs. sequential training.</b>
""",
},
{
"title": "Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model",
"author": "Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, Omer Levy",
"organization": "Meta, Waymo, University of Southern California",
"date": "20240820",
"venue": "ICLR 2025",
"pdf_url": "https://arxiv.org/pdf/2408.11039",
"code_url": "",
"name": "Transfusion",
"comment": "",
"category": "Foundation Algorithms & Models",
"jupyter_notes": "",
"info": "**",
"summary": """It trains a unified model (7B) on 2T multi-modal tokens by predicting discrete text tokens and diffusing continuous image tokens.""",
"details": 
"""
<ul>
    <li> <b>Data.</b> Use total 2T tokens from: (1) Llama 2 tokenizer and corpus (2T tokens), (2) 380M Shutterstock images and captions (resized to 256x256).
    <li> <b>Structure.</b> It applies next-token prediction on discrete text tokens and diffusion loss on continuous image tokens: <i>L=L_LM+lambda*L_diffusion</i>. It uses <i>modality-specific</i> components with unshared parameters: embedding layer for text, and VAE (U-Net or linear structure, 8x8-8c) with linear or up/down blocks for images. It applies causal mask on text tokens and bidirectional mask on image tokens.
    <li> <b>Training details.</b> Optimizer=AdamW, lr=3e-4, 250K steps, lambda=5, train_timesteps=1000, infer_timesteps=250, cfg=3.
    <li> <b>Performance.</b> In text-to-image generation task, Transfusion exceeds Chameleon at less than a third of the compute. In image-to-text generation task, Transfusion exceeds Chameleon at 21.8% of the FLOPs. In text-to-text generation task, Transfusion exceeds Chameleon at 50% of FLOPs.
</ul>
fig: fig1.png 500
cap: <b>Transfusion structure.</b>
fig: fig2.png 500
cap: Transfusion outperforms Chameleon while <b>scaling</b>.
fig: fig3.png 400
cap: Transfusion outperforms Chameleon by using few <b>FLOPs</b>, both are 7B.
fig: fig4.png 500
cap: Transfusion achieves competitive results compared with <b>Llama2</b>.
fig: fig5.png 400
cap: <b>Encoder:</b> U-Net is better than linear (maybe due to it brings more inductive bias). <b>Attention:</b> bidirectional is better than causal. 
fig: fig6.png 500
cap: <b>Small patch size</b> leads to better performance by providing more visual tokens.
fig: fig7.png 500
cap: <b>Overall performance.</b>
""",
},
{
"title": "GPT-4o System Card",
"author": "",
"organization": "OpenAI",
"date": "20241025",
"venue": "arXiv 2024",
"pdf_url": "https://arxiv.org/pdf/2410.21276?",
"code_url": "",
"name": "GPT-4o",
"comment": "",
"category": "Foundation Algorithms & Models",
"jupyter_notes": "",
"info": "**",
"summary": """It proposes a unified autoregressive model trained end-to-end across text, vision, and audio.""",
"details": 
"""
fig: fig1.png 500 fig2.png 500
cap: <b>Visual generation capability of GPT-4o evaluated by <a href="https://arxiv.org/pdf/2504.05979">this paper</a>.</b> <i>Text rendering:</i> spelling, alignment, formatting in document. <i>Compositional generation and prompt following:</i> assemble complex scene elements, styles, attributes. <i>Geometric consistency and viewpoint realism:</i> 3D view synthesis, camera control, depth-conditioned rendering. <i>Comprehensive image transformation:</i> from low-level to high-level tasks.
""",
},
{
"title": "Unified Autoregressive Visual Generation and Understanding with Continuous Tokens",
"author": "Lijie Fan, Luming Tang, Siyang Qin, Tianhong Li, Xuan Yang, Siyuan Qiao, Andreas Steiner, Chen Sun, Yuanzhen Li, Tao Zhu, Michael Rubinstein, Michalis Raptis, Deqing Sun, Radu Soricut",
"organization": "Google DeepMind, MIT",
"date": "20250317",
"venue": "arXiv 2025",
"pdf_url": "https://arxiv.org/pdf/2503.13436",
"code_url": "",
"name": "UniFluid",
"comment": "",
"category": "Foundation Algorithms & Models",
"jupyter_notes": "",
"info": "",
"summary": """It achieves visual generation and understanding by applying diffusion loss on continuous visual tokens and cross-entropy loss on discrete text tokens.""",
"details": 
"""
fig: fig1.png 500
cap: <b>Framework:</b> joint training of visual generation and understanding tasks through next-token prediction. <b>Tokenizer:</b> use VAE to provide tokens for visual generation, use SigLIP to provide tokens for visual understanding, use SentencePiece to provide text tokens. <b>Prediction head:</b> use <i>modality-specific prediction heads</i> to calculate losses and sampling for each modality. <b>Loss:</b> image understanding loss on text answer + image generation loss on image tokens. <b>Training details:</b> batchsize=2048, optimizer=AdamW, lr=1e-4, steps=1M, init_ckpt=Gemma-2.
fig: fig3.png 800
cap: There is <b>trade-off</b> between generation & understanding.
fig: fig2.png 250
cap: <b>Unified training improves generation.</b>
fig: fig4.png 500
cap: <b>Better pre-trained LLM backbone</b> leads to better visual generation and understanding performance.
""",
},
]