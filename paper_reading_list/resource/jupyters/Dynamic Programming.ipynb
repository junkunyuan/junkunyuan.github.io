{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41c56566",
   "metadata": {},
   "source": [
    "#  Dynamic Programming\n",
    "\n",
    "Written by [Junkun Yuan](https://junkunyuan.github.io/) (yuanjk0921@outlook.com).\n",
    "\n",
    "See paper reading list and notes [here](https://junkunyuan.github.io/paper_reading_list/paper_reading_list.html).\n",
    "\n",
    "Last updated on Sep 06, 2025; &nbsp; First committed on Sep 06, 2025."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29b5251",
   "metadata": {},
   "source": [
    "**References**\n",
    "\n",
    "- [**Hands-on RL**](https://github.com/boyu-ai/Hands-on-RL/blob/main/%E7%AC%AC2%E7%AB%A0-%E5%A4%9A%E8%87%82%E8%80%81%E8%99%8E%E6%9C%BA%E9%97%AE%E9%A2%98.ipynb)\n",
    "\n",
    "**Contents**\n",
    "- Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca07ea1",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "[**Dynamic programming**](https://en.wikipedia.org/wiki/Dynamic_programming) aims to simplify a complicated problem by breaking it down into simpler sub-problems in a recursive manner.\n",
    "\n",
    "[**Value iteration**](https://en.wikipedia.org/wiki/Markov_decision_process#Value_iteration) employs optimal Bellman equation to obtian optimal state value through Bellman equaiton.\n",
    "\n",
    "[**Policy iteration**](https://en.wikipedia.org/wiki/Markov_decision_process#Policy_iteration) consists of **policy evaluation** and **policy improvement**, where policy evaluation obtains state value function with Bellman equation as a dynamic programming process.\n",
    "\n",
    "Value iteration and policy iteration require (1) access to the **transition function** and **reward function**, and (2) a discrete and finite state space and action space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf935cb8",
   "metadata": {},
   "source": [
    "## Cliff Walking\n",
    "\n",
    "The **Cliff Walking** problem is a reinforcement learning gridworld task where an agent must navigate from a *start state* to a *goal state* while avoiding a \"cliff\" of *terminal states* that yield large negative rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb1adcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "class CliffWalkingEnv:\n",
    "    def __init__(self, ncol=12, nrow=4):\n",
    "        self.ncol = ncol  # column of the grid\n",
    "        self.nrow = nrow  # row of the grid\n",
    "        ## transition matrix P[state][action] = [(p, next_state, reward, done)]\n",
    "        self.P = self.createP()\n",
    "\n",
    "    def createP(self):\n",
    "        ## Initialize the transition matrix\n",
    "        P = [[[] for j in range(4)] for i in range(self.nrow * self.ncol)]\n",
    "        ## 4 actions: up, down, left, right\n",
    "        change = [[0, -1], [0, 1], [-1, 0], [1, 0]]\n",
    "        for i in range(self.nrow):\n",
    "            for j in range(self.ncol):\n",
    "                for a in range(4):\n",
    "                    ## At cliff or goal state, any action has reward 0\n",
    "                    if i == self.nrow - 1 and j > 0:\n",
    "                        P[i * self.ncol + j][a] = [(1, i * self.ncol + j, 0, True)]\n",
    "                        continue\n",
    "                    ## At other states\n",
    "                    next_x = min(self.ncol - 1, max(0, j + change[a][0]))\n",
    "                    next_y = min(self.nrow - 1, max(0, i + change[a][1]))\n",
    "                    next_state = next_y * self.ncol + next_x\n",
    "                    reward = -1\n",
    "                    done = False\n",
    "                    ## Next step at cliff or goal state\n",
    "                    if next_y == self.nrow - 1 and next_x > 0:\n",
    "                        done = True\n",
    "                        if next_x != self.ncol - 1:  # at cliff\n",
    "                            reward = -100\n",
    "                    P[i * self.ncol + j][a] = [(1, next_state, reward, done)]\n",
    "        return P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0dbff9",
   "metadata": {},
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b959379",
   "metadata": {},
   "source": [
    "### Policy Evaluation\n",
    "\n",
    "Polisy evaluation calculates state value function of a policy.\n",
    "\n",
    "Given the Bellman equation:\n",
    "\n",
    "$$\n",
    "V^{\\pi}(s)=\\mathbb{E}_{\\pi}[R_t+\\gamma V^{\\pi}(S_{t+1})|S_{t}=s]=\\sum_{a\\in\\mathcal{A}}\\pi(a|s)(r(s,a)+\\gamma\\sum_{s'\\in S}p(s'|s,a)V^{\\pi}(s')),\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
