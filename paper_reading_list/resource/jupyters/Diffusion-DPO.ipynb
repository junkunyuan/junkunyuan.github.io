{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c64d62f-b0e4-4d15-8e7c-351266147c9a",
   "metadata": {},
   "source": [
    "# Diffusion-DPO\n",
    "\n",
    "Written by [Junkun Yuan](https://junkunyuan.github.io/) (yuanjk0921@outlook.com).\n",
    "\n",
    "See paper reading list and notes [here](https://junkunyuan.github.io/paper_reading_list/paper_reading_list.html).\n",
    "\n",
    "Last updated on Jul 12, 2025; &nbsp; First committed on Mar 30, 2025."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cea39b7-62eb-405e-9dbd-fa164aaa5dfb",
   "metadata": {},
   "source": [
    "**References**\n",
    "- [**Diffusion Model Alignment Using Direct Preference Optimization** *(CVPR 2024)*](https://arxiv.org/pdf/2311.12908)\n",
    "\n",
    "**Contents**\n",
    "- Diffusion-DPO\n",
    "- PyTorch Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f361145-3170-4a9c-b716-edc2779aa595",
   "metadata": {},
   "source": [
    "## Diffusion-DPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2469ed8-29c7-4e6d-b10e-b88e6108a76d",
   "metadata": {},
   "source": [
    "**Direct Preference Optimization (DPO)** is designed to directly optimize a model based on human preferences, <font color=red>without a reward model</font> or complex reinforcement learning algorithms.\n",
    "\n",
    "**Why using DPO for diffusion models?** (1) Existing RL-based methods are limited to small prompt sets; (2) Training using feedback from a reward model suffers from *mode collapse or reward hacking* and limited feedback types.\n",
    "\n",
    "DPO learns human preference from a preference dataset $\\mathcal{D}$ consisting of data pairs of **winning samples** $\\boldsymbol{x}_0^w$ & **losing samples** $\\boldsymbol{x}_0^l$ associated with the same **condition/prompt** $\\boldsymbol{c}$.\n",
    "\n",
    "The **[Bradley-Terry (reward) model](https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model)** $r$ parameterized by $\\phi$ is used to learns human preference by maximizing likelihood for binary classification with sigmoid $\\sigma$:\n",
    "\n",
    "$$\n",
    "L_{\\text{BT}}(\\phi) = -\\mathbb{E}_{\\boldsymbol{c},\\boldsymbol{x}_0^w,\\boldsymbol{x}_0^l}[\\log\\frac{e^{\\boldsymbol{x}_0^w}}{e^{\\boldsymbol{x}_0^w} + e^{\\boldsymbol{x}_0^l}}] = -\\mathbb{E}_{\\boldsymbol{c},\\boldsymbol{x}_0^w,\\boldsymbol{x}_0^l}[\\log\\sigma(r_{\\phi}(\\boldsymbol{c},\\boldsymbol{x}_0^w)-r_{\\phi}(\\boldsymbol{c},\\boldsymbol{x}_0^l))]. \\ \\ \\ \\ \\text{[Eq. (4) of the Diffusion-DPO paper]}\n",
    "$$\n",
    "\n",
    "Given a sample $\\boldsymbol{x}_0$ with condition $\\boldsymbol{c}$ generated by a model $p_{\\theta}(\\boldsymbol{x}_0|\\boldsymbol{c})$, Reinforcement Learning from Human Feedback (**RLHF**) optimizes $p_{\\theta}(\\boldsymbol{x}_0|\\boldsymbol{c})$ to maximize $r(\\boldsymbol{c},\\boldsymbol{x}_0)$ with weight regularization hyper-parameter $\\beta$ from a reference model $p_{\\text{ref}}$\n",
    "\n",
    "$$\n",
    "\\max_{p_{\\theta}}\\mathbb{E}_{\\boldsymbol{c}\\sim\\mathcal{D},\\boldsymbol{x}_0\\sim p_{\\theta}(\\boldsymbol{x}_0|\\boldsymbol{c})}[r(\\boldsymbol{c},\\boldsymbol{x}_0)]-\\beta\\cdot\\mathbb{D}_{\\text{KL}}[p_{\\theta}(\\boldsymbol{x}_0|\\boldsymbol{c})||p_{\\text{ref}}(\\boldsymbol{x}_0|\\boldsymbol{c})]. \\ \\ \\ \\ \\text{[Eq. (5) of the Diffusion-DPO paper]}\n",
    "$$\n",
    "\n",
    "Here, we use **diffusion latents** $\\boldsymbol{x}_{0:T}$ to derive a solution:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "&\\min_{p_{\\theta}}\\mathbb{E}_{p_{\\theta}(\\boldsymbol{x}_0|\\boldsymbol{c})}[-r(\\boldsymbol{c},\\boldsymbol{x}_0)/\\beta] + \\mathbb{D}_{\\text{KL}}(p_{\\theta}(\\boldsymbol{x}_0|\\boldsymbol{c})||p_{\\text{ref}}(\\boldsymbol{x}_0|\\boldsymbol{c})) \\\\\n",
    "\\le& \\min_{p_{\\theta}}\\mathbb{E}_{p_{\\theta}(\\boldsymbol{x}_0|\\boldsymbol{c})}[-r(\\boldsymbol{c},\\boldsymbol{x}_0)/\\beta] + \\mathbb{D}_{\\text{KL}}(p_{\\theta}(\\boldsymbol{x}_{0:T}|\\boldsymbol{c})||p_{\\text{ref}}(\\boldsymbol{x}_{0:T}|\\boldsymbol{c})) \\\\\n",
    "=& \\min_{p_{\\theta}}\\mathbb{E}_{p_{\\theta}(\\boldsymbol{x}_{0:T}|\\boldsymbol{c})}[-R(\\boldsymbol{c},\\boldsymbol{x}_{0:T})/\\beta] + \\mathbb{D}_{\\text{KL}}(p_{\\theta}(\\boldsymbol{x}_{0:T}|\\boldsymbol{c})||p_{\\text{ref}}(\\boldsymbol{x}_{0:T}|\\boldsymbol{c})) \\\\\n",
    "=&\\min_{p_{\\theta}}\\mathbb{E}_{p_{\\theta}(\\boldsymbol{x}_{0:T}|\\boldsymbol{c})}(\\log\\frac{p_{\\theta}(\\boldsymbol{x}_{0:T}|\\boldsymbol{c})}{p_{\\text{ref}}(\\boldsymbol{x}_{0:T}|\\boldsymbol{c})\\exp(R(\\boldsymbol{c},\\boldsymbol{x}_{0:T})/\\beta)/Z(\\boldsymbol{c})}-\\log Z(\\boldsymbol{c}))\\\\\n",
    "=&\\min_{p_{\\theta}}\\mathbb{D}_{\\text{KL}}(p_{\\theta}(\\boldsymbol{x}_{0:T}|\\boldsymbol{c})||p_{\\text{ref}}(\\boldsymbol{x}_{0:T}|\\boldsymbol{c})\\exp(R(\\boldsymbol{c},\\boldsymbol{x}_{0:T})/\\beta)/Z(\\boldsymbol{c})), \\ \\ \\ \\ \\text{[Eq. (17) of the Diffusion-DPO paper]}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "where $Z(\\boldsymbol{c})=\\sum_{\\boldsymbol{x}}p_{\\text{ref}}(\\boldsymbol{x}_{0:T}|\\boldsymbol{c})\\exp(R(\\boldsymbol{c},\\boldsymbol{x}_{0:T})/\\beta)$ is the partition function. It leads to the unique global optimal solution $p_{\\theta}^*$:\n",
    "\n",
    "$$\n",
    "p_{\\theta}^*(\\boldsymbol{x}_{0:T}|\\boldsymbol{c})=p_{\\text{ref}}(\\boldsymbol{x}_{0:T}|\\boldsymbol{c})\\exp(R(\\boldsymbol{c},\\boldsymbol{x}_{0:T})/\\beta)/Z(\\boldsymbol{c}) \\ \\ \\ \\ \\text{[Eq. (6) of the Diffusion-DPO paper]}.\n",
    "$$\n",
    "\n",
    "Based on the eqution above, the reward function is derived as\n",
    "$$\n",
    "R(\\boldsymbol{c},\\boldsymbol{x}_{0:T})=\\beta\\cdot\\log\\frac{p_{\\theta}^*(\\boldsymbol{x}_{0:T}|\\boldsymbol{c})}{p_{\\text{ref}}(\\boldsymbol{x}_{0:T}|\\boldsymbol{c})} + \\beta\\cdot\\log Z(\\boldsymbol{c}). \\ \\ \\text{[Eq. (7) of the Diffusion-DPO paper]}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594c74ab-f868-460e-b6df-ca1aa55dc61e",
   "metadata": {},
   "source": [
    "Based on Eq. (4) of the Diffusion-DPO paper, we have the reward objective (omit condition $\\boldsymbol{c}$ here)\n",
    "\n",
    "$$\n",
    "L(\\theta)=-\\mathbb{E}_{\\boldsymbol{x}_{1:T}^w\\sim q(\\boldsymbol{x}_{1:T}|\\boldsymbol{x}_0^w), \\boldsymbol{x}_{1:T}^l\\sim q(\\boldsymbol{x}_{1:T}|\\boldsymbol{x}_0^l)}[\\log\\sigma(\\beta\\log\\frac{p_{\\theta}(\\boldsymbol{x}_{0:T}^w)}{p_{\\text{ref}}(\\boldsymbol{x}^w_{0:T})} - \\beta\\log\\frac{p_{\\theta}(\\boldsymbol{x}_{0:T}^l)}{p_{\\text{ref}}(\\boldsymbol{x}_{0:T}^l)})]. \\ \\ \\text{[Eq. (8) of the Diffusion-DPO paper]} \\\\\n",
    "$$\n",
    "\n",
    "We then derive the first formulation of optimization objective of Diffusion-DPO:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "&\\min L_{1}(\\theta)\\\\\n",
    "=&-\\mathbb{E}_{\\boldsymbol{x}_{1:T}^w\\sim q(\\boldsymbol{x}_{1:T}|\\boldsymbol{x}_0^w), \\boldsymbol{x}_{1:T}^l\\sim q(\\boldsymbol{x}_{1:T}|\\boldsymbol{x}_0^l)}[\\log\\sigma(\\beta\\log\\frac{p_{\\theta}(\\boldsymbol{x}_{0:T}^w)}{p_{\\text{ref}}(\\boldsymbol{x}^w_{0:T})} - \\beta\\log\\frac{p_{\\theta}(\\boldsymbol{x}_{0:T}^l)}{p_{\\text{ref}}(\\boldsymbol{x}_{0:T}^l)})] \\\\\n",
    "=&-\\log\\sigma(\\beta\\mathbb{E}_{\\boldsymbol{x}_{1:T}^w\\sim q(\\boldsymbol{x}_{1:T}|\\boldsymbol{x}_0^w), \\boldsymbol{x}_{1:T}^l\\sim q(\\boldsymbol{x}_{1:T}|\\boldsymbol{x}_0^l)}[\\log\\frac{p_{\\theta}(\\boldsymbol{x}_{0:T}^w)}{p_{\\text{ref}}(\\boldsymbol{x}^w_{0:T})} - \\log\\frac{p_{\\theta}(\\boldsymbol{x}_{0:T}^l)}{p_{\\text{ref}}(\\boldsymbol{x}_{0:T}^l)}]) \\\\\n",
    "=&-\\log\\sigma(\\beta\\mathbb{E}_{\\boldsymbol{x}_{1:T}^w\\sim q(\\boldsymbol{x}_{1:T}|\\boldsymbol{x}_0^w), \\boldsymbol{x}_{1:T}^l\\sim q(\\boldsymbol{x}_{1:T}|\\boldsymbol{x}_0^l)}[\\sum_{t=1}^T\\log\\frac{p_{\\theta}(\\boldsymbol{x}_{t-1}^w|\\boldsymbol{x}_{t}^w)}{p_{\\text{ref}}(\\boldsymbol{x}_{t-1}^w|\\boldsymbol{x}_{t}^w)} - \\log\\frac{p_{\\theta}(\\boldsymbol{x}_{t-1}^l|\\boldsymbol{x}_{t}^l)}{p_{\\text{ref}}(\\boldsymbol{x}_{t-1}^l|\\boldsymbol{x}_{t}^l)}]) \\\\\n",
    "=&-\\log\\sigma(\\beta T\\mathbb{E}_t\\mathbb{E}_{\\boldsymbol{x}_{t-1,t}^w\\sim q(\\boldsymbol{x}_{t-1,t}|\\boldsymbol{x}_0^w), \\boldsymbol{x}_{t-1, t}^l\\sim q(\\boldsymbol{x}_{t-1, t}|\\boldsymbol{x}_0^l)}[\\log\\frac{p_{\\theta}(\\boldsymbol{x}_{t-1}^w|\\boldsymbol{x}_{t}^w)}{p_{\\text{ref}}(\\boldsymbol{x}_{t-1}^w|\\boldsymbol{x}_{t}^w)} - \\log\\frac{p_{\\theta}(\\boldsymbol{x}_{t-1}^l|\\boldsymbol{x}_{t}^l)}{p_{\\text{ref}}(\\boldsymbol{x}_{t-1}^l|\\boldsymbol{x}_{t}^l)}]) \\\\\n",
    "=&-\\mathbb{E}_{t,\\boldsymbol{x}_{t}^w\\sim q(\\boldsymbol{x}_{t}|\\boldsymbol{x}_0^w), \\boldsymbol{x}_{t}^l\\sim q(\\boldsymbol{x}_{t}|\\boldsymbol{x}_0^l)}\\log\\sigma(\\beta T\\mathbb{E}_{\\boldsymbol{x}_{t-1}^w\\sim p_{\\theta}(\\boldsymbol{x}_{t-1}|\\boldsymbol{x}_t^w), \\boldsymbol{x}_{t-1}^l\\sim p_{\\theta}(\\boldsymbol{x}_{t-1}|\\boldsymbol{x}_t^l)}[\\log\\frac{p_{\\theta}(\\boldsymbol{x}_{t-1}^w|\\boldsymbol{x}_{t}^w)}{p_{\\text{ref}}(\\boldsymbol{x}_{t-1}^w|\\boldsymbol{x}_{t}^w)} - \\log\\frac{p_{\\theta}(\\boldsymbol{x}_{t-1}^l|\\boldsymbol{x}_{t}^l)}{p_{\\text{ref}}(\\boldsymbol{x}_{t-1}^l|\\boldsymbol{x}_{t}^l)}]) \\\\\n",
    "=& -\\mathbb{E}_{(\\boldsymbol{x}_0^w, \\boldsymbol{x}_0^l)\\sim\\mathcal{D},t\\sim\\mathcal{U}(0,T),\\boldsymbol{x}_{t}^w\\sim q(\\boldsymbol{x}_{t}^w|\\boldsymbol{x}_0^w),\\boldsymbol{x}_{t}^l\\sim q(\\boldsymbol{x}_{t}^l|\\boldsymbol{x}_0^l)}\\log\\sigma(-\\beta T \\\\\n",
    "&(\\mathbb{D}_{\\text{KL}}(q(\\boldsymbol{x}_{t-1}^{w}|\\boldsymbol{x}_{0,t}^{w})||p_{\\theta}(\\boldsymbol{x}_{t-1}^{w}|\\boldsymbol{x}_{t}^{w}))-\\mathbb{D}_{\\text{KL}}(q(\\boldsymbol{x}_{t-1}^{w}|\\boldsymbol{x}_{0,t}^{w})||p_{\\text{ref}}(\\boldsymbol{x}_{t-1}^{w}|\\boldsymbol{x}_{t}^{w}))-\\\\\n",
    "&(\\mathbb{D}_{\\text{KL}}(q(\\boldsymbol{x}_{t-1}^{l}|\\boldsymbol{x}_{0,t}^{l})||p_{\\theta}(\\boldsymbol{x}_{t-1}^{l}|\\boldsymbol{x}_{t}^{l}))-\\mathbb{D}_{\\text{KL}}(q(\\boldsymbol{x}_{t-1}^{l}|\\boldsymbol{x}_{0,t}^{l})||p_{\\text{ref}}(\\boldsymbol{x}_{t-1}^{l}|\\boldsymbol{x}_{t}^{l}))))). \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Thus, we have\n",
    "\n",
    "<font color=red>\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\min L_{1}(\\theta)=&-\\mathbb{E}_{t,\\boldsymbol{\\epsilon}^{w},\\boldsymbol{\\epsilon}^{l}}\\log\\sigma[-\\beta T\\omega(\\lambda_t)\\\\\n",
    "&(||\\epsilon^w-\\epsilon_{\\theta}(\\boldsymbol{x}_t^{w},t)||_2^2 - ||\\epsilon^w-\\epsilon_{\\text{ref}}(\\boldsymbol{x}_t^{w},t)||_2^2 -\\\\\n",
    "&(||\\epsilon^l-\\epsilon_{\\theta}(\\boldsymbol{x}_t^{l},t)||_2^2 - ||\\epsilon^l-\\epsilon_{\\text{ref}}(\\boldsymbol{x}_t^{l},t)||_2^2))]. \\ \\ \\ \\ \\text{[Eq. (14) of the Diffusion-DPO paper]} \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "</font>\n",
    "\n",
    "<font color=red>Intuitively, it encourages the online model to predict more accurately on winning samples compared to the reference model; conversely, it does the opposite on losing samples.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513c760f-b1cd-4f1c-a1cc-d491f83843ae",
   "metadata": {},
   "source": [
    "We then derive the second formulation. Replace $q(\\boldsymbol{x}_{t-1,t}|\\boldsymbol{x}_0)$ by $p_{\\theta}(\\boldsymbol{x}_{t-1,t}|\\boldsymbol{x}_0)$ in the 4-th equation of $\\min L_1$, we have\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\min L_{2}(\\theta)=&-\\log\\sigma(\\beta T\\mathbb{E}_t\\mathbb{E}_{\\boldsymbol{x}_{t-1,t}^w\\sim p_{\\theta}(\\boldsymbol{x}_{t-1,t}|\\boldsymbol{x}_0^w), \\boldsymbol{x}_{t-1, t}^l\\sim p_{\\theta}(\\boldsymbol{x}_{t-1, t}|\\boldsymbol{x}_0^l)}[\\log\\frac{p_{\\theta}(\\boldsymbol{x}_{t-1}^w|\\boldsymbol{x}_{t}^w)}{p_{\\text{ref}}(\\boldsymbol{x}_{t-1}^w|\\boldsymbol{x}_{t}^w)} - \\log\\frac{p_{\\theta}(\\boldsymbol{x}_{t-1}^l|\\boldsymbol{x}_{t}^l)}{p_{\\text{ref}}(\\boldsymbol{x}_{t-1}^l|\\boldsymbol{x}_{t}^l)}]). \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "We find that using $q(\\boldsymbol{x}_{t}|\\boldsymbol{x}_0)p_{\\theta}(\\boldsymbol{x}_{t-1}|\\boldsymbol{x}_t)$ to approximate $p_{\\theta}(\\boldsymbol{x}_{t-1,t}|\\boldsymbol{x}_0)$ yields lower error because\n",
    "\n",
    "$$\n",
    "\\mathbb{D}_{\\text{KL}}(q(\\boldsymbol{x}_{t}|\\boldsymbol{x}_0)p_{\\theta}(\\boldsymbol{x}_{t-1}|\\boldsymbol{x}_t)||p_{\\theta}(\\boldsymbol{x}_{t-1,t}|\\boldsymbol{x}_0)) = \\mathbb{D}_{\\text{KL}}(q(\\boldsymbol{x}_{t}|\\boldsymbol{x}_0)||p_{\\theta}(\\boldsymbol{x}_{t}|\\boldsymbol{x}_0)) < \\mathbb{D}_{\\text{KL}}(q(\\boldsymbol{x}_{t-1,t}|\\boldsymbol{x}_0)p_{\\theta}(\\boldsymbol{x}_{t-1,t}|\\boldsymbol{x}_0)).\n",
    "$$\n",
    "\n",
    "This time, we use $q(\\boldsymbol{x}_{t}|\\boldsymbol{x}_0)p_{\\theta}(\\boldsymbol{x}_{t-1}|\\boldsymbol{x}_t)$ to approximate $p_{\\theta}(\\boldsymbol{x}_{t-1,t}|\\boldsymbol{x}_0)$ and rewrite the formulation:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "&\\min L_{2}(\\theta)\\\\\n",
    "=&-\\log\\sigma(\\beta T\\mathbb{E}_t\\mathbb{E}_{\\boldsymbol{x}_{t-1,t}^w\\sim q(\\boldsymbol{x}_{t}|\\boldsymbol{x}_0^w)p_{\\theta}(\\boldsymbol{x}_{t-1}|\\boldsymbol{x}_t^w), \\boldsymbol{x}_{t-1, t}^l\\sim q(\\boldsymbol{x}_{t}|\\boldsymbol{x}_0^l)p_{\\theta}(\\boldsymbol{x}_{t-1}|\\boldsymbol{x}_t^l)}[\\log\\frac{p_{\\theta}(\\boldsymbol{x}_{t-1}^w|\\boldsymbol{x}_{t}^w)}{p_{\\text{ref}}(\\boldsymbol{x}_{t-1}^w|\\boldsymbol{x}_{t}^w)} - \\log\\frac{p_{\\theta}(\\boldsymbol{x}_{t-1}^l|\\boldsymbol{x}_{t}^l)}{p_{\\text{ref}}(\\boldsymbol{x}_{t-1}^l|\\boldsymbol{x}_{t}^l)}]) \\\\\n",
    "=&-\\log\\sigma(\\beta T\\mathbb{E}_{t,\\boldsymbol{x}_{t}^w\\sim q(\\boldsymbol{x}_{t}|\\boldsymbol{x}_0^w), \\boldsymbol{x}_{t}^l\\sim q(\\boldsymbol{x}_{t}|\\boldsymbol{x}_0^l)}\\mathbb{E}_{\\boldsymbol{x}_{t-1}^w\\sim p_{\\theta}(\\boldsymbol{x}_{t-1}|\\boldsymbol{x}_t^w), \\boldsymbol{x}_{t-1}^l\\sim p_{\\theta}(\\boldsymbol{x}_{t-1}|\\boldsymbol{x}_t^l)}[\\log\\frac{p_{\\theta}(\\boldsymbol{x}_{t-1}^w|\\boldsymbol{x}_{t}^w)}{p_{\\text{ref}}(\\boldsymbol{x}_{t-1}^w|\\boldsymbol{x}_{t}^w)} - \\log\\frac{p_{\\theta}(\\boldsymbol{x}_{t-1}^l|\\boldsymbol{x}_{t}^l)}{p_{\\text{ref}}(\\boldsymbol{x}_{t-1}^l|\\boldsymbol{x}_{t}^l)}]) \\\\\n",
    "=&-\\mathbb{E}_{t,\\boldsymbol{x}_{t}^w\\sim q(\\boldsymbol{x}_{t}|\\boldsymbol{x}_0^w) \\boldsymbol{x}_{t}^l\\sim q(\\boldsymbol{x}_{t}|\\boldsymbol{x}_0^l)}\\log\\sigma(\\beta T\\mathbb{E}_{\\boldsymbol{x}_{t-1}^w\\sim p_{\\theta}(\\boldsymbol{x}_{t-1}|\\boldsymbol{x}_t^w), \\boldsymbol{x}_{t-1}^l\\sim p_{\\theta}(\\boldsymbol{x}_{t-1}|\\boldsymbol{x}_t^l)}[\\log\\frac{p_{\\theta}(\\boldsymbol{x}_{t-1}^w|\\boldsymbol{x}_{t}^w)}{p_{\\text{ref}}(\\boldsymbol{x}_{t-1}^w|\\boldsymbol{x}_{t}^w)} - \\log\\frac{p_{\\theta}(\\boldsymbol{x}_{t-1}^l|\\boldsymbol{x}_{t}^l)}{p_{\\text{ref}}(\\boldsymbol{x}_{t-1}^l|\\boldsymbol{x}_{t}^l)}]). \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Similarly, we have\n",
    "\n",
    "<font color=red>\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\min L_{2}(\\theta)=&-\\mathbb{E}_{t,\\boldsymbol{\\epsilon}^w,\\boldsymbol{\\epsilon}^l}\\log\\sigma(-\\beta T w(\\lambda_t)(||\\boldsymbol{\\epsilon}_{\\theta}(\\boldsymbol{x}_t^w,t)-\\boldsymbol{\\epsilon}_{\\text{ref}}(\\boldsymbol{x}_t^w,t)||^2_2 - ||\\boldsymbol{\\epsilon}_{\\theta}(\\boldsymbol{x}_t^l,t)-\\boldsymbol{\\epsilon}_{\\text{ref}}(\\boldsymbol{x}_t^l,t)||^2_2)). \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "</font>\n",
    "\n",
    "<font color=red>Intuitively, it aligns the outputs of the online model and the reference model on winning samples; conversely, it does the opposite way on losing samples.</font> However, since the online model and the reference model are initialized from the same model, this loss can not directly be optimized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37b6ed9-f88c-490d-a579-231efad6b033",
   "metadata": {},
   "source": [
    "**Experiments**\n",
    "\n",
    "- Use 851K sample pairs with 59K unique prompts from Pick-a-Pic dataset.\n",
    "- A <font color=red>learning rate of $\\frac{2000}{\\beta}2.048\\cdot10^{-8}$</font> is used with 25% of linear warmup.\n",
    "- <font color=red>$\\beta=2000$ for SD1.5 and $\\beta=5000$ for SDXL</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fce720-6fae-4b64-b81a-14b0772a6ec5",
   "metadata": {},
   "source": [
    "## PyTorch Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e20e9b8-294b-4d25-893e-f19f9eeec625",
   "metadata": {},
   "outputs": [],
   "source": [
    "## --------------------------------------------------------------------------------\n",
    "## Simple implementation of Diffusion-DPO loss\n",
    "## Modified from https://github.com/SalesforceAIResearch/DiffusionDPO\n",
    "## --------------------------------------------------------------------------------\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def get_dpo_loss(model_pred, ref_pred, target, beta_dpo):\n",
    "    \"\"\"\n",
    "    Calculate Diffusion-DPO L1 loss.\n",
    "    model_pred (`torch.Tensor`): online model prediction on both winning and losing samples.\n",
    "    ref_pred (`torch.Tensor`): reference model prediction on both winning and losing samples.\n",
    "    target (`torch.Tensor`): ground-truth of prediction target for both winning and losing samples.\n",
    "    beta_dpo (`float`): beta hyper-parameter.\n",
    "    \"\"\"\n",
    "    ## Target prediction loss of online model on winning & losing sampels.\n",
    "    model_losses = (model_pred - target).pow(2).mean(dim=[1,2,3])\n",
    "    model_losses_w, model_losses_l = model_losses.chunk(2)\n",
    "\n",
    "    ## Target prediction loss of reference model on winning & losing sampels.\n",
    "    ref_losses = (ref_pred - target).pow(2).mean(dim=[1,2,3])\n",
    "    ref_losses_w, ref_losses_l = ref_losses.chunk(2)\n",
    "    \n",
    "    term = model_losses_w - ref_losses_w - (model_losses_l  - ref_losses_l)\n",
    "    loss = -F.logsigmoid(-0.5 * beta_dpo * term).mean()\n",
    "\n",
    "    return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
