{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49fd480",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "## --------------------------------------------------------------------------------\n",
    "## Simple implementation of multi-head self-attention\n",
    "## Modified from timm: https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py\n",
    "## --------------------------------------------------------------------------------\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super().__init__(hidden_size, num_heads)\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        self.qkv = nn.Linear(hidden_size, hidden_size * 3)\n",
    "        self.proj = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, N, C = x.shape\n",
    "        # 1st computation complexity: N * C * 3C = 3N(C^2)\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)  # shape: (3, B, heads, N, head_dim)\n",
    "        q, k, v = qkv.unbind(0)  # shape: (B, heads, N, head_dim)\n",
    "\n",
    "        q = q * (self.head_dim ** -0.5)\n",
    "        # 2nd computation complexity: N * C * N = C(N^2)\n",
    "        attn = q @ k.transpose(-2, -1)  # shape: (B, heads, N, N)\n",
    "        if mask is not None:\n",
    "            attn += (mask * -1e9)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        # 3rd computation complexity: N * N * C = C(N^2)\n",
    "        x = attn @ v  # shape: (B, heads, N, head_dim)\n",
    "\n",
    "        x = x.transpose(1, 2).reshape(B, N, C)  # shape: (B, N, dim)\n",
    "        \n",
    "        # 4th computation complexity: N * C * C = N(C^2)\n",
    "        x = self.proj(x)\n",
    "\n",
    "        # total computation complexity: 3N(C^2) + C(N^2) + C(N^2) + N(C^2) = 4N(C^2) + 2C(N^2)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
