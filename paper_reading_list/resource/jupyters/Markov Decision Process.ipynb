{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45046ef5",
   "metadata": {},
   "source": [
    "#  Markov Decision Process\n",
    "\n",
    "Written by [Junkun Yuan](https://junkunyuan.github.io/) (yuanjk0921@outlook.com).\n",
    "\n",
    "See paper reading list and notes [here](https://junkunyuan.github.io/paper_reading_list/paper_reading_list.html).\n",
    "\n",
    "Last updated on Aug 31, 2025; &nbsp; First committed on Aug 31, 2025."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4e1638",
   "metadata": {},
   "source": [
    "**References**\n",
    "\n",
    "- [**Hands-on RL**](https://github.com/boyu-ai/Hands-on-RL/blob/main/%E7%AC%AC2%E7%AB%A0-%E5%A4%9A%E8%87%82%E8%80%81%E8%99%8E%E6%9C%BA%E9%97%AE%E9%A2%98.ipynb)\n",
    "\n",
    "**Contents**\n",
    "- Markov Process\n",
    "- Markov Reward Process\n",
    "- Markov Decision Process\n",
    "- Monte-Carlo Method\n",
    "- Occupancy Measure\n",
    "- Optimal Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6557be15",
   "metadata": {},
   "source": [
    "## Markov Process\n",
    "\n",
    "[**Stochastic process**](https://en.wikipedia.org/wiki/Stochastic_process) is a collection of random variables indexed by time or space that describes the evolution of a system subject to randomness. Let $S_t$ be the state set at timestep $t$, the state in the next timestep is represented as $P(S_{t+1}|S_1,...,S_t)$.\n",
    "\n",
    "[**Markov property**](https://en.wikipedia.org/wiki/Markov_property) refers to the memoryless property of a stochastic process, which means that its future evolution is independent of its history. That is, $P(S_{t+1}|S_t)=P(S_{t+1}|S_1, ...,S_t)$.\n",
    "\n",
    "[**Markov chain** or **Markov process**](https://en.wikipedia.org/wiki/Markov_chain) is a stochastic process with the Markov property. It can be represented as $<\\mathcal{S}, \\mathcal{P}>$, where $\\mathcal{S}$ is a state set and $\\mathcal{P}$ is a state transition matrix.\n",
    "\n",
    "Given a Markov process, we can start at a state and continue to move forward and generate a **state episode**, this process is also called **sampling**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979408ec",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Markov Reward Process\n",
    "\n",
    "A [**Markov reward model** or **Markov reward process (MRP)**](https://en.wikipedia.org/wiki/Markov_reward_model) is a stochastic process which extends Markov chain by adding a reward rate to each state. \n",
    "\n",
    "MRP consists of $<\\mathcal{S}, \\mathcal{P}, r, \\gamma>$ where $r$ is a reward function and $\\gamma\\in[0,1)$ is a discount factor. A larger $\\gamma$ pays more attention to long-term cumulative rewards, and vice versa.\n",
    "\n",
    "A **Return** $G_t$ is the decaying cumulative reward: $G_t=R_t+\\gamma R_{t+1}+\\gamma^2 R_{t+2}+...=\\sum_{k=0}^{\\inf}\\gamma^k R_{t+k}$, where $R_t$ is the obtained reward at timestep $t$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd76681",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The return of the state episode is -2.5.\n"
     ]
    }
   ],
   "source": [
    "## --------------------------------------------------------------------------------\n",
    "## Compute return of Markov reward process\n",
    "## --------------------------------------------------------------------------------\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "rewards = [-1, -2, -2, 10, 1, 0]  # reward function\n",
    "gamma = 0.5  # discount factor\n",
    "\n",
    "def compute_return(start_index, chain, gamma):\n",
    "    G = 0\n",
    "    for i in reversed(range(start_index, len(chain))):\n",
    "        G = gamma * G + rewards[chain[i] - 1]\n",
    "    return G\n",
    "\n",
    "chain = [1, 2, 3, 6]  # a state episode\n",
    "start_index = 0\n",
    "G = compute_return(start_index, chain, gamma)\n",
    "print(\"The return of the state episode is %s.\" % G)\n",
    "## --------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20eaa96",
   "metadata": {},
   "source": [
    "A **value function** evaluates **value**, i.e., the expected reward, of a state. That is, $V(s)=\\mathbb{E}[G_t|S_t=s]$.\n",
    "\n",
    "[**Bellman equation**](https://en.wikipedia.org/wiki/Bellman_equation) $V(s)=r(s)+\\gamma\\sum_{s'\\in\\mathcal{S}}p(s'|s)V(s')$ can be derived by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "V(s)\n",
    "&=\\mathbb{E}[G_t|S_t=s] \\\\\n",
    "&=\\mathbb{E}[R_t+\\gamma R_{t+1} + \\gamma^2 R_{t+2}+...|S_t=s] \\\\\n",
    "&=\\mathbb{E}[R_t+\\gamma G_{t+1}|S_t=s] \\\\\n",
    "&=\\mathbb{E}[R_t + \\gamma V(S_{t+1})|S_t=s] \\\\\n",
    "&=r(s)+\\gamma\\sum_{s'\\in\\mathcal{S}}p(s'|s)V(s')\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Let $\\mathcal{S}=\\{s_1,...,s_n\\}$, $\\mathcal{V}=[V(s_1),...,V(s_n)]^T$, $\\mathcal{R}=[r(s_1),...,r(s_n)]^T$, the Bellman equation would be $\\mathcal{V}=\\mathcal{R}+\\gamma\\mathcal{P}\\mathcal{V}$. \n",
    "\n",
    "We then have $O(n^3)$ compute complexity of the solution to value function: $\\mathcal{V}=(I-\\gamma\\mathcal{P})^{-1}\\mathcal{R}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182d72da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value function is \n",
      " [[-2.01950168]\n",
      " [-2.21451846]\n",
      " [ 1.16142785]\n",
      " [10.53809283]\n",
      " [ 3.58728554]\n",
      " [ 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "## --------------------------------------------------------------------------------\n",
    "## Compute value by Bellman equation\n",
    "## --------------------------------------------------------------------------------\n",
    "def compute(P, rewards, gamma, states_num):\n",
    "    rewards = np.array(rewards).reshape((-1, 1))\n",
    "    value = np.dot(np.linalg.inv(np.eye(states_num, states_num) - gamma * P),\n",
    "                   rewards)\n",
    "    return value\n",
    "\n",
    "## State transition matrix\n",
    "P = [\n",
    "    [0.9, 0.1, 0.0, 0.0, 0.0, 0.0],\n",
    "    [0.5, 0.0, 0.5, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.6, 0.0, 0.4],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.3, 0.7],\n",
    "    [0.0, 0.2, 0.3, 0.5, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
    "]\n",
    "P = np.array(P)\n",
    "V = compute(P, rewards, gamma, 6)\n",
    "print(\"The value function is \\n\", V)\n",
    "## --------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a260256",
   "metadata": {},
   "source": [
    "## Markov Decision Process\n",
    "\n",
    "[**Markov decision process (MDP)**](https://en.wikipedia.org/wiki/Markov_decision_process) (also called a stochastic dynamic program or stochastic control problem), is a model for sequential decision making when outcomes are uncertain.\n",
    "\n",
    "MDP is consist of $<\\mathcal{S},\\mathcal{A},P,r, \\gamma>$, where $\\mathcal{S}$ is the state set, $\\mathcal{A}$ is the action set, $\\gamma$ is a discount factor, $r(s,a)$ is the reward function, $P(s'|s,a)$ is the state transition function.\n",
    "\n",
    "The model in MDP is called **agent**, and its strategy to take action is called **policy**. That is, $\\pi(a|s)=P(A_t=a|S_t=s)$.\n",
    "\n",
    "The **state-value function** $V^{\\pi}(s)$ is the expected return obtained by starting from state $s$. That is, $V^{\\pi}(s)=\\mathbb{E}_{\\pi}[G_t|S_t=s]$.\n",
    "\n",
    "The **action-value function** $Q^{\\pi}(s,a)$ is the expected return obtained by starting from state $s$ and taking action $a$. That is, $Q^{\\pi}(s,a)=\\mathbb{E}_{\\pi}[G_t|S_t=s,A_t=a]$.\n",
    "\n",
    "We have their relationship: (1) $V^{\\pi}(s) =\\sum_{a\\in\\mathcal{A}} \\pi(a|s) Q^{\\pi}(s,a)$; (2) $Q^{\\pi}(s,a)=r(s,a)+\\gamma\\sum_{s'\\in\\mathcal{S}}P(s'|s,a)V^{\\pi}(s')$.\n",
    "\n",
    "The **Bellman equaiton** is\n",
    "$$\n",
    "V^{\\pi}(s)=\\mathbb{E}_{\\pi}[R_t+\\gamma V^{\\pi}(S_{t+1})|S_{t}=s]=\\sum_{a\\in\\mathcal{A}}\\pi(a|s)(r(s,a)+\\gamma\\sum_{s'\\in S}p(s'|s,a)V^{\\pi}(s')),\n",
    "$$\n",
    "\n",
    "$$\n",
    "Q^{\\pi}(s,a)=\\mathbb{E}_{\\pi}[R_t+\\gamma Q^{\\pi}(S_{t+1},A_{t+1})|S_t=s, A_t=a]=r(s,a)+\\gamma\\sum_{s'\\in S}p(s'|s,a)\\sum_{a'\\in\\mathcal{A}}\\pi(a'|s')Q^{\\pi}(s',a').\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e929806f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The state-value function:\n",
      " [[-1.22555411]\n",
      " [-1.67666232]\n",
      " [ 0.51890482]\n",
      " [ 6.0756193 ]\n",
      " [ 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "## --------------------------------------------------------------------------------\n",
    "## Calculate state-value function\n",
    "## --------------------------------------------------------------------------------\n",
    "S = [\"s1\", \"s2\", \"s3\", \"s4\", \"s5\"]  # state set\n",
    "A = [\"保持s1\", \"前往s1\", \"前往s2\", \"前往s3\", \"前往s4\", \"前往s5\", \"概率前往\"]  # action set\n",
    "## state transition function\n",
    "P = {\n",
    "    \"s1-保持s1-s1\": 1.0,\n",
    "    \"s1-前往s2-s2\": 1.0,\n",
    "    \"s2-前往s1-s1\": 1.0,\n",
    "    \"s2-前往s3-s3\": 1.0,\n",
    "    \"s3-前往s4-s4\": 1.0,\n",
    "    \"s3-前往s5-s5\": 1.0,\n",
    "    \"s4-前往s5-s5\": 1.0,\n",
    "    \"s4-概率前往-s2\": 0.2,\n",
    "    \"s4-概率前往-s3\": 0.4,\n",
    "    \"s4-概率前往-s4\": 0.4,\n",
    "}\n",
    "## reward funciton\n",
    "R = {\n",
    "    \"s1-保持s1\": -1,\n",
    "    \"s1-前往s2\": 0,\n",
    "    \"s2-前往s1\": -1,\n",
    "    \"s2-前往s3\": -2,\n",
    "    \"s3-前往s4\": -2,\n",
    "    \"s3-前往s5\": 0,\n",
    "    \"s4-前往s5\": 10,\n",
    "    \"s4-概率前往\": 1,\n",
    "}\n",
    "gamma = 0.5  # dicount factor\n",
    "MDP = (S, A, P, R, gamma)\n",
    "\n",
    "## Policy 1, random\n",
    "Pi_1 = {\n",
    "    \"s1-保持s1\": 0.5,\n",
    "    \"s1-前往s2\": 0.5,\n",
    "    \"s2-前往s1\": 0.5,\n",
    "    \"s2-前往s3\": 0.5,\n",
    "    \"s3-前往s4\": 0.5,\n",
    "    \"s3-前往s5\": 0.5,\n",
    "    \"s4-前往s5\": 0.5,\n",
    "    \"s4-概率前往\": 0.5,\n",
    "}\n",
    "## Policy 2\n",
    "Pi_2 = {\n",
    "    \"s1-保持s1\": 0.6,\n",
    "    \"s1-前往s2\": 0.4,\n",
    "    \"s2-前往s1\": 0.3,\n",
    "    \"s2-前往s3\": 0.7,\n",
    "    \"s3-前往s4\": 0.5,\n",
    "    \"s3-前往s5\": 0.5,\n",
    "    \"s4-前往s5\": 0.1,\n",
    "    \"s4-概率前往\": 0.9,\n",
    "}\n",
    "\n",
    "# 把输入的两个字符串通过“-”连接,便于使用上述定义的P、R变量\n",
    "def join(str1, str2):\n",
    "    return str1 + '-' + str2\n",
    "\n",
    "## MRP from MDP + policy 1\n",
    "P_from_mdp_to_mrp = [\n",
    "    [0.5, 0.5, 0.0, 0.0, 0.0],\n",
    "    [0.5, 0.0, 0.5, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.5, 0.5],\n",
    "    [0.0, 0.1, 0.2, 0.2, 0.5],\n",
    "    [0.0, 0.0, 0.0, 0.0, 1.0],\n",
    "]\n",
    "P_from_mdp_to_mrp = np.array(P_from_mdp_to_mrp)\n",
    "R_from_mdp_to_mrp = [-0.5, -1.5, -1.0, 5.5, 0]\n",
    "\n",
    "V = compute(P_from_mdp_to_mrp, R_from_mdp_to_mrp, gamma, 5)\n",
    "print(\"The state-value function:\\n\", V)\n",
    "## --------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c07901",
   "metadata": {},
   "source": [
    "## Monte-Carlo Method\n",
    "\n",
    "[**Monte-Carlo methods**](https://en.wikipedia.org/wiki/Monte_Carlo_method) are algorithms that rely on repeated random sampling to obtain numerical results.\n",
    "\n",
    "The value can be directly estimated by the Monte-Carlo method: $V^{\\pi}(s)=\\mathbb{E}_{\\pi}[G_t|S_t=s]\\approx\\frac{1}{N}\\sum_{i=1}^{N}G_t^{(i)}$.\n",
    "\n",
    "Another way is to maintain a state count and reward summation:\n",
    "- Sample several sequence\n",
    "- Update state count and reward summation of states in each sequence\n",
    "- Estimate the value by averaging the reward of each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0a0adf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 1-st episode\n",
      " [('s1', '前往s2', 0, 's2'), ('s2', '前往s1', -1, 's1'), ('s1', '保持s1', -1, 's1'), ('s1', '前往s2', 0, 's2'), ('s2', '前往s3', -2, 's3'), ('s3', '前往s5', 0, 's5')]\n",
      "The 4-th episode\n",
      " [('s4', '前往s5', 10, 's5')]\n"
     ]
    }
   ],
   "source": [
    "def sample(MDP, Pi, timestep_max, number):\n",
    "    S, A, P, R, gamma = MDP\n",
    "    episodes = []\n",
    "    for _ in range(number):\n",
    "        episode = []\n",
    "        timestep = 0\n",
    "        ## randomly select a state as the start except for s5\n",
    "        s = S[np.random.randint(4)]\n",
    "        ## If the current state is the terminal state or the timestep is too long, finished\n",
    "        while s != \"s5\" and timestep <= timestep_max:\n",
    "            timestep += 1\n",
    "            rand, temp = np.random.rand(), 0\n",
    "            ## Select action based on policy\n",
    "            for a_opt in A:\n",
    "                temp += Pi.get(join(s, a_opt), 0)\n",
    "                if temp > rand:\n",
    "                    a = a_opt\n",
    "                    r = R.get(join(s, a), 0)\n",
    "                    break\n",
    "            rand, temp = np.random.rand(), 0\n",
    "            ## Transfer to the next state\n",
    "            for s_opt in S:\n",
    "                temp += P.get(join(join(s, a), s_opt), 0)\n",
    "                if temp > rand:\n",
    "                    s_next = s_opt\n",
    "                    break\n",
    "            episode.append((s, a, r, s_next))\n",
    "            s = s_next\n",
    "        episodes.append(episode)\n",
    "    return episodes\n",
    "\n",
    "## Sample 5 times\n",
    "episodes = sample(MDP, Pi_1, 20, 5)\n",
    "print('The 1-st episode\\n', episodes[0])\n",
    "print('The 4-th episode\\n', episodes[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09c3784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用蒙特卡洛方法计算MDP的状态价值为\n",
      " {'s1': -1.221722518998472, 's2': -1.6913418637046331, 's3': 0.5057505430728482, 's4': 5.984319469137363, 's5': 0}\n"
     ]
    }
   ],
   "source": [
    "## --------------------------------------------------------------------------------\n",
    "## Estmate state value function by Monter-Carlo method\n",
    "## --------------------------------------------------------------------------------\n",
    "def MC(episodes, V, N, gamma):\n",
    "    for episode in episodes:\n",
    "        G = 0\n",
    "        for i in range(len(episode) - 1, -1, -1):\n",
    "            (s, a, r, s_next) = episode[i]\n",
    "            G = r + gamma * G\n",
    "            N[s] = N[s] + 1\n",
    "            V[s] = V[s] + (G - V[s]) / N[s]\n",
    "\n",
    "timestep_max = 20\n",
    "## Sample 1000 times\n",
    "episodes = sample(MDP, Pi_1, timestep_max, 1000)\n",
    "V = {\"s1\": 0, \"s2\": 0, \"s3\": 0, \"s4\": 0, \"s5\": 0}\n",
    "N = {\"s1\": 0, \"s2\": 0, \"s3\": 0, \"s4\": 0, \"s5\": 0}\n",
    "MC(episodes, V, N, gamma=0.5)\n",
    "print(\"The estimated state-value fucntion\\n\", V)\n",
    "## --------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11e0550",
   "metadata": {},
   "source": [
    "## Occupancy Measure\n",
    "\n",
    "We define the probability of the agent to achieve state $s$ at timestep $t$ with policy $\\pi$ is $P_t^{\\pi}(s)$, then we have the MDP initial state distribution $P_0^{\\pi}(s)=\\nu^{\\pi}_0(s)$, we have the **state visitation distribution**:\n",
    "$$\n",
    "\\nu^{\\pi}(s)=(1-\\gamma)\\sum_{t=0}^{\\inf}\\gamma^{t}P_{t}^{\\pi}(s).\n",
    "$$\n",
    "It has the following property:\n",
    "$$\n",
    "\\nu^{\\pi}(s')=(1-\\gamma)\\nu_0(s')+\\gamma\\int P(s'|s,a)\\pi(a|s)\\nu^{\\pi}(s)dsda.\n",
    "$$\n",
    "The **occupancy measure** is the the probability of action-state pair $(s,a)$, reprensented as \n",
    "\n",
    "$$\n",
    "\\rho^{\\pi}(s,a)=(1-\\gamma)\\sum_{t=0}^{\\inf}\\gamma^{t}P_t^\\pi(s)\\pi(a|s).\n",
    "$$ \n",
    "\n",
    "We have $\\rho^{\\pi}(s,a)=\\nu^{\\pi}(s)\\pi(a|s)$.\n",
    "\n",
    "**Theorem 1.** The occupancy measure $\\rho^{\\pi_1}$ and $\\rho^{\\pi_2}$ obtained by the agent interacting with the same MDP under different policies $\\pi_1$ and $\\pi_2$ satisfy: $\\rho^{\\pi_1}=\\rho^{\\pi_2}\\Leftrightarrow\\pi_1=\\pi_2$.\n",
    "\n",
    "**Theorem 2.** Given a legal occupancy measure $\\rho$, the unique policy that generates it is $\\pi_{\\rho}=\\frac{\\rho(s,a)}{\\sum_{a'}\\rho(s,a')}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b775027c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.112567796310472 0.23199480615618912\n"
     ]
    }
   ],
   "source": [
    "## --------------------------------------------------------------------------------\n",
    "## Estimate occupany\n",
    "## --------------------------------------------------------------------------------\n",
    "def occupancy(episodes, s, a, timestep_max, gamma):\n",
    "    rho = 0\n",
    "    total_times = np.zeros(timestep_max)  # 记录每个时间步t各被经历过几次\n",
    "    occur_times = np.zeros(timestep_max)  # 记录(s_t,a_t)=(s,a)的次数\n",
    "    for episode in episodes:\n",
    "        for i in range(len(episode)):\n",
    "            (s_opt, a_opt, r, s_next) = episode[i]\n",
    "            total_times[i] += 1\n",
    "            if s == s_opt and a == a_opt:\n",
    "                occur_times[i] += 1\n",
    "    for i in reversed(range(timestep_max)):\n",
    "        if total_times[i]:\n",
    "            rho += gamma**i * occur_times[i] / total_times[i]\n",
    "    return (1 - gamma) * rho\n",
    "\n",
    "\n",
    "gamma = 0.5\n",
    "timestep_max = 1000\n",
    "\n",
    "episodes_1 = sample(MDP, Pi_1, timestep_max, 1000)\n",
    "episodes_2 = sample(MDP, Pi_2, timestep_max, 1000)\n",
    "rho_1 = occupancy(episodes_1, \"s4\", \"概率前往\", timestep_max, gamma)\n",
    "rho_2 = occupancy(episodes_2, \"s4\", \"概率前往\", timestep_max, gamma)\n",
    "print(rho_1, rho_2)\n",
    "## --------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5e23c9",
   "metadata": {},
   "source": [
    "## Optimal Policy\n",
    "\n",
    "**Optimial policy** $\\pi$ satisfy $V^{\\pi}(s)\\geq V^{\\pi'}(s)$ for any $\\pi'$.\n",
    "\n",
    "**Optimal state value function** is $V^*(s)=\\max_{\\pi}V^{\\pi}(s)$; and **optimal value function** is $Q^*(s,a)=\\max_{\\pi}Q^{\\pi}(s,a)$.\n",
    "\n",
    "They have the relationship: $Q^*(s,a)=r(s,a)+\\gamma\\sum_{s'\\in\\mathcal{S}}P(s'|s,a)V^*(s')$. Meanwhile, $V^*(s)=\\max_{a\\in\\mathcal{A}}Q^*(s,a)$.\n",
    "\n",
    "We then have **Bellman optimally equation**:\n",
    "\n",
    "$$\n",
    "V^*(s)=\\max_{a\\in\\mathcal{A}}\\{r(s,a) + \\gamma\\sum_{s'\\in\\mathcal{S}}p(s'|s,a)V^*(s')\\},\n",
    "$$\n",
    "\n",
    "$$\n",
    "Q^*(s,a)=r(s,a)+\\gamma\\sum_{s'\\in\\mathcal{S}}p(s'|s,a)\\max_{a'\\in\\mathcal{A}}Q^*(s',a').\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
