
<!DOCTYPE html>
<html>
<head>
    <link rel="stylesheet" href="resource/html.css" type="text/css">
    <link rel="shortcut icon" href="resource/my_photo.jpg">
    <title>Paper Reading List</title>
    <meta name="description" content="Paper Reading List">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <div id="layout-content" style="margin-top:25px">
<body>

<h1 id="top">Visual Generative Models</h1>
<p><b><font size=3><font color='#D93053'>32</font> papers on Visual Generative Models.</font></b></p>
<p>Curated by <a href="https://junkunyuan.github.io/">Junkun Yuan</a>.</p>
<p>Click <a href="paper_reading_list.html">here</a> to go back to main contents.</p>
<p><font color=#B0B0B0>Last updated on June 30, 2025 at 19:46 (UTC+8).</font></p>
<h2>Foundation Algorithms & Models</h2>
            <p class="little_split"></p>
            <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('SimpleAR-details')"><i>SimpleAR: Pushing the Frontier of Autoregressive Visual Generation through Pretraining, SFT, and RL</i></p>
            <p class="paper_detail">Junke Wang, Zhi Tian, Xun Wang, Xinyu Zhang, Weilin Huang, Zuxuan Wu, Yu-Gang Jiang</p>
            <p class="paper_detail">Fudan University, ByteDance Seed</p>
            <p class="paper_detail"><b><font color=#202020>Apr 15, 2025 &nbsp; SimpleAR</font></b> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/wdrink/SimpleAR/">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2504.11455">arXiv 2025</a> &nbsp; <font color=#D0D0D0></font></p>
            
            <div id='SimpleAR-details' class="info_detail">
                <p class="summary">A vanilla, open-sourced AR model (0.5B) for 1K text-to-image generation, trained by pre-training, SFT, RL (GRPO), and acceleration.</p>
                
                <p>
<ul>
    <li> Use <i>Qwen</i> structure and taking <i>Cosmos</i> as the visual tokenizer with 64K codebook and 16 ratio downsampling.
    <li> Training stages: (1) pre-training on 512 resolution; (2) SFT on 1024 resolution; (3) RL on 1024 resolution.
    <li> Use LLM initialization does not improve DPG-Bench performance.
    <li> Use 2D RoPE will not improve performance, but is necessary for dynamic resolution generation.
    <li> Use GRPO with CLIP as the reward model improves more than using HPS v2.
    <li> Use some acceleration techniques: KV cache, vLLM serving, and speculative jacobi decoding.
</ul>
</p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split"></p>
            <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('Seedream 3.0-details')"><i>Seedream 3.0 Technical Report</i></p>
            <p class="paper_detail">ByteDance Seed Vision Team</p>
            <p class="paper_detail">ByteDance</p>
            <p class="paper_detail"><b><font color=#202020>Apr 15, 2025 &nbsp; Seedream 3.0</font></b>  &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2504.11346">arXiv 2025</a> &nbsp; <font color=#D0D0D0></font></p>
            
            <div id='Seedream 3.0-details' class="info_detail">
                <p class="summary"><b>ByteDance Seed Vision Team</b>'s text-to-image generation model, improving Seedream 2.0 by representation alignment, larger reward models.</p>
                
                <p>
<ul>
    <li> Employ defect-aware training: stop gradient on watermarks, subtitles, overlaid text, mosaic pattern.
    <li> Introduce a <i>representation alignment loss</i>: cosine distance between the feature of MMDiT and DINOv2-L.
    <li> Find <i>scaling property of VLM-based reward model</i>.
    <li> Other improvements: (1) mixed-resolution training; (2) <i>cross-modality RoPE</i>; (3) diverse aesthetic captions in SFT.
</ul>
<figure>
    <img src='resource/figs/2025-04-15-Seedream 3.0-fig1.png' width=400>
    <figcaption><b>Figure 1.</b> Seedream3.0 achieves the best ELO performance.</figcaption>
</figure>
</p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split"></p>
            <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('Seaweed-7B-details')"><i>Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model</i></p>
            <p class="paper_detail">ByteDance Seaweed Team</p>
            <p class="paper_detail">ByteDance</p>
            <p class="paper_detail"><b><font color=#202020>Apr 11, 2025 &nbsp; Seaweed-7B</font></b>  &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2504.08685">arXiv 2025</a> &nbsp; <font color=#D0D0D0></font></p>
            
            <div id='Seaweed-7B-details' class="info_detail">
                <p class="summary"><b>ByteDance Seaweed Team</b>'s text-to-video and image-to-video generation model (7B), trained on O(100M) videos using 665K H100 GPU hours.</p>
                
                <p>
<figure>
    <img src='resource/figs/2025-04-11-Seaweed-7B-fig2.png' width=450>
    <figcaption><b>Figure 1.</b> <b>VAE</b> with compression ratio of 16x16x4 (48 channels) or 8x8x4 (16 channels). Using L1 + KL + LPIPS + adversarial losses. Using an <i>image discriminator and a video discriminator</i> is better than using either one. <i>Compressing using VAE outperforms patchification in DiT, and faster</i>.</figcaption>
</figure>
<figure>
    <img src='resource/figs/2025-04-11-Seaweed-7B-fig3.png' width=200>
    <figcaption><b>Figure 2.</b> <b>VAE training stages</b> for images and videos.</figcaption>
</figure>
<figure>
    <img src='resource/figs/2025-04-11-Seaweed-7B-fig4.png' width=800>
    <figcaption><b>Figure 3.</b> Use <b>mixed resolution & durations & frame rate</b> VAE training converges slower but performs better than training on a low resolution.</figcaption>
</figure>
<figure>
    <img src='resource/figs/2025-04-11-Seaweed-7B-fig6.png' width=650>
    <figcaption><b>Figure 4.</b> <b>Full attention</b> enjoys training scalability.</figcaption>
</figure>
<figure>
    <img src='resource/figs/2025-04-11-Seaweed-7B-fig5.png' width=250>
    <figcaption><b>Figure 5.</b> The proposed <b>hybrid-stream</b> is better than dual-stream (MMDiT).</figcaption>
</figure>
<figure>
    <img src='resource/figs/2025-04-11-Seaweed-7B-fig7.png' width=500>
    <figcaption><b>Figure 6.</b> <b>4-stage pre-training.</b> (1) <b>Multi-task pre-training:</b> text-to-video, image-to-video, video-to-video. Input features and conditions are channel-concatenated, with a binary mask indicating the condition. Ratio of image-to-video is 20% during pre-training, and increases to 50%-75% detached for fine-tuning. (2) <b>SFT:</b> use 700K good videos and 50K top videos; The semantic alignment ability drops a little. (3) <b>RLHF:</b> lr=1e-7, beta=100, select win-lose from 4 candidates. (4) <b>Distillation:</b> trajectory segmented consistency distillation + CFG distillation + adversarial training, distill to 8 steps.</figcaption>
</figure>
<figure>
    <img src='resource/figs/2025-04-11-Seaweed-7B-fig8.png' width=300>
    <figcaption><b>Figure 7.</b> <b>ELO performance</b> on image-to-video generation.</figcaption>
</figure>
</p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split"></p>
            <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('Wan-details')"><i>Wan: Open and Advanced Large-Scale Video Generative Models</i></p>
            <p class="paper_detail">Tongyi Wanxiang</p>
            <p class="paper_detail">Alibaba</p>
            <p class="paper_detail"><b><font color=#202020>Mar 26, 2025 &nbsp; Wan</font></b> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/Wan-Video/Wan2.1/">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2503.20314">arXiv 2025</a> &nbsp; <font color=#D0D0D0></font></p>
            
            <div id='Wan-details' class="info_detail">
                <p class="summary"><b>Alibaba Tongyi Wanxiang</b>'s text-to-video and image-to-video generation models (14B) with DiT structure.</p>
                
                <p>
<p><b>Data procssing pipeline</b>. <i>Fundamental dimensions:</i> text, aesthetic, NSFW score, watermark and logo, black border, overexposure, synthetic image, blur, duration and resolution. <i>Visual quality:</i> clustering, scoring. <i>Motion quality:</i> optimal motion, medium-quality motion, static videos, camera-driven motion, low-quality motion, shaky camera footage. <i>Visual text data:</i> hundreds of millions of text-containing images by rendering Chinese characters on a pure white background and large amounts from real-world data. <i>Captions:</i> celebrities, landmarks, movie characters, object counting, OCR, camera angle and motion, categories, relational understanding, re-caption, editing instruction caption, group image description, human-annotated captions.</p>
<figure>
    <img src='resource/figs/2025-03-26-Wan-fig2.png' width=500>
    <figcaption><b>Figure 1.</b> <b>VAE</b> with 8x8x4 compression ratio is trained by L1 reconstruction loss + KL loss + LPIPS perceptual loss. </figcaption>
</figure>
<figure>
    <img src='resource/figs/2025-03-26-Wan-fig3.png' width=600>
    <figcaption><b>Figure 2.</b> <b>Architecture</b>. Text prompt encoded by umT5 is injected by cross-attention; timestep is embedded by MLP; using flow-matching loss.</figcaption>
</figure>
<figure>
    <img src='resource/figs/2025-03-26-Wan-fig5.png' width=600>
    <figcaption><b>Figure 3.</b> <b>I2V framework.</b> Image condition is incorporated through channel-concat and <i>CLIP image encodings</i>.</figcaption>
</figure>
</p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split"></p>
            <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('Step-Video-TI2V-details')"><i>Step-Video-TI2V Technical Report: A State-of-the-Art Text-Driven Image-to-Video Generation Model</i></p>
            <p class="paper_detail">Step-Video Team</p>
            <p class="paper_detail">StepFun</p>
            <p class="paper_detail"><b><font color=#202020>Mar 14, 2025 &nbsp; Step-Video-TI2V</font></b> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/stepfun-ai/Step-Video-TI2V/">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2503.11251">arXiv 2025</a> &nbsp; <font color=#D0D0D0></font></p>
            
            <div id='Step-Video-TI2V-details' class="info_detail">
                <p class="summary"><b>StepFun</b>'s image-to-video generation model (30B), trained upon Step-Video-T2V, by incorporating conditions of motion and channel-concat image.</p>
                
                <p>
<figure>
    <img src='resource/figs/2025-03-14-Step-Video-TI2V-fig1.png' width=600>
    <figcaption>
        <b>Figure 1.</b> <b>Image condition:</b> channel-concat of <i>noise-augmented</i> image condition.
        <b>Motion condition:</b> optical flow-based motion + timestep. </figcaption>
</figure>
</p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split"></p>
            <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('Seedream2.0-details')"><i>Seedream 2.0: A Native Chinese-English Bilingual Image Generation Foundation Model</i></p>
            <p class="paper_detail">ByteDance's Seed Vision Team</p>
            <p class="paper_detail">ByteDance</p>
            <p class="paper_detail"><b><font color=#202020>Mar 10, 2025 &nbsp; Seedream2.0</font></b>  &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2503.07703">arXiv 2025</a> &nbsp; <font color=#D0D0D0></font></p>
            
            <div id='Seedream2.0-details' class="info_detail">
                <p class="summary"><b>ByteDance Sead Vision Team</b> 's image generation model that employs MMDiT structure and has Chinese-English bilingual capability.</p>
                
                <p>
<ul>
    <li> Use a <i>self-developed bilingual LLM</i> and Glyph-Aligned ByT5 as text encoders.
    <li> Use a <i>self-developed VAE</i>.
    <li> Use learned positional embeddings on text tokens and scaled 2D RoPE on image tokens.
    <li> Training stages: pre-training => continue training => supervised fine-tuning => human feedback alignment.
    <li> Inference stages: user prompt => prompt engineering => text encoding => generation => refinement => output.
    <li> User experience platform: Doubao (豆包) and Dreamina (即梦).
</ul>
<figure>
    <img src='resource/figs/2025-03-10-Seedream2.0-fig1.png' width=700>
    <figcaption><b>Figure 1.</b> Performance with English and Chinese prompts.</figcaption>
</figure>
<figure>
    <img src='resource/figs/2025-03-10-Seedream2.0-fig2.png' width=400>
    <figcaption><b>Figure 2.</b> Pre-training data system.</figcaption>
</figure>
<figure>
    <img src='resource/figs/2025-03-10-Seedream2.0-fig6.png' width=600>
    <figcaption><b>Figure 3.</b> Model structure is similar to MMDiT (SD3).</figcaption>
</figure>
</p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split"></p>
            <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('Step-Video-T2V-details')"><i>Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model</i></p>
            <p class="paper_detail">Step-Video Team</p>
            <p class="paper_detail">StepFun</p>
            <p class="paper_detail"><b><font color=#202020>Feb 14, 2025 &nbsp; Step-Video-T2V</font></b> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/stepfun-ai/Step-Video-T2V/">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2502.10248">arXiv 2025</a> &nbsp; <font color=#D0D0D0></font></p>
            
            <div id='Step-Video-T2V-details' class="info_detail">
                <p class="summary"><b>StepFun</b>'s open-sourced model (30B) with DiT structure for text-to-video generation.</p>
                
                <p>
<figure>
    <img src='resource/figs/2025-02-14-Step-Video-T2V-fig1.png' width=500>
    <figcaption><b>Figure 1.</b> <b>Main structure.</b> a VAE with a 8x8x4 compression ratio and 16 channels, bilingual text encoders (HunyuanCLIP and Step-LLM), DiT with RoPE-3D and QK-Norm, and a DPO pipeline. Text prompt conditions are incorporated into DiT by cross-attention modules.</figcaption>
</figure>
<figure>
    <img src='resource/figs/2025-02-14-Step-Video-T2V-fig2.png' width=400>
    <figcaption><b>Figure 2.</b> <b>VAE</b> compresses videos by 16x16x8 with 16 feature channels.</figcaption>
</figure>
<figure>
    <img src='resource/figs/2025-02-14-Step-Video-T2V-fig4.png' width=500>
    <figcaption><b>Figure 3.</b> <b>DPO framework.</b> use training data prompts and handcrafted prompts to generate samples, which are scored through human annotation or reward models. Diffusion-DPO method is adapted here by reducing beta and increasing learning rate for achieving faster convergence.</figcaption>
</figure>
<figure>
    <img src='resource/figs/2025-02-14-Step-Video-T2V-fig5.png' width=900>
    <figcaption><b>Figure 4.</b> Using <b>2B video-text pairs</b>, <b>3.8B image-text pairs</b>. <i>Filters:</i> video segmentation, video quality assessment, aesthetic score, NSFW score, watermark detection, subtitle detection, saturation score, blur score, black border detection, video motion assessment, K-means-based concept balancing, and CLIP score alignment. <i>Video captioning:</i> short caption, dense caption, and original title.</figcaption>
</figure>
<figure>
    <img src='resource/figs/2025-02-14-Step-Video-T2V-fig7.png' width=600>
    <figcaption><b>Figure 5.</b> Pre-training stages. </figcaption>
</figure>
</p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split"></p>
            <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('Infinity-details')"><i>Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis</i></p>
            <p class="paper_detail">Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, Xiaobing Liu</p>
            <p class="paper_detail">ByteDance</p>
            <p class="paper_detail"><b><font color=#202020>Dec 05, 2024 &nbsp; Infinity</font></b> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/FoundationVision/Infinity/">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2412.04431">CVPR 2025</a> &nbsp; <font color=#D0D0D0></font></p>
            
            <div id='Infinity-details' class="info_detail">
                <p class="summary">It improves VAR by applying <b>bitwise modeling</b> that makes vocabulary "infinity" to open up new posibilities of discrete text-to-image generation.</p>
                
                <p>
<figure>
    <img src='resource/figs/2024-12-05-Infinity-fig1.png' width=500>
    <figcaption><b>Figure 1.</b> <b>Viusal tokenization and quantization:</b> instead of predicting <i>2**d</i> indices, infinite-vocabulary classifier predicts <i>d</i> bits instead.</figcaption>
</figure>
<figure>
    <img src='resource/figs/2024-12-05-Infinity-fig3.png' width=400>
    <figcaption><b>Figure 2.</b> <b>Infinity:</b> it is fast and better.</figcaption>
</figure>
<figure>
    <img src='resource/figs/2024-12-05-Infinity-fig4.png' width=300>
    <figcaption><b>Figure 3.</b> <b>Tokenizer:</b> it outperforms continuous SD VAE.</figcaption>
</figure>
<figure>
    <img src='resource/figs/2024-12-05-Infinity-fig5.png' width=500>
    <figcaption><b>Figure 4.</b> <b>Inifinite-Vocabulary Classifier:</b> it needs low memory but performs better.</figcaption>
</figure>
<figure>
    <img src='resource/figs/2024-12-05-Infinity-fig8.png' width=400>
    <figcaption><b>Figure 5.</b> <b>Self-correction</b> mitigates the train-test discrepancy.</figcaption>
</figure>
<figure>
    <img src='resource/figs/2024-12-05-Infinity-fig6.png' width=700>
    <figcaption><b>Figure 6.</b> <b>Scaling up vocabulary:</b> vocabulary size and model size scale well.</figcaption>
</figure>
<figure>
    <img src='resource/figs/2024-12-05-Infinity-fig7.png' width=750>
    <figcaption><b>Figure 7.</b> <b>Scaling up model size:</b> there is strong correlation between validation loss and evaluation metrics (as observed by Fluid).</figcaption>
</figure>
<figure>
    <img src='resource/figs/2024-12-05-Infinity-fig9.png' width=900>
    <figcaption><b>Figure 8.</b> Using <b>2D RoPE</b> outperforms using APE.</figcaption>
</figure>
</p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split"></p>
            <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('HunyuanVideo-details')"><i>HunyuanVideo: A Systematic Framework For Large Video Generative Models</i></p>
            <p class="paper_detail">Hunyuan Multimodal Generation Team</p>
            <p class="paper_detail">Tencent</p>
            <p class="paper_detail"><b><font color=#202020>Dec 03, 2024 &nbsp; HunyuanVideo</font></b> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/Tencent/HunyuanVideo/">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2412.03603">arXiv 2024</a> &nbsp; <font color=#D0D0D0></font></p>
            
            <div id='HunyuanVideo-details' class="info_detail">
                <p class="summary"><b>Tencent Hunyuan Team</b>'s open-sourced text-to-video and image-to-video generation model (13B) with diffusion transformer (FLUX structure).</p>
                
                <p>
</p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split"></p>
            <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('Movie Gen-details')"><i>Movie Gen: A Cast of Media Foundation Models</i></p>
            <p class="paper_detail">Movie Gen Team</p>
            <p class="paper_detail">Meta</p>
            <p class="paper_detail"><b><font color=#202020>Oct 17, 2024 &nbsp; Movie Gen</font></b>  &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2410.13720">arXiv 2024</a> &nbsp; <font color=#D0D0D0></font></p>
            
            <div id='Movie Gen-details' class="info_detail">
                <p class="summary"><b>Meta Movie Gen Team</b>'s diffusion transformer-based model (30B) for 16s / 1080p / 16fps video and synchronized audio generation.</p>
                
                <p>
</p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split"></p>
            <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('Fluid-details')"><i>Fluid: Scaling Autoregressive Text-to-image Generative Models with Continuous Tokens</i></p>
            <p class="paper_detail">Lijie Fan, Tianhong Li, Siyang Qin, Yuanzhen Li, Chen Sun, Michael Rubinstein, Deqing Sun, Kaiming He, Yonglong Tian</p>
            <p class="paper_detail">Google DeepMind, MIT</p>
            <p class="paper_detail"><b><font color=#202020>Oct 17, 2024 &nbsp; Fluid</font></b>  &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2410.13863">ICLR 2025</a> &nbsp; <font color=#D0D0D0>International Conference on Learning Representations</font></p>
            
            <div id='Fluid-details' class="info_detail">
                <p class="summary">It shows auto-regressive models with <b>continuous tokens beat discrete tokens counterpart</b>, and finds some empirical observations during scaling.</p>
                
                <p>
<figure>
    <img src='resource/figs/2024-10-17-Fluid-fig1.png' width=500>
    <figcaption>
        <b>Figure 1.</b> <b>Image tokenizer:</b> discrete (VQGAN) or continuous (VAE). <b>Text tokenizer:</b> discrete (T5-XXL).
        <b>Model structure:</b> transformer with cross-attention modules attending to text embeddings.
        <b>Loss:</b> cross-entropy loss on text tokens and diffusion loss on image tokens.
    </figcaption>
</figure>
<figure>
    <img src='resource/figs/2024-10-17-Fluid-fig2.png' width=700>
    <figcaption><b>Figure 2.</b> Scaling behavior of validation loss on <b>model size</b>.</figcaption>
</figure>
<figure>
    <img src='resource/figs/2024-10-17-Fluid-fig3.png' width=700>
    <figcaption><b>Figure 3.</b> <b>Random-order masks</b> on <b>continuous image tokens</b> perform the best. Continuous prefers random order, discrete prefers raster order. </figcaption>
</figure>
<figure>
    <img src='resource/figs/2024-10-17-Fluid-fig4.png' width=700>
    <figcaption><b>Figure 4.</b> Random-order masks on continuous tokens scale with <b>training computes</b>.</figcaption>
</figure>
<figure>
    <img src='resource/figs/2024-10-17-Fluid-fig5.png' width=700>
    <figcaption><b>Figure 5.</b> Strong correlation between <b>validation loss</b> and <b>evaluation metrics</b>.</figcaption>
</figure>
</p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split"></p>
            <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('DiT-MoE-details')"><i>Scaling Diffusion Transformers to 16 Billion Parameters</i></p>
            <p class="paper_detail">Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, Junshi Huang</p>
            <p class="paper_detail">Kunlun Inc.</p>
            <p class="paper_detail"><b><font color=#202020>Jul 16, 2024 &nbsp; DiT-MoE</font></b> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/feizc/DiT-MoE/">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2407.11633">arXiv 2024</a> &nbsp; <font color=#D0D0D0></font></p>
            
            <div id='DiT-MoE-details' class="info_detail">
                <p class="summary">It proposes diffusion transformer (16B) with <b>Mixture-of-Experts</b> by inserting experts into DiT blocks for image generation.</p>
                
                <p>
<ul>
    <li> Incorporating <i>shared expert routing</i> improves convergence and performance, but the improvement is little when using more than one.
    <li> Increasing experts reduces loss but introduces more loss spikes.
</ul>
<figure>
    <img src='resource/figs/2024-07-16-DiT-MoE-fig1.png' width=700>
    <figcaption><b>Figure 1.</b> It is built upon DiT and replaces MLP within Transformer blocks by sparsely activated mixture of MLPs as experts.</figcaption>
</figure>
</p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split"></p>
            <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('LlamaGen-details')"><i>Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation</i></p>
            <p class="paper_detail">Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, Zehuan Yuan</p>
            <p class="paper_detail">The University of Hong Kong, ByteDance</p>
            <p class="paper_detail"><b><font color=#202020>Jun 10, 2024 &nbsp; LlamaGen</font></b> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/FoundationVision/LlamaGen/">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2406.06525">arXiv 2024</a> &nbsp; <font color=#D0D0D0></font></p>
            
            <div id='LlamaGen-details' class="info_detail">
                <p class="summary">It shows that applying "next-token prediction" to <b>vanilla autoregressive language models</b> can achieve good  image generation performance.</p>
                
                <p>
<ul>
    <li> It trains a discrete visual tokenizer that is competitive to the continuous ones, e.g., SD VAE, SDXL VAE, Consistency Decoder from OpenAI.
    <li> Vanilla autoregressive models, e.g., LlaMA, without inductive biases on visual signals can serve as the basis of image generation system.
    <li> The model is trained on 50M subset of LAION-COCO and 10M internal high aesthetics quality images.
</ul>
</p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split"></p>
            <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('VAR-details')"><i>Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction</i></p>
            <p class="paper_detail">Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, Liwei Wang</p>
            <p class="paper_detail">Peking University, Bytedance</p>
            <p class="paper_detail"><b><font color=#202020>Apr 03, 2024 &nbsp; VAR</font></b> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/FoundationVision/VAR/">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2404.02905">NeurIPS 2024</a> &nbsp; <font color=#D0D0D0>Advances in Neural Information Processing Systems</font></p>
            <p class="paper_detail"><font color=#FF000>NeurIPS 2024 best paper award.</font></p>
            <div id='VAR-details' class="info_detail">
                <p class="summary">It improves <u>auto-regressive</u> image generation on image quality, inference speed, data efficiency, and scalability, by proposing <b>next-scale prediction</b>.</p>
                
                <p>
<figure>
    <img src='resource/figs/2024-04-03-VAR-fig1.png' width=700>
    <figcaption> <b>Figure 1.</b> <b>Next-scale prediction:</b> start from 1x1 token map; at each step, it predicts the next higher-resolution token map given all previous ones.</figcaption>
</figure>
<figure>
    <img src='resource/figs/2024-04-03-VAR-fig3.png' width=800>
    <figcaption><b>Figure 2.</b> <b>Training pipeline of tokenzier and VAR.</b>  Tokenzier (similar to VQ-VAE): the same architecture and training data (OpenImages), using codebook of 4096 and spatial downsample ratio of 16. VAR: the standard transformer with AdaLN; not use RoPE, SwiGLU MLP, RMS Norm. </figcaption>
</figure>
<figure>
    <img src='resource/figs/2024-04-03-VAR-fig4.png' width=700>
    <figcaption><b>Figure 3.</b> <b>Algorithms of tokenizer:</b> encoding and reconstruction.</figcaption>
</figure>
<figure>
    <img src='resource/figs/2024-04-03-VAR-fig2.png' width=350>
    <figcaption><b>Figure 4.</b> VAR shows good <b>scaling behavior</b>, and significantly outperforms DiT.</figcaption>
</figure>
</p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split"></p>
            <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('SDXL-details')"><i>SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis</i></p>
            <p class="paper_detail">Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, Robin Rombach</p>
            <p class="paper_detail">Stability AI</p>
            <p class="paper_detail"><b><font color=#202020>Jul 04, 2023 &nbsp; SDXL</font></b> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/Stability-AI/generative-models/">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2307.01952">ICLR 2024</a> &nbsp; <font color=#D0D0D0>International Conference on Learning Representations</font></p>
            
            <div id='SDXL-details' class="info_detail">
                <p class="summary">It improves older SD by employing <b>larger UNet backbone</b>, <b>resolution conditions</b>, <b>two text encoders</b>, and a <b>refinement model</b>.</p>
                
                <p>
<p><b>Architecture of SDXL:</b>.<br>
(1) It has 2.6B parameters with different transformer blocks, SD 1.4/1.5/2.0/2.1 has about 860M parameters.<br>
(2) It uses two text encoders: OpenCLIP ViT-bigG & CLIP ViT-L.<br>
(3) The embeddings of height & width and cropping top & left and bucketing heigh & width are added to timestep embeddings as conditions.<br>
(4) It improves VAE by employing EMA and a larger batchsize of 256.<br>
(5) It employs a refinement model of SDEdit to refine visual details.</p>
<p><b>Training stages:</b> (1) reso=256x256, steps=600,000, batchsize=2048; (2) reso=512x512, steps=200,000; (3) mixed resolution and aspect ratio training.</p>
<figure>
    <img src='resource/figs/2023-07-04-SDXL-fig1.png' width=600>
</figure>
</p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split"></p>
            <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('DiT-details')"><i>Scalable Diffusion Models with Transformers</i></p>
            <p class="paper_detail">William Peebles, Saining Xie</p>
            <p class="paper_detail">UC Berkeley, New York University</p>
            <p class="paper_detail"><b><font color=#202020>Dec 19, 2022 &nbsp; DiT</font></b> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/facebookresearch/DiT/">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2212.09748">ICCV 2023</a> &nbsp; <font color=#D0D0D0>International Conference on Computer Vision</font></p>
            
            <div id='DiT-details' class="info_detail">
                <p class="summary">It replaces the conventional U-Net structure with <b>transformer</b> for scalable image generation, the timestep and condition are injected by adaLN-Zero.</p>
                 
                <p><a href="https://github.com/junkunyuan/junkunyuan.github.io/blob/main/paper_reading_list/resource/jupyters/visual_generative_models-dit.ipynb" class="note">(see notes in jupyter)</a></p>
                
                <p>
<figure>
    <img src='resource/figs/2022-12-19-DiT-fig1.png' width=800>
    <figcaption><b>Figure 1.</b> Using adaLN-Zero structure to inject timestep and class condition performs better than using cross-attention or in-context.</figcaption>
</figure>
</p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split"></p>
            <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('CogVideo-details')"><i>CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers</i></p>
            <p class="paper_detail">Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, Jie Tang</p>
            <p class="paper_detail">Tsinghua University, BAAI</p>
            <p class="paper_detail"><b><font color=#202020>May 29, 2022 &nbsp; CogVideo</font></b> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/THUDM/CogVideo/">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2205.15868">ICLR 2023</a> &nbsp; <font color=#D0D0D0>International Conference on Learning Representations</font></p>
            
            <div id='CogVideo-details' class="info_detail">
                <p class="summary">It proposes a transformer-based video generation model (9B) that performs <b>auto-regressive</b> frame  generation and recursive frame interpolatation</p>
                
                <p>
<figure>
    <img src='resource/figs/2022-05-29-cogvideo-fig1.png' width=500>
    <figcaption><b>Figure 1.</b> CogVideo is trained upon CogView2. It generates frames auto-regressively and interpolates them recursively.</figcaption>
</figure>
</p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split"></p>
            <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('LDM-details')"><i>High-Resolution Image Synthesis with Latent Diffusion Models</i></p>
            <p class="paper_detail">Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer</p>
            <p class="paper_detail">Heidelberg University, Runway ML</p>
            <p class="paper_detail"><b><font color=#202020>Dec 20, 2021 &nbsp; LDM</font></b> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/CompVis/latent-diffusion/">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2112.10752">CVPR 2022</a> &nbsp; <font color=#D0D0D0></font></p>
            <p class="paper_detail"><font color=#FF000>It makes high-resolution image synthesis efficiently by performing generation in a compressed VAE latent space. It has over 20,000 citations (as of Jun 29, 2025).</font></p>
            <div id='LDM-details' class="info_detail">
                <p class="summary">It achieves efficient high-resolution image generation by applying diffusion and denoising processes in the <b>compressed VAE latent space</b>.</p>
                
                <p>
<figure>
    <img src='resource/figs/2021-12-20-ldm-fig1.png' width=400>
    <figcaption><b>Figure 1.</b> Diffusion and denoising processes are conducted in the compressed VAE latent space. The conditions are injected by cross-attention.</figcaption>
</figure>
</p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split"></p>
            <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('CFG-details')"><i>Classifier-Free Diffusion Guidance</i></p>
            <p class="paper_detail">Jonathan Ho, Tim Salimans</p>
            <p class="paper_detail">Google Research, Brain team</p>
            <p class="paper_detail"><b><font color=#202020>Dec 08, 2021 &nbsp; CFG</font></b>  &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2207.12598">NeurIPS workshop 2021</a> &nbsp; <font color=#D0D0D0>Advances in Neural Information Processing Systems</font></p>
            
            <div id='CFG-details' class="info_detail">
                <p class="summary">It improves conditional image generation with <b>classifier-free condition guidance</b> by jointly training a conditional model and an unconditional model.</p>
                 
                <p><a href="https://github.com/junkunyuan/junkunyuan.github.io/blob/main/paper_reading_list/resource/jupyters/visual_generative_models-cfg.ipynb" class="note">(see notes in jupyter)</a></p>
                
                <p></p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split"></p>
            <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('DDPM-details')"><i>Denoising Diffusion Probabilistic Models</i></p>
            <p class="paper_detail">Jonathan Ho, Ajay Jain, Pieter Abbeel</p>
            <p class="paper_detail">UC Berkeley</p>
            <p class="paper_detail"><b><font color=#202020>Jun 19, 2020 &nbsp; DDPM</font></b> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/hojonathanho/diffusion/">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2006.11239">NeurIPS 2020</a> &nbsp; <font color=#D0D0D0>Advances in Neural Information Processing Systems</font></p>
            <p class="paper_detail"><font color=#FF000>It achieves high-quality image synthesis through iterative denoising diffusion processes. It has over 20,000 citations (as of Jun 29 2025).</font></p>
            <div id='DDPM-details' class="info_detail">
                <p class="summary">It proposes <b>denoising diffusion probabilistic models</b> that iteratively denoises data from random noise.</p>
                 
                <p><a href="https://github.com/junkunyuan/junkunyuan.github.io/blob/main/paper_reading_list/resource/jupyters/visual_generative_models-ddpm.ipynb" class="note">(see notes in jupyter)</a></p>
                
                <p>
<figure>
    <img src='resource/figs/2020-06-19-ddpm-fig1.png' width=400>
    <figcaption><b>Figure 1.</b> Diffusion (forward) and denoising (reverse) processes of DDPM.</figcaption>
</figure>
<figure>
    <img src='resource/figs/2020-06-19-ddpm-fig2.png' width=600>
    <figcaption><b>Figure 2.</b> Training and sampling algorithms of DDPM.</figcaption>
</figure>
</p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split"></p>
            <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('VQ-VAE-2-details')"><i>Generating Diverse High-Fidelity Images with VQ-VAE-2</i></p>
            <p class="paper_detail">Ali Razavi, Aaron van den Oord, Oriol Vinyals</p>
            <p class="paper_detail">DeepMind</p>
            <p class="paper_detail"><b><font color=#202020>Jun 02, 2019 &nbsp; VQ-VAE-2</font></b>  &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/1906.00446">NeurIPS 2019</a> &nbsp; <font color=#D0D0D0>Advances in Neural Information Processing Systems</font></p>
            
            <div id='VQ-VAE-2-details' class="info_detail">
                <p class="summary">In order to generate large scale images efficiently, it improves VQ-VAE by employing a <b>hierarchical organization</b>.</p>
                
                <p>
<ul>
    <li><b>Structure:</b> (1) a top-level encoder to learn top-level priors from images; (2) a bottom-level encoder to learn bottom-level priors from images and top-level priors; (3) a decoder to generate images from both top-level and bottom-level priors.</li>
    <li><b>Training stage 1:</b> training the top-level encoder and the bottom-level encoder to encode images onto the two levels of discrete latent space.</li>
    <li><b>Training stage 2:</b> training PixelCNN to predict bottom-level priors from top-level priors, while fixing the two encoders.</li>
    <li><b>Sampling:</b> (1) sampling a top-level prior; (2) predicting bottom-level prior from the top-level prior using the trained PixelCNN; (3) generating images from both the top-level and the bottom-level priors by the trained decoder.</li>
</ul>
<figure>
    <img src='resource/figs/2019-06-02-VQ-VAE-2-fig1.png' width=900>
    <figcaption><b>Figure 1.</b> Training and sampling frameworks.</figcaption>
</figure>
<figure>
    <img src='resource/figs/2019-06-02-VQ-VAE-2-fig2.png' width=550>
    <figcaption><b>Figure 2.</b> Training and sampling algorithms.</figcaption>
</figure>
</p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split"></p>
            <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('VQ-VAE-details')"><i>Neural Discrete Representation Learning</i></p>
            <p class="paper_detail">Aaron van den Oord, Oriol Vinyals, Koray Kavukcuoglu</p>
            <p class="paper_detail">DeepMind</p>
            <p class="paper_detail"><b><font color=#202020>Nov 02, 2017 &nbsp; VQ-VAE</font></b>  &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/1711.00937">NeurIPS 2017</a> &nbsp; <font color=#D0D0D0>Advances in Neural Information Processing Systems</font></p>
            
            <div id='VQ-VAE-details' class="info_detail">
                <p class="summary">It proposes <b>vector quantised variational autoencoder</b> to generate discrete codes while the prior is also learned.</p>
                
                <p>
<ul>
    <li><b>Posterior collapse problem:</b> a strong decoder and a strong KL constraint could make the learned posterior <i>q(z|x)</i> very close to prior <i>p(z)</i>, so that the conditional generation task collapses to an unconditional generation task.</li>
    <li><b>How VQ-VAE avoids the collapse problem by employing discrete codes/latents?</b> (1) It learns <i>q(z|x)</i> by choosing one from some candidates rather than directly generating a simple prior; (2) The learned <i>q(z|x)</i> is continuous but <i>p(z)</i> is discrete, so the encoder can not be "lazy".</li>
    <li><b>Optimization objectives:</b> (1) The decoder is optimized by a recontruction loss; (2) The encoder is optimized by a reconstruction loss and a matching loss; (3) The embedding is optimized by a matching loss.</li>
    <li><b>How to back-propagate gradient with quantization exists? Straight-Through Estimator:</b> directly let the graident of loss to the quantized embedding equal to the gradient of loss to the embedding that before being quantized.</li>
</ul>
<figure>
    <img src='resource/figs/2017-11-02-VQ-VAE-fig1.png' width=900>
</figure>
</p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            <h2>Datasets & Evaluation</h2>
            <p class="little_split"></p>
            <div style="border-left: 8px solid #FFD6AD; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('UnifiedReward-details')"><i>Unified Reward Model for Multimodal Understanding and Generation</i></p>
            <p class="paper_detail">Yibin Wang, Yuhang Zang, Hao Li, Cheng Jin, Jiaqi Wang</p>
            <p class="paper_detail">Fudan University, Shanghai Innovation Institute, Shanghai AI Lab, Shanghai Academy of Artificial Intelligence for Science</p>
            <p class="paper_detail"><b><font color=#202020>Mar 07, 2025 &nbsp; UnifiedReward</font></b> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/CodeGoat24/UnifiedReward/">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2503.05236">arXiv 2025</a> &nbsp; <font color=#D0D0D0></font></p>
            
            <div id='UnifiedReward-details' class="info_detail">
                <p class="summary">It fine-tunes LLaVA-OneVision 7B for both <b>multimodal understanding & generation evaluation</b> by pairwise ranking & pointwise scoring.</p>
                
                <p></p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split"></p>
            <div style="border-left: 8px solid #FFD6AD; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('T2I-CompBench-details')"><i>T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation</i></p>
            <p class="paper_detail">Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, Xihui Liu</p>
            <p class="paper_detail">The University of Hong Kong, Huawei Noah's Ark Lab</p>
            <p class="paper_detail"><b><font color=#202020>Jul 12, 2023 &nbsp; T2I-CompBench</font></b> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/Karine-Huang/T2I-CompBench/">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2307.06350">NeurIPS 2023</a> &nbsp; <font color=#D0D0D0>Advances in Neural Information Processing Systems</font></p>
            
            <div id='T2I-CompBench-details' class="info_detail">
                <p class="summary">It uses 6000 prompts to evaluate model capability on compositional generation, including attribute binding, object relationship, complex compositions.</p>
                
                <p>
<ul>
    <li> Attribute binding prompts: at least two objects with two attributes from color, shape, texture.
    <li> Object relationship prompts: at least two objects with spatial relationship or non-spatial relationship.
    <li> Complex compositions prompts: more than two objects or more than two sub-categories.
</ul>
<figure>
    <img src='resource/figs/2023-07-12-t2icompbench-fig1.png' width=700>
    <figcaption><li> <b>Figure 1.</b> Use disentangled BLIP-VQA to evaluate attribute binding, UniDet-based metric to evaluate spatial relationship, CLIPScore to evaluate non-spatial relationship, and 3-in-1 metric (average score of the three metrics) to evaluate complex compositions.</figcaption>
</figure>
</p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split"></p>
            <div style="border-left: 8px solid #FFD6AD; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('HPS v2-details')"><i>Human Preference Score v2: A Solid Benchmark for Evaluating Human Preferences of Text-to-Image Synthesis</i></p>
            <p class="paper_detail">Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, Hongsheng Li</p>
            <p class="paper_detail">CUHK, SenseTime Research, Shanghai Jiao Tong University, Centre for Perceptual and Interactive Intelligence</p>
            <p class="paper_detail"><b><font color=#202020>Jun 15, 2023 &nbsp; HPS v2</font></b> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/tgxs002/HPSv2/">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2306.09341">arXiv 2023</a> &nbsp; <font color=#D0D0D0></font></p>
            
            <div id='HPS v2-details' class="info_detail">
                <p class="summary">It proposes HPD v2: 798K human preferences on 433K pairs of images; HPS v2: fine-tuned CLIP on HPD v2 for image generation evaluation.</p>
                
                <p>
<figure>
    <img src='resource/figs/2023-06-15-hpsv2-fig1.png' width=800>
    <figcaption><b>Figure 1.</b> (1) Clean prompts from COCO captions and DiffusionDB by ChatGPT; (2) Generate images using 9 text-to-image generation models; (3) Rank and annotate each pair of images by humans; (4) Finetune CLIP and obtain a preference model to provide HPS v2 evaluation score.</figcaption>
</figure>
</p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split"></p>
            <div style="border-left: 8px solid #FFD6AD; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('PickScore-details')"><i>Pick-a-Pic: An Open Dataset of User Preferences for Text-to-Image Generation</i></p>
            <p class="paper_detail">Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, Omer Levy</p>
            <p class="paper_detail">Tel Aviv University, Stability AI</p>
            <p class="paper_detail"><b><font color=#202020>May 02, 2023 &nbsp; PickScore</font></b> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/yuvalkirstain/PickScore/">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2305.01569">NeurIPS 2023</a> &nbsp; <font color=#D0D0D0>Advances in Neural Information Processing Systems</font></p>
            
            <div id='PickScore-details' class="info_detail">
                <p class="summary">Pick-a-Pic: use a web app to collect user preferences; PickScore: train a CLIP-based model on preference data for image generation evaluation.</p>
                
                <p>
</p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split"></p>
            <div style="border-left: 8px solid #FFD6AD; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('ImageReward-details')"><i>ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation</i></p>
            <p class="paper_detail">Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, Yuxiao Dong</p>
            <p class="paper_detail">Tsinghua University, Zhipu AI, Beijing University of Posts and Telecommunications</p>
            <p class="paper_detail"><b><font color=#202020>Apr 12, 2023 &nbsp; ImageReward</font></b> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/THUDM/ImageReward/">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2304.05977">NeurIPS 2023</a> &nbsp; <font color=#D0D0D0>Advances in Neural Information Processing Systems</font></p>
            
            <div id='ImageReward-details' class="info_detail">
                <p class="summary">It trains BLIP on 137K human preference image pairs for image generation and use it to tune diffusion models by Reward Feedback Learning (ReFL).</p>
                
                <p>
<figure>
    <img src='resource/figs/2023-04-12-imagereward-fig1.png' width=600>
    <figcaption><b>Figure 1.</b> (1) use DiffusionDB prompts to generate images; (2) Rate and rank; (3) Train ImageReward using ranking data; (4) tune models via ReFL.</figcaption>
</figure>
</p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split"></p>
            <div style="border-left: 8px solid #FFD6AD; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('HPS-details')"><i>Human Preference Score: Better Aligning Text-to-Image Models with Human Preference</i></p>
            <p class="paper_detail">Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, Hongsheng Li</p>
            <p class="paper_detail">CUHK, SenseTime Research, Shanghai Jiao Tong University, Centre for Perceptual and Interactive Intelligence, Shanghai AI Lab</p>
            <p class="paper_detail"><b><font color=#202020>Mar 25, 2023 &nbsp; HPS</font></b> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/tgxs002/align_sd/">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2303.14420">ICCV 2023</a> &nbsp; <font color=#D0D0D0>International Conference on Computer Vision</font></p>
            
            <div id='HPS-details' class="info_detail">
                <p class="summary">It fine-tunes CLIP on annotated 98K SD generated images from 25K prompts for image generation evaluation.</p>
                
                <p>
<figure>
    <img src='resource/figs/2023-03-25-hps-fig1.png' width=650>
    <figcaption><b>Figure 1.</b> <b>Train score model:</b> the same as CLIP except for the sample with the highest preference is taken as the positive; <b>Finetune image generation model using the score model:</b> append a special token to the prompts of worse images for training; remove that token during inference.</figcaption>
</figure>
</p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split"></p>
            <div style="border-left: 8px solid #FFD6AD; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('CLIPScore-details')"><i>CLIPScore: A Reference-free Evaluation Metric for Image Captioning</i></p>
            <p class="paper_detail">Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, Yejin Choi</p>
            <p class="paper_detail">Allen Institute for AI, University of Washington</p>
            <p class="paper_detail"><b><font color=#202020>Apr 18, 2021 &nbsp; CLIPScore</font></b> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/jmhessel/clipscore">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2104.08718">EMNLP 2021</a> &nbsp; <font color=#D0D0D0></font></p>
            
            <div id='CLIPScore-details' class="info_detail">
                <p class="summary">It proposes a reference-free metric mainly focusing on semantic alignment for image generation evaluation.</p>
                
                <p>
<ul>
    <li> CLIPScore calculates the cosine similarity between a caption and an image, multiplying the result by 2.5 (some use 1.).
    <li> CLIPScore is sensitive to adversarially constructed image captions.
    <li> CLIPScore generalizes well on never-before-seen images.
    <li> CLIPScore frees from the shortcomings of n-gram matching that disfavors good captions with new words and favors captions with familiar words.
</ul>
</p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split"></p>
            <div style="border-left: 8px solid #FFD6AD; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('FVD-details')"><i>FVD: A new Metric for Video Generation</i></p>
            <p class="paper_detail">Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphaël Marinier, Marcin Michalski, Sylvain Gelly</p>
            <p class="paper_detail">Johannes Kepler University, IDSIA, Google Brain</p>
            <p class="paper_detail"><b><font color=#202020>May 04, 2019 &nbsp; FVD</font></b>  &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://openreview.net/pdf?id=rylgEULtdN">ICLR workshop 2019</a> &nbsp; <font color=#D0D0D0>International Conference on Learning Representations</font></p>
            
            <div id='FVD-details' class="info_detail">
                <p class="summary">Extend FID for <u>video generation</u> evaluation by replacing 2D InceptionNet with pre-trained Inflated 3D convnet.</p>
                
                <p>
</p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split"></p>
            <div style="border-left: 8px solid #FFD6AD; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('FID-details')"><i>GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium</i></p>
            <p class="paper_detail">Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Sepp Hochreiter</p>
            <p class="paper_detail">Johannes Kepler University Linz</p>
            <p class="paper_detail"><b><font color=#202020>Jun 26, 2017 &nbsp; FID</font></b>  &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/1706.08500">NeurIPS 2017</a> &nbsp; <font color=#D0D0D0>Advances in Neural Information Processing Systems</font></p>
            
            <div id='FID-details' class="info_detail">
                <p class="summary">Calculate <b>Fréchet distance</b> between Gaussian distributions of InceptionNet features of real-world and synthetic data for image generation evaluation.</p>
                
                <p>
</p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split"></p>
            <div style="border-left: 8px solid #FFD6AD; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('Inception Score-details')"><i>Improved Techniques for Training GANs</i></p>
            <p class="paper_detail">Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen</p>
            <p class="paper_detail">OpenAI</p>
            <p class="paper_detail"><b><font color=#202020>Jun 10, 2016 &nbsp; Inception Score</font></b> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/openai/improved-gan/">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/1606.03498">NeurIPS 2016</a> &nbsp; <font color=#D0D0D0>Advances in Neural Information Processing Systems</font></p>
            
            <div id='Inception Score-details' class="info_detail">
                <p class="summary">Calculate <b>KL divergence between p(y|x) and p(y)</b> that aims to minimize the entropy across predictions and maximize the entropy across predictions of classes for image generation evaluation.</p>
                
                <p>
</p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
    <script>
        function toggleTable(tableId) {
            const container = document.getElementById(tableId);
            const button = container.previousElementSibling;
            const isVisible = window.getComputedStyle(container).display !== 'none';
            if (!isVisible) {
                const images = container.querySelectorAll('.lazy-load');
                images.forEach(img => {
                    if (!img.src && img.dataset.src) {
                        img.src = img.dataset.src;
                    }
                });
                container.style.display = 'block';
                
            } else {
                container.style.display = 'none';
                
            }
        }
    </script>
    
</body>
</html>
