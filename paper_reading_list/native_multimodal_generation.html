
<!DOCTYPE html>
<html>
<head>
    <link rel="stylesheet" href="resource/html.css" type="text/css">
    <link rel="shortcut icon" href="resource/my_photo.jpg">
    <title>Paper Reading List</title>
    <meta name="description" content="Paper Reading List">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <div id="layout-content" style="margin-top:25px">
<body>

<h1 id="top">Native Multimodal Generation</h1>
<p><b><font size=3><font color='#D93053'>1</font> papers on Native Multimodal Generation.</font></b></p>
<p>Curated by <a href="https://junkunyuan.github.io/">Junkun Yuan</a>.</p>
<p>Click <a href="paper_reading_list.html">here</a> to go back to main contents.</p>
<p><font color=#B0B0B0>Last updated on July 01, 2025 at 19:44 (UTC+8).</font></p>
<hr><p id='table'><b>Table of contents:</b></p><ul><li><a class="no_dec" href="#Foundation Algorithms & Models-table">Foundation Algorithms & Models</a></li></ul><h2 id="Foundation Algorithms & Models-table"><a class="no_dec" href="#top">Foundation Algorithms & Models</a></h2>
            <p class="little_split"></p>
            <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('UniFluid-Foundation Algorithms & Models-details')"><i>Unified Autoregressive Visual Generation and Understanding with Continuous Tokens</i></p>
            <p class="paper_detail">Lijie Fan, Luming Tang, Siyang Qin, Tianhong Li, Xuan Yang, Siyuan Qiao, Andreas Steiner, Chen Sun, Yuanzhen Li, Tao Zhu, Michael Rubinstein, Michalis Raptis, Deqing Sun, Radu Soricut</p>
            <p class="paper_detail">Google DeepMind, MIT</p>
            <p class="paper_detail"><b><font color=#202020>Mar 17, 2025 &nbsp; UniFluid</font></b>  &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2503.13436">arXiv 2025</a> &nbsp; <font color=#D0D0D0></font></p>
            
            <div id='UniFluid-Foundation Algorithms & Models-details' class="info_detail">
                <p class="summary">It achieves visual generation and understanding by applying diffusion loss on continuous visual tokens and cross-entropy loss on discrete text tokens.</p>
                
                <p>
<figure>
    <img src='resource/figs/2025-03-17-UniFluid-fig1.png' width=500>
    <figcaption><b>Figure 1.</b> <b>Framework:</b> joint training of visual generation and understanding tasks through next-token prediction. <b>Tokenizer:</b> use VAE to provide tokens for visual generation, use SigLIP to provide tokens for visual understanding, use SentencePiece to provide text tokens. <b>Prediction head:</b> use <i>modality-specific prediction heads</i> to calculate losses and sampling for each modality. <b>Loss:</b> image understanding loss on text answer + image generation loss on image tokens. <b>Training details:</b> batchsize=2048, optimizer=AdamW, lr=1e-4, steps=1M, init_ckpt=Gemma-2.
    </figcaption>
</figure>
<figure>
    <img src='resource/figs/2025-03-17-UniFluid-fig3.png' width=800>
    <figcaption><b>Figure 2.</b> There is <b>trade-off</b> between generation & understanding.</figcaption>
</figure>
<figure>
    <img src='resource/figs/2025-03-17-UniFluid-fig2.png' width=250>
    <figcaption><b>Figure 3.</b> <b>Unified training improves generation.</b></figcaption>
</figure>
<figure>
    <img src='resource/figs/2025-03-17-UniFluid-fig4.png' width=500>
    <figcaption><b>Figure 4.</b> <b>Better pre-trained LLM backbone</b> leads to better visual generation and understanding performance.</figcaption>
</figure>
</p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
    <script>
        function toggleTable(tableId) {
            const container = document.getElementById(tableId);
            const button = container.previousElementSibling;
            const isVisible = window.getComputedStyle(container).display !== 'none';
            if (!isVisible) {
                const images = container.querySelectorAll('.lazy-load');
                images.forEach(img => {
                    if (!img.src && img.dataset.src) {
                        img.src = img.dataset.src;
                    }
                });
                container.style.display = 'block';
                
            } else {
                container.style.display = 'none';
                
            }
        }
    </script>
    
</body>
</html>
