
<!DOCTYPE html>
<html>
<head>
<link rel="stylesheet" href="resource/html.css" type="text/css">
<link rel="shortcut icon" href="resource/my_photo.jpg">
<title>Paper Reading List</title>
<meta name="description" content="Paper Reading List">
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<div id="layout-content" style="margin-top:25px">
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.24.1/prism.min.js"></script>
            <link href="https://cdn.jsdelivr.net/npm/prismjs@1.24.1/themes/prism.css" rel="stylesheet">
            <script src="https://cdn.jsdelivr.net/npm/prismjs@1.24.1/components/prism-bash.min.js"></script>
            <script src="https://cdn.jsdelivr.net/npm/prismjs@1.24.1/components/prism-python.min.js"></script>
            <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
            <body>

<h1 id="top">Native Multimodal Generation</h1>
<p class="larger"><b>Integrate and generate multiple modalities (e.g., text, images, and videos) within a unified model.</b></p>
<p class="larger"><b><font color='#D93053'>4</font> papers</b></p>
<p>Curated by <a href="https://junkunyuan.github.io/">Junkun Yuan</a>.</p>
<p>Click <a href="paper_reading_list.html">here</a> to go back to main contents.</p>
<p><font color=#B0B0B0>Last updated on August 14, 2025 at 22:28 (UTC+8).</font></p>
<hr><p id='table' class="huger"><b>Table of contents:</b></p><p>Papers are displayed chronologically, some important or inspiring works are highlighted in <font color="#B04040">red</font>.</p><ul><li><a class="no_dec larger" id="Foundation Algorithms & Models" href="#Foundation Algorithms & Models-table"><b>Foundation Algorithms & Models</b></a></li><p><a class="no_dec" href="#Transfusionfoundation algorithms & models"><font color=#B04040>Transfusion</font></a> <font color=#B0B0B0>&nbsp;|&nbsp;</font> <a class="no_dec" href="#GPT-4ofoundation algorithms & models"><font color=#B04040>GPT-4o</font></a> <font color=#B0B0B0>&nbsp;|&nbsp;</font> <a class="no_dec" href="#UniFluidfoundation algorithms & models"><font color=#404040>UniFluid</font></a> <font color=#B0B0B0>&nbsp;|&nbsp;</font> <a class="no_dec" href="#BLIP3-ofoundation algorithms & models"><font color=#404040>BLIP3-o</font></a></p></ul><h2 id="Foundation Algorithms & Models-table"><a class="no_dec" href="#Foundation Algorithms & Models">Foundation Algorithms & Models</a></h2>
            <p class="little_split" id='BLIP3-ofoundation algorithms & models'></p>
            <div style="border-left: 14px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('BLIP3-o-Foundation Algorithms & Models-details')"><i>BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture, Training and Dataset</i></p>
            <p class="paper_detail">Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, Le Xue, Caiming Xiong, Ran Xu</p>
            <p class="paper_detail">Salesforce Research, University of Maryland, Virginia Tech, New York University, University of Washington, UC Davis</p>
            <p class="paper_detail"><b>May 14, 2025 &nbsp; <font color=#404040>BLIP3-o</font></b> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/JiuhaiChen/BLIP3o">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2505.09568">arXiv 2025</a> &nbsp; <font color=#D0D0D0>arXiv preprint</font></p>
            
            
            <div id='BLIP3-o-Foundation Algorithms & Models-details' class="info_detail">
                <hr class="dashed">
                <p><font color=#202020>It finds it is beneficial to generate CLIP features by employing flow matching loss, and use sequential training of understanding and generation.</font></p>
                
                <p><ul>
<li> <b>Structure.</b> Use Qwen 2.5-VL-7B-Instruct and freeze it, and train a 1.4B diffusion transformer (Lumina-Next) on it.
<li> <b>Data.</b> Pre-training data: 25M open-source data and 30M proprietary data, with captions generated by Qwen 2.5-VL. Instruction tuning data: 60K.
</ul>
<figure>
<img data-src='resource\figs\BLIP3-o\BLIP3-o-fig1.png' width=550>
<figcaption>
<b>Figure 1.</b> <b>Structure.</b> It unifies the visual understanding and generation by using CLIP encoder.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\BLIP3-o\BLIP3-o-fig2.png' width=550>
<figcaption>
<b>Figure 2.</b> <b>Design choices</b> on image generation in unified multimodal model.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\BLIP3-o\BLIP3-o-fig3.png' width=600>
<figcaption>
<b>Figure 3.</b> Performance on different <b>design choices</b>. CLIP + Flow Matching is a better choice.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\BLIP3-o\BLIP3-o-fig4.png' width=600>
<figcaption>
<b>Figure 4.</b> <b>Joint training vs. sequential training.</b>
</figcaption>
</figure></p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split" id='UniFluidfoundation algorithms & models'></p>
            <div style="border-left: 14px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('UniFluid-Foundation Algorithms & Models-details')"><i>Unified Autoregressive Visual Generation and Understanding with Continuous Tokens</i></p>
            <p class="paper_detail">Lijie Fan, Luming Tang, Siyang Qin, Tianhong Li, Xuan Yang, Siyuan Qiao, Andreas Steiner, Chen Sun, Yuanzhen Li, Tao Zhu, Michael Rubinstein, Michalis Raptis, Deqing Sun, Radu Soricut</p>
            <p class="paper_detail">Google DeepMind, MIT</p>
            <p class="paper_detail"><b>Mar 17, 2025 &nbsp; <font color=#404040>UniFluid</font></b>  &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2503.13436">arXiv 2025</a> &nbsp; <font color=#D0D0D0>arXiv preprint</font></p>
            
            
            <div id='UniFluid-Foundation Algorithms & Models-details' class="info_detail">
                <hr class="dashed">
                <p><font color=#202020>It achieves visual generation and understanding by applying diffusion loss on continuous visual tokens and cross-entropy loss on discrete text tokens.</font></p>
                
                <p><figure>
<img data-src='resource\figs\UniFluid\UniFluid-fig1.png' width=500>
<figcaption>
<b>Figure 1.</b> <b>Framework:</b> joint training of visual generation and understanding tasks through next-token prediction. <b>Tokenizer:</b> use VAE to provide tokens for visual generation, use SigLIP to provide tokens for visual understanding, use SentencePiece to provide text tokens. <b>Prediction head:</b> use <i>modality-specific prediction heads</i> to calculate losses and sampling for each modality. <b>Loss:</b> image understanding loss on text answer + image generation loss on image tokens. <b>Training details:</b> batchsize=2048, optimizer=AdamW, lr=1e-4, steps=1M, init_ckpt=Gemma-2.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\UniFluid\UniFluid-fig3.png' width=800>
<figcaption>
<b>Figure 2.</b> There is <b>trade-off</b> between generation & understanding.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\UniFluid\UniFluid-fig2.png' width=250>
<figcaption>
<b>Figure 3.</b> <b>Unified training improves generation.</b>
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\UniFluid\UniFluid-fig4.png' width=500>
<figcaption>
<b>Figure 4.</b> <b>Better pre-trained LLM backbone</b> leads to better visual generation and understanding performance.
</figcaption>
</figure></p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split" id='GPT-4ofoundation algorithms & models'></p>
            <div style="border-left: 14px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('GPT-4o-Foundation Algorithms & Models-details')"><i>GPT-4o System Card</i></p>
            
            <p class="paper_detail">OpenAI</p>
            <p class="paper_detail"><b>Oct 25, 2024 &nbsp; <font color=#B04040>GPT-4o</font></b>  &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2410.21276?">arXiv 2024</a> &nbsp; <font color=#D0D0D0>arXiv preprint</font></p>
            
            
            <div id='GPT-4o-Foundation Algorithms & Models-details' class="info_detail">
                <hr class="dashed">
                <p><font color=#202020>It proposes a unified autoregressive model trained end-to-end across text, vision, and audio.</font></p>
                
                <p><figure>
<img data-src='resource\figs\GPT-4o\GPT-4o-fig1.png' width=500>
<img data-src='resource\figs\GPT-4o\GPT-4o-fig2.png' width=500>
<figcaption>
<b>Figure 1.</b> <b>Visual generation capability of GPT-4o evaluated by <a href="https://arxiv.org/pdf/2504.05979">this paper</a>.</b> <i>Text rendering:</i> spelling, alignment, formatting in document. <i>Compositional generation and prompt following:</i> assemble complex scene elements, styles, attributes. <i>Geometric consistency and viewpoint realism:</i> 3D view synthesis, camera control, depth-conditioned rendering. <i>Comprehensive image transformation:</i> from low-level to high-level tasks.
</figcaption>
</figure></p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split" id='Transfusionfoundation algorithms & models'></p>
            <div style="border-left: 14px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('Transfusion-Foundation Algorithms & Models-details')"><i>Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model</i></p>
            <p class="paper_detail">Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, Omer Levy</p>
            <p class="paper_detail">Meta, Waymo, University of Southern California</p>
            <p class="paper_detail"><b>Aug 20, 2024 &nbsp; <font color=#B04040>Transfusion</font></b>  &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2408.11039">ICLR 2025</a> &nbsp; <font color=#D0D0D0>International Conference on Learning Representations</font></p>
            
            
            <div id='Transfusion-Foundation Algorithms & Models-details' class="info_detail">
                <hr class="dashed">
                <p><font color=#202020>It trains a unified model (7B) on 2T multi-modal tokens by predicting discrete text tokens and diffusing continuous image tokens.</font></p>
                
                <p><ul>
<li> <b>Data.</b> Use total 2T tokens from: (1) Llama 2 tokenizer and corpus (2T tokens), (2) 380M Shutterstock images and captions (resized to 256x256).
<li> <b>Structure.</b> It applies next-token prediction on discrete text tokens and diffusion loss on continuous image tokens: <i>L=L_LM+lambda*L_diffusion</i>. It uses <i>modality-specific</i> components with unshared parameters: embedding layer for text, and VAE (U-Net or linear structure, 8x8-8c) with linear or up/down blocks for images. It applies causal mask on text tokens and bidirectional mask on image tokens.
<li> <b>Training details.</b> Optimizer=AdamW, lr=3e-4, 250K steps, lambda=5, train_timesteps=1000, infer_timesteps=250, cfg=3.
<li> <b>Performance.</b> In text-to-image generation task, Transfusion exceeds Chameleon at less than a third of the compute. In image-to-text generation task, Transfusion exceeds Chameleon at 21.8% of the FLOPs. In text-to-text generation task, Transfusion exceeds Chameleon at 50% of FLOPs.
</ul>
<figure>
<img data-src='resource\figs\Transfusion\Transfusion-fig1.png' width=500>
<figcaption>
<b>Figure 1.</b> <b>Transfusion structure.</b>
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Transfusion\Transfusion-fig2.png' width=500>
<figcaption>
<b>Figure 2.</b> Transfusion outperforms Chameleon while <b>scaling</b>.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Transfusion\Transfusion-fig3.png' width=400>
<figcaption>
<b>Figure 3.</b> Transfusion outperforms Chameleon by using few <b>FLOPs</b>, both are 7B.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Transfusion\Transfusion-fig4.png' width=500>
<figcaption>
<b>Figure 4.</b> Transfusion achieves competitive results compared with <b>Llama2</b>.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Transfusion\Transfusion-fig5.png' width=400>
<figcaption>
<b>Figure 5.</b> <b>Encoder:</b> U-Net is better than linear (maybe due to it brings more inductive bias). <b>Attention:</b> bidirectional is better than causal.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Transfusion\Transfusion-fig6.png' width=500>
<figcaption>
<b>Figure 6.</b> <b>Small patch size</b> leads to better performance by providing more visual tokens.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Transfusion\Transfusion-fig7.png' width=500>
<figcaption>
<b>Figure 7.</b> <b>Overall performance.</b>
</figcaption>
</figure></p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
    <script>
        function toggleTable(tableId) {
            const container = document.getElementById(tableId);
            const button = container.previousElementSibling;
            const isVisible = window.getComputedStyle(container).display !== 'none';
            if (!isVisible) {
                const images = container.querySelectorAll('img');
                images.forEach(img => {
                    if (img.dataset.src !== '') {img.src = img.dataset.src;}
                });
                container.style.display = 'block';
            } else {container.style.display = 'none';}
        }
    </script>
    
<button id="backToTop" title="back to top">↑</button>
<script>
    const button = document.getElementById("backToTop");
    window.addEventListener("scroll", () => {
    if (document.documentElement.scrollTop > 300) {
        button.style.display = "block";
    } else {
        button.style.display = "none";
    }
    });

    function updateButtonPosition() {
    const bodyRect = document.body.getBoundingClientRect();
    const windowWidth = window.innerWidth;
    const rightOffset = Math.max((windowWidth - bodyRect.width) / 2, 10);
    button.style.right = rightOffset + "px";
    }

    window.addEventListener("resize", updateButtonPosition);
    window.addEventListener("load", updateButtonPosition);

    button.addEventListener("click", () => {
    window.scrollTo({ top: 0, behavior: 'smooth' });
    });
</script>

</body>
</html>
