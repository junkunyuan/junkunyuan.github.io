<!DOCTYPE html>
<html>
<head>
    <link rel="stylesheet" href="resource/html.css" type="text/css">
    <link rel="shortcut icon" href="resource/my_photo.jpg">
    <title>Paper Reading List</title>
    <meta name="description" content="Paper Reading List">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <div id="layout-content" style="margin-top:25px">
</head>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.24.1/prism.min.js"></script>
        <link href="https://cdn.jsdelivr.net/npm/prismjs@1.24.1/themes/prism.css" rel="stylesheet">
        <script src="https://cdn.jsdelivr.net/npm/prismjs@1.24.1/components/prism-bash.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/prismjs@1.24.1/components/prism-python.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <body>

<h1 id="top">Computer Vision (CV)</h1>
<p class="larger"><b>Understand and generate visual information such as images and videos.</b></p>
<p class="larger"><b><font color='#C55253'>143</font> papers</b></p>
<p>Written by <a href="https://junkunyuan.github.io/">Junkun Yuan</a>.</p>
<p>Click <a href="paper_reading_list.html">here</a> to go back to main contents.</p>
<p><font color=#B0B0B0>Last updated on October 03, 2025 at 11:05 (UTC+8).</font></p>
<hr><p id='table' class="huger"><b>Table of contents:</b></p><p>Papers are displayed in reverse chronological order. Some highly-impact or inspiring works are highlighted in <font color="#C55253">red</font>.</p><ul><li><a class="larger low_margin no_dec" style="font-family: 'Arial, sans-serif'; color: #6D94C5;" id="Foundation Algorithms & Models of Understanding" href="#Foundation Algorithms & Models of Understanding-table"><b>Foundation Algorithms & Models of Understanding</b></a></li>
    <p style="display: flex; flex-wrap: wrap; font-size: 13px;">
        <a href="#MAEfoundation algorithms & models of understanding" class="no_dec"><font color=#C55253><b>MAE</b> <font style="color:#AAAAAA;font-size:11px;">(CVPR 2022)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#BEiTfoundation algorithms & models of understanding" class="no_dec"><font color=#C55253><b>BEiT</b> <font style="color:#AAAAAA;font-size:11px;">(ICLR 2022)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#MoCo v3foundation algorithms & models of understanding" class="no_dec"><font color=#C55253><b>MoCo v3</b> <font style="color:#AAAAAA;font-size:11px;">(ICCV 2021)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#SimSiamfoundation algorithms & models of understanding" class="no_dec"><font color=#C55253><b>SimSiam</b> <font style="color:#AAAAAA;font-size:11px;">(CVPR 2021)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#MoCofoundation algorithms & models of understanding" class="no_dec"><font color=#C55253><b>MoCo</b> <font style="color:#AAAAAA;font-size:11px;">(CVPR 2020)</font></font></a>
    </p>
    <br><li><a class="larger low_margin no_dec" style="font-family: 'Arial, sans-serif'; color: #6D94C5;" id="Foundation Algorithms & Models of Generation" href="#Foundation Algorithms & Models of Generation-table"><b>Foundation Algorithms & Models of Generation</b></a></li>
    <p style="display: flex; flex-wrap: wrap; font-size: 13px;">
        <a href="#Video Zero-shotfoundation algorithms & models of generation" class="no_dec"><font color=#777777><b>Video Zero-shot</b> <font style="color:#AAAAAA;font-size:11px;">(arXiv 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#Seedream 4.0foundation algorithms & models of generation" class="no_dec"><font color=#777777><b>Seedream 4.0</b> <font style="color:#AAAAAA;font-size:11px;">(arXiv 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#Qwen-Imagefoundation algorithms & models of generation" class="no_dec"><font color=#777777><b>Qwen-Image</b> <font style="color:#AAAAAA;font-size:11px;">(arXiv 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#Lumos-1foundation algorithms & models of generation" class="no_dec"><font color=#777777><b>Lumos-1</b> <font style="color:#AAAAAA;font-size:11px;">(arXiv 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#Magi-1foundation algorithms & models of generation" class="no_dec"><font color=#777777><b>Magi-1</b> <font style="color:#AAAAAA;font-size:11px;">(arXiv 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#SimpleARfoundation algorithms & models of generation" class="no_dec"><font color=#777777><b>SimpleAR</b> <font style="color:#AAAAAA;font-size:11px;">(arXiv 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#Seedream 3.0foundation algorithms & models of generation" class="no_dec"><font color=#777777><b>Seedream 3.0</b> <font style="color:#AAAAAA;font-size:11px;">(arXiv 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#Seaweed-7Bfoundation algorithms & models of generation" class="no_dec"><font color=#777777><b>Seaweed-7B</b> <font style="color:#AAAAAA;font-size:11px;">(arXiv 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#Wanfoundation algorithms & models of generation" class="no_dec"><font color=#777777><b>Wan</b> <font style="color:#AAAAAA;font-size:11px;">(arXiv 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#Step-Video-TI2Vfoundation algorithms & models of generation" class="no_dec"><font color=#777777><b>Step-Video-TI2V</b> <font style="color:#AAAAAA;font-size:11px;">(arXiv 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#Seedream2.0foundation algorithms & models of generation" class="no_dec"><font color=#777777><b>Seedream2.0</b> <font style="color:#AAAAAA;font-size:11px;">(arXiv 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#uEDMfoundation algorithms & models of generation" class="no_dec"><font color=#777777><b>uEDM</b> <font style="color:#AAAAAA;font-size:11px;">(ICML 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#Step-Video-T2Vfoundation algorithms & models of generation" class="no_dec"><font color=#777777><b>Step-Video-T2V</b> <font style="color:#AAAAAA;font-size:11px;">(arXiv 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#Flow Matching Guidefoundation algorithms & models of generation" class="no_dec"><font color=#777777><b>Flow Matching Guide</b> <font style="color:#AAAAAA;font-size:11px;">(arXiv 2024)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#Infinityfoundation algorithms & models of generation" class="no_dec"><font color=#777777><b>Infinity</b> <font style="color:#AAAAAA;font-size:11px;">(CVPR 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#HunyuanVideofoundation algorithms & models of generation" class="no_dec"><font color=#777777><b>HunyuanVideo</b> <font style="color:#AAAAAA;font-size:11px;">(arXiv 2024)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#Movie Genfoundation algorithms & models of generation" class="no_dec"><font color=#777777><b>Movie Gen</b> <font style="color:#AAAAAA;font-size:11px;">(arXiv 2024)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#Fluidfoundation algorithms & models of generation" class="no_dec"><font color=#777777><b>Fluid</b> <font style="color:#AAAAAA;font-size:11px;">(ICLR 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#DiT-MoEfoundation algorithms & models of generation" class="no_dec"><font color=#777777><b>DiT-MoE</b> <font style="color:#AAAAAA;font-size:11px;">(arXiv 2024)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#LlamaGenfoundation algorithms & models of generation" class="no_dec"><font color=#777777><b>LlamaGen</b> <font style="color:#AAAAAA;font-size:11px;">(arXiv 2024)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#VARfoundation algorithms & models of generation" class="no_dec"><font color=#C55253><b>VAR</b> <font style="color:#AAAAAA;font-size:11px;">(NeurIPS 2024)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#SDXLfoundation algorithms & models of generation" class="no_dec"><font color=#C55253><b>SDXL</b> <font style="color:#AAAAAA;font-size:11px;">(ICLR 2024)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#DiTfoundation algorithms & models of generation" class="no_dec"><font color=#C55253><b>DiT</b> <font style="color:#AAAAAA;font-size:11px;">(ICCV 2023)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#Flow Matchingfoundation algorithms & models of generation" class="no_dec"><font color=#C55253><b>Flow Matching</b> <font style="color:#AAAAAA;font-size:11px;">(ICLR 2023)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#Unified Perspectivefoundation algorithms & models of generation" class="no_dec"><font color=#777777><b>Unified Perspective</b> <font style="color:#AAAAAA;font-size:11px;">(arXiv 2022)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#CogVideofoundation algorithms & models of generation" class="no_dec"><font color=#777777><b>CogVideo</b> <font style="color:#AAAAAA;font-size:11px;">(ICLR 2023)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#LDMfoundation algorithms & models of generation" class="no_dec"><font color=#C55253><b>LDM</b> <font style="color:#AAAAAA;font-size:11px;">(CVPR 2022)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#CFGfoundation algorithms & models of generation" class="no_dec"><font color=#C55253><b>CFG</b> <font style="color:#AAAAAA;font-size:11px;">(NeurIPS workshop 2021)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#DDIMfoundation algorithms & models of generation" class="no_dec"><font color=#C55253><b>DDIM</b> <font style="color:#AAAAAA;font-size:11px;">(ICLR 2021)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#DDPMfoundation algorithms & models of generation" class="no_dec"><font color=#C55253><b>DDPM</b> <font style="color:#AAAAAA;font-size:11px;">(NeurIPS 2020)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#VQ-VAE-2foundation algorithms & models of generation" class="no_dec"><font color=#C55253><b>VQ-VAE-2</b> <font style="color:#AAAAAA;font-size:11px;">(NeurIPS 2019)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#VQ-VAEfoundation algorithms & models of generation" class="no_dec"><font color=#C55253><b>VQ-VAE</b> <font style="color:#AAAAAA;font-size:11px;">(NeurIPS 2017)</font></font></a>
    </p>
    <br><li><a class="larger low_margin no_dec" style="font-family: 'Arial, sans-serif'; color: #6D94C5;" id="Reinforcement Learning of Generation" href="#Reinforcement Learning of Generation-table"><b>Reinforcement Learning of Generation</b></a></li>
    <p style="display: flex; flex-wrap: wrap; font-size: 13px;">
        <a href="#RDPOreinforcement learning of generation" class="no_dec"><font color=#777777><b>RDPO</b> <font style="color:#AAAAAA;font-size:11px;">(arXiv 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#D-Fusionreinforcement learning of generation" class="no_dec"><font color=#777777><b>D-Fusion</b> <font style="color:#AAAAAA;font-size:11px;">(ICML 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#DanceGRPOreinforcement learning of generation" class="no_dec"><font color=#777777><b>DanceGRPO</b> <font style="color:#AAAAAA;font-size:11px;">(arXiv 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#InPOreinforcement learning of generation" class="no_dec"><font color=#777777><b>InPO</b> <font style="color:#AAAAAA;font-size:11px;">(CVPR 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#Survey on Pre. Ali.reinforcement learning of generation" class="no_dec"><font color=#777777><b>Survey on Pre. Ali.</b> <font style="color:#AAAAAA;font-size:11px;">(arXiv 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#CaPOreinforcement learning of generation" class="no_dec"><font color=#777777><b>CaPO</b> <font style="color:#AAAAAA;font-size:11px;">(CVPR 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#Flow-RWR, Flow-DPOreinforcement learning of generation" class="no_dec"><font color=#777777><b>Flow-RWR, Flow-DPO</b> <font style="color:#AAAAAA;font-size:11px;">(arXiv 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#PPDreinforcement learning of generation" class="no_dec"><font color=#777777><b>PPD</b> <font style="color:#AAAAAA;font-size:11px;">(CVPR 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#VideoDPOreinforcement learning of generation" class="no_dec"><font color=#777777><b>VideoDPO</b> <font style="color:#AAAAAA;font-size:11px;">(CVPR 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#SPOreinforcement learning of generation" class="no_dec"><font color=#777777><b>SPO</b> <font style="color:#AAAAAA;font-size:11px;">(CVPR 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#Curriculum DPOreinforcement learning of generation" class="no_dec"><font color=#777777><b>Curriculum DPO</b> <font style="color:#AAAAAA;font-size:11px;">(CVPR 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#InstructVideoreinforcement learning of generation" class="no_dec"><font color=#777777><b>InstructVideo</b> <font style="color:#AAAAAA;font-size:11px;">(CVPR 2024)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#Diffusion-DPOreinforcement learning of generation" class="no_dec"><font color=#C55253><b>Diffusion-DPO</b> <font style="color:#AAAAAA;font-size:11px;">(CVPR 2024)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#DDPOreinforcement learning of generation" class="no_dec"><font color=#C55253><b>DDPO</b> <font style="color:#AAAAAA;font-size:11px;">(ICLR 2024)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#ReFLreinforcement learning of generation" class="no_dec"><font color=#777777><b>ReFL</b> <font style="color:#AAAAAA;font-size:11px;">(NeurIPS 2023)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#promptistreinforcement learning of generation" class="no_dec"><font color=#777777><b>promptist</b> <font style="color:#AAAAAA;font-size:11px;">(NeurIPS 2023)</font></font></a>
    </p>
    <br><li><a class="larger low_margin no_dec" style="font-family: 'Arial, sans-serif'; color: #6D94C5;" id="Inference-Time Improvement of Generation" href="#Inference-Time Improvement of Generation-table"><b>Inference-Time Improvement of Generation</b></a></li>
    <p style="display: flex; flex-wrap: wrap; font-size: 13px;">
        <a href="#Inference can Beat Pretraininginference-time improvement of generation" class="no_dec"><font color=#777777><b>Inference can Beat Pretraining</b> <font style="color:#AAAAAA;font-size:11px;">(arXiv 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#PARMinference-time improvement of generation" class="no_dec"><font color=#777777><b>PARM</b> <font style="color:#AAAAAA;font-size:11px;">(CVPR 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#Inference-Time Scaling Analysisinference-time improvement of generation" class="no_dec"><font color=#C55253><b>Inference-Time Scaling Analysis</b> <font style="color:#AAAAAA;font-size:11px;">(CVPR 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#Z-Samplinginference-time improvement of generation" class="no_dec"><font color=#777777><b>Z-Sampling</b> <font style="color:#AAAAAA;font-size:11px;">(ICLR 2025)</font></font></a>
    </p>
    <br><li><a class="larger low_margin no_dec" style="font-family: 'Arial, sans-serif'; color: #6D94C5;" id="Acceleration of Generation" href="#Acceleration of Generation-table"><b>Acceleration of Generation</b></a></li>
    <p style="display: flex; flex-wrap: wrap; font-size: 13px;">
        <a href="#MeanFlowacceleration of generation" class="no_dec"><font color=#C55253><b>MeanFlow</b> <font style="color:#AAAAAA;font-size:11px;">(arXiv 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#SD3-Turboacceleration of generation" class="no_dec"><font color=#C55253><b>SD3-Turbo</b> <font style="color:#AAAAAA;font-size:11px;">(SIGGRAPH Asia 2024)</font></font></a>
    </p>
    <br><li><a class="larger low_margin no_dec" style="font-family: 'Arial, sans-serif'; color: #6D94C5;" id="Datasets & Evaluation of Generation" href="#Datasets & Evaluation of Generation-table"><b>Datasets & Evaluation of Generation</b></a></li>
    <p style="display: flex; flex-wrap: wrap; font-size: 13px;">
        <a href="#HPSv3datasets & evaluation of generation" class="no_dec"><font color=#777777><b>HPSv3</b> <font style="color:#AAAAAA;font-size:11px;">(ICCV 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#UnifiedRewarddatasets & evaluation of generation" class="no_dec"><font color=#777777><b>UnifiedReward</b> <font style="color:#AAAAAA;font-size:11px;">(arXiv 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#VisionRewarddatasets & evaluation of generation" class="no_dec"><font color=#777777><b>VisionReward</b> <font style="color:#AAAAAA;font-size:11px;">(arXiv 2024)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#T2V-CompBenchdatasets & evaluation of generation" class="no_dec"><font color=#777777><b>T2V-CompBench</b> <font style="color:#AAAAAA;font-size:11px;">(T2V-CompBench)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#VQAScoredatasets & evaluation of generation" class="no_dec"><font color=#777777><b>VQAScore</b> <font style="color:#AAAAAA;font-size:11px;">(ECCV 2024)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#Vbenchdatasets & evaluation of generation" class="no_dec"><font color=#C55253><b>Vbench</b> <font style="color:#AAAAAA;font-size:11px;">(CVPR 2024)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#GenEvaldatasets & evaluation of generation" class="no_dec"><font color=#777777><b>GenEval</b> <font style="color:#AAAAAA;font-size:11px;">(NeurIPS 2023)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#T2I-CompBenchdatasets & evaluation of generation" class="no_dec"><font color=#777777><b>T2I-CompBench</b> <font style="color:#AAAAAA;font-size:11px;">(NeurIPS 2023)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#HPS v2datasets & evaluation of generation" class="no_dec"><font color=#C55253><b>HPS v2</b> <font style="color:#AAAAAA;font-size:11px;">(arXiv 2023)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#PickScoredatasets & evaluation of generation" class="no_dec"><font color=#C55253><b>PickScore</b> <font style="color:#AAAAAA;font-size:11px;">(NeurIPS 2023)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#ImageRewarddatasets & evaluation of generation" class="no_dec"><font color=#C55253><b>ImageReward</b> <font style="color:#AAAAAA;font-size:11px;">(NeurIPS 2023)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#HPSdatasets & evaluation of generation" class="no_dec"><font color=#777777><b>HPS</b> <font style="color:#AAAAAA;font-size:11px;">(ICCV 2023)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#CLIPScoredatasets & evaluation of generation" class="no_dec"><font color=#C55253><b>CLIPScore</b> <font style="color:#AAAAAA;font-size:11px;">(EMNLP 2021)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#FVDdatasets & evaluation of generation" class="no_dec"><font color=#777777><b>FVD</b> <font style="color:#AAAAAA;font-size:11px;">(ICLR workshop 2019)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#FIDdatasets & evaluation of generation" class="no_dec"><font color=#C55253><b>FID</b> <font style="color:#AAAAAA;font-size:11px;">(NeurIPS 2017)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#Inception Scoredatasets & evaluation of generation" class="no_dec"><font color=#C55253><b>Inception Score</b> <font style="color:#AAAAAA;font-size:11px;">(NeurIPS 2016)</font></font></a>
    </p>
    <br><li><a class="larger low_margin no_dec" style="font-family: 'Arial, sans-serif'; color: #6D94C5;" id="Controllable Generation" href="#Controllable Generation-table"><b>Controllable Generation</b></a></li>
    <p style="display: flex; flex-wrap: wrap; font-size: 13px;">
        <a href="#Follow-Your-Emojicontrollable generation" class="no_dec"><font color=#777777><b>Follow-Your-Emoji</b> <font style="color:#AAAAAA;font-size:11px;">(SIGGRAPH-Asia 2024)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#ControlNetcontrollable generation" class="no_dec"><font color=#C55253><b>ControlNet</b> <font style="color:#AAAAAA;font-size:11px;">(ICCV 2023)</font></font></a>
    </p>
    <br><li><a class="larger low_margin no_dec" style="font-family: 'Arial, sans-serif'; color: #6D94C5;" id="Editing & Inpainting & Outpainting Generation" href="#Editing & Inpainting & Outpainting Generation-table"><b>Editing & Inpainting & Outpainting Generation</b></a></li>
    <p style="display: flex; flex-wrap: wrap; font-size: 13px;">
        <a href="#Trans-Adapterediting & inpainting & outpainting generation" class="no_dec"><font color=#777777><b>Trans-Adapter</b> <font style="color:#AAAAAA;font-size:11px;">(ICCV 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#MTADiffusionediting & inpainting & outpainting generation" class="no_dec"><font color=#777777><b>MTADiffusion</b> <font style="color:#AAAAAA;font-size:11px;">(CVPR 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#HomoGenediting & inpainting & outpainting generation" class="no_dec"><font color=#777777><b>HomoGen</b> <font style="color:#AAAAAA;font-size:11px;">(CVPR 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#VideoRepainterediting & inpainting & outpainting generation" class="no_dec"><font color=#777777><b>VideoRepainter</b> <font style="color:#AAAAAA;font-size:11px;">(CVPR 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#Step1X-Editediting & inpainting & outpainting generation" class="no_dec"><font color=#777777><b>Step1X-Edit</b> <font style="color:#AAAAAA;font-size:11px;">(arXiv 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#ATAediting & inpainting & outpainting generation" class="no_dec"><font color=#777777><b>ATA</b> <font style="color:#AAAAAA;font-size:11px;">(CVPR 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#TurboFillediting & inpainting & outpainting generation" class="no_dec"><font color=#777777><b>TurboFill</b> <font style="color:#AAAAAA;font-size:11px;">(CVPR 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#OmniPaintediting & inpainting & outpainting generation" class="no_dec"><font color=#777777><b>OmniPaint</b> <font style="color:#AAAAAA;font-size:11px;">(ICCV 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#SAGIediting & inpainting & outpainting generation" class="no_dec"><font color=#777777><b>SAGI</b> <font style="color:#AAAAAA;font-size:11px;">(ICCV 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#BVINetediting & inpainting & outpainting generation" class="no_dec"><font color=#777777><b>BVINet</b> <font style="color:#AAAAAA;font-size:11px;">(ICCV 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#RADediting & inpainting & outpainting generation" class="no_dec"><font color=#777777><b>RAD</b> <font style="color:#AAAAAA;font-size:11px;">(CVPR 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#Pincoediting & inpainting & outpainting generation" class="no_dec"><font color=#777777><b>Pinco</b> <font style="color:#AAAAAA;font-size:11px;">(ICCV 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#OmniEditediting & inpainting & outpainting generation" class="no_dec"><font color=#C55253><b>OmniEdit</b> <font style="color:#AAAAAA;font-size:11px;">(ICLR 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#TD-Paintediting & inpainting & outpainting generation" class="no_dec"><font color=#777777><b>TD-Paint</b> <font style="color:#AAAAAA;font-size:11px;">(ICLR 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#CAT-Diffusionediting & inpainting & outpainting generation" class="no_dec"><font color=#777777><b>CAT-Diffusion</b> <font style="color:#AAAAAA;font-size:11px;">(ECCV 2024)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#Follow-Your-Canvasediting & inpainting & outpainting generation" class="no_dec"><font color=#777777><b>Follow-Your-Canvas</b> <font style="color:#AAAAAA;font-size:11px;">(AAAI 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#Brush2Promptediting & inpainting & outpainting generation" class="no_dec"><font color=#777777><b>Brush2Prompt</b> <font style="color:#AAAAAA;font-size:11px;">(CVPR 2024)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#Paint by Inpaintediting & inpainting & outpainting generation" class="no_dec"><font color=#777777><b>Paint by Inpaint</b> <font style="color:#AAAAAA;font-size:11px;">(CVPR 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#StrDiffusionediting & inpainting & outpainting generation" class="no_dec"><font color=#777777><b>StrDiffusion</b> <font style="color:#AAAAAA;font-size:11px;">(CVPR 2024)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#Latent Codesediting & inpainting & outpainting generation" class="no_dec"><font color=#777777><b>Latent Codes</b> <font style="color:#AAAAAA;font-size:11px;">(CVPR 2024)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#BrushNetediting & inpainting & outpainting generation" class="no_dec"><font color=#C55253><b>BrushNet</b> <font style="color:#AAAAAA;font-size:11px;">(ECCV 2024)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#ROVIediting & inpainting & outpainting generation" class="no_dec"><font color=#777777><b>ROVI</b> <font style="color:#AAAAAA;font-size:11px;">(CVPR 2024)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#HD-Painterediting & inpainting & outpainting generation" class="no_dec"><font color=#777777><b>HD-Painter</b> <font style="color:#AAAAAA;font-size:11px;">(ICLR 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#ASUKAediting & inpainting & outpainting generation" class="no_dec"><font color=#777777><b>ASUKA</b> <font style="color:#AAAAAA;font-size:11px;">(CVPR 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#AVIDediting & inpainting & outpainting generation" class="no_dec"><font color=#777777><b>AVID</b> <font style="color:#AAAAAA;font-size:11px;">(CVPR 2024)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#PowerPaintediting & inpainting & outpainting generation" class="no_dec"><font color=#C55253><b>PowerPaint</b> <font style="color:#AAAAAA;font-size:11px;">(ECCV 2024)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#TPMediting & inpainting & outpainting generation" class="no_dec"><font color=#777777><b>TPM</b> <font style="color:#AAAAAA;font-size:11px;">(ICLR 2024)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#SmartBrushediting & inpainting & outpainting generation" class="no_dec"><font color=#C55253><b>SmartBrush</b> <font style="color:#AAAAAA;font-size:11px;">(CVPR 2023)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#PSMediting & inpainting & outpainting generation" class="no_dec"><font color=#777777><b>PSM</b> <font style="color:#AAAAAA;font-size:11px;">(ICLR 2024)</font></font></a>
    </p>
    <br><li><a class="larger low_margin no_dec" style="font-family: 'Arial, sans-serif'; color: #6D94C5;" id="Stylization Generation" href="#Stylization Generation-table"><b>Stylization Generation</b></a></li>
    <p style="display: flex; flex-wrap: wrap; font-size: 13px;">
        <a href="#OmniStyle2stylization generation" class="no_dec"><font color=#777777><b>OmniStyle2</b> <font style="color:#AAAAAA;font-size:11px;">(arXiv 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#SCFlowstylization generation" class="no_dec"><font color=#777777><b>SCFlow</b> <font style="color:#AAAAAA;font-size:11px;">(ICCV 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#AIComposerstylization generation" class="no_dec"><font color=#777777><b>AIComposer</b> <font style="color:#AAAAAA;font-size:11px;">(ICCV 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#CSD-VARstylization generation" class="no_dec"><font color=#777777><b>CSD-VAR</b> <font style="color:#AAAAAA;font-size:11px;">(ICCV 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#DGPSTstylization generation" class="no_dec"><font color=#777777><b>DGPST</b> <font style="color:#AAAAAA;font-size:11px;">(ICCV 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#OmniStylestylization generation" class="no_dec"><font color=#777777><b>OmniStyle</b> <font style="color:#AAAAAA;font-size:11px;">(CVPR 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#DuoLoRAstylization generation" class="no_dec"><font color=#777777><b>DuoLoRA</b> <font style="color:#AAAAAA;font-size:11px;">(ICCV 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#Semantixstylization generation" class="no_dec"><font color=#777777><b>Semantix</b> <font style="color:#AAAAAA;font-size:11px;">(ICLR 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#SaMamstylization generation" class="no_dec"><font color=#777777><b>SaMam</b> <font style="color:#AAAAAA;font-size:11px;">(CVPR 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#V-Styliststylization generation" class="no_dec"><font color=#777777><b>V-Stylist</b> <font style="color:#AAAAAA;font-size:11px;">(CVPR 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#SMSstylization generation" class="no_dec"><font color=#777777><b>SMS</b> <font style="color:#AAAAAA;font-size:11px;">(ICCV 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#SCSAstylization generation" class="no_dec"><font color=#777777><b>SCSA</b> <font style="color:#AAAAAA;font-size:11px;">(CVPR 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#K-LoRAstylization generation" class="no_dec"><font color=#777777><b>K-LoRA</b> <font style="color:#AAAAAA;font-size:11px;">(CVPR 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#MaskSTstylization generation" class="no_dec"><font color=#777777><b>MaskST</b> <font style="color:#AAAAAA;font-size:11px;">(ICLR 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#HSIstylization generation" class="no_dec"><font color=#777777><b>HSI</b> <font style="color:#AAAAAA;font-size:11px;">(CVPR 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#StyleSSPstylization generation" class="no_dec"><font color=#777777><b>StyleSSP</b> <font style="color:#AAAAAA;font-size:11px;">(CVPR 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#IntroStylestylization generation" class="no_dec"><font color=#777777><b>IntroStyle</b> <font style="color:#AAAAAA;font-size:11px;">(ICCV 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#StyleStudiostylization generation" class="no_dec"><font color=#777777><b>StyleStudio</b> <font style="color:#AAAAAA;font-size:11px;">(CVPR 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#StyleMasterstylization generation" class="no_dec"><font color=#777777><b>StyleMaster</b> <font style="color:#AAAAAA;font-size:11px;">(CVPR 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#LoRA.rarstylization generation" class="no_dec"><font color=#777777><b>LoRA.rar</b> <font style="color:#AAAAAA;font-size:11px;">(ICCV 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#UnZipLoRAstylization generation" class="no_dec"><font color=#777777><b>UnZipLoRA</b> <font style="color:#AAAAAA;font-size:11px;">(ICCV 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#VarInvstylization generation" class="no_dec"><font color=#777777><b>VarInv</b> <font style="color:#AAAAAA;font-size:11px;">(ICLR 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#CompReverstylization generation" class="no_dec"><font color=#777777><b>CompRever</b> <font style="color:#AAAAAA;font-size:11px;">(ECCV 2024)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#FineStylestylization generation" class="no_dec"><font color=#777777><b>FineStyle</b> <font style="color:#AAAAAA;font-size:11px;">(NeurIPS 2024)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#ACFunstylization generation" class="no_dec"><font color=#777777><b>ACFun</b> <font style="color:#AAAAAA;font-size:11px;">(NeurIPS 2024)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#StyleTokenizerstylization generation" class="no_dec"><font color=#777777><b>StyleTokenizer</b> <font style="color:#AAAAAA;font-size:11px;">(ECCV 2024)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#Style-Editorstylization generation" class="no_dec"><font color=#777777><b>Style-Editor</b> <font style="color:#AAAAAA;font-size:11px;">(CVPR 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#RB-Modulationstylization generation" class="no_dec"><font color=#777777><b>RB-Modulation</b> <font style="color:#AAAAAA;font-size:11px;">(ICLR 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#B-LoRAstylization generation" class="no_dec"><font color=#777777><b>B-LoRA</b> <font style="color:#AAAAAA;font-size:11px;">(ECCV 2024)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#ZipLoRAstylization generation" class="no_dec"><font color=#777777><b>ZipLoRA</b> <font style="color:#AAAAAA;font-size:11px;">(ECCV 2024)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#InstaStylestylization generation" class="no_dec"><font color=#777777><b>InstaStyle</b> <font style="color:#AAAAAA;font-size:11px;">(ECCV 2024)</font></font></a>
    </p>
    <br><li><a class="larger low_margin no_dec" style="font-family: 'Arial, sans-serif'; color: #6D94C5;" id="Interactive Generation" href="#Interactive Generation-table"><b>Interactive Generation</b></a></li>
    <p style="display: flex; flex-wrap: wrap; font-size: 13px;">
        <a href="#Matrix-Game 2.0interactive generation" class="no_dec"><font color=#777777><b>Matrix-Game 2.0</b> <font style="color:#AAAAAA;font-size:11px;">(arXiv 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#Yaninteractive generation" class="no_dec"><font color=#777777><b>Yan</b> <font style="color:#AAAAAA;font-size:11px;">(arXiv 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#Matrix-Gameinteractive generation" class="no_dec"><font color=#777777><b>Matrix-Game</b> <font style="color:#AAAAAA;font-size:11px;">(arXiv 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#GameFactoryinteractive generation" class="no_dec"><font color=#777777><b>GameFactory</b> <font style="color:#AAAAAA;font-size:11px;">(ICCV 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#Genieinteractive generation" class="no_dec"><font color=#C55253><b>Genie</b> <font style="color:#AAAAAA;font-size:11px;">(ICML 2024)</font></font></a>
    </p>
    <br></ul><h2 id="Foundation Algorithms & Models of Understanding-table"><a class="no_dec" href="#Foundation Algorithms & Models of Understanding">Foundation Algorithms & Models of Understanding</a></h2>
    <p class="little_split" id='MAEfoundation algorithms & models of understanding'></p>
    <div style="border-left: 16px solid #9ACAF0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('MAE-Foundation Algorithms & Models of Understanding-details')"><i>Masked Autoencoders Are Scalable Vision Learners</i></p>
    <p class="paper_detail">Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick</p>
    <p class="paper_detail">Facebook AI Research (FAIR)</p>
    <p class="paper_detail">Conference on Computer Vision and Pattern Recognition (<b><font color=#404040>CVPR</font></b>), <b>2022</b></font></p>
    <p class="paper_detail"><b>Nov 11, 2021</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#C55253>MAE</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2111.06377">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/facebookresearch/mae">code</a></p>
    <p class="paper_detail"><font color=#C55253>It introduces an efficient self-supervised learning paradigm that reconstructs missing image patches, enabling scalable pretraining with reduced computational cost, and significantly improving performance and transferability across vision benchmarks. It has over 11,000 citations (as of Sep 2025).</font></p>
    
    <div id='MAE-Foundation Algorithms & Models of Understanding-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It introduces a <b>masked autoencoder</b> that reconstructs 75% masked patches, enabling scalable self-supervised pre-training of Vision Transformers.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='BEiTfoundation algorithms & models of understanding'></p>
    <div style="border-left: 16px solid #9ACAF0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('BEiT-Foundation Algorithms & Models of Understanding-details')"><i>BEiT: BERT Pre-Training of Image Transformers</i></p>
    <p class="paper_detail">Hangbo Bao, Li Dong, Songhao Piao, Furu Wei</p>
    <p class="paper_detail">Harbin Institute of Technology, Microsoft Research</p>
    <p class="paper_detail">International Conference on Learning Representations (<b><font color=#404040>ICLR</font></b>), <b>2022</b></font></p>
    <p class="paper_detail"><b>Jun 15, 2021</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#C55253>BEiT</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2106.08254">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/microsoft/unilm">code</a></p>
    
    
    <div id='BEiT-Foundation Algorithms & Models of Understanding-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It introduces <b>masked image modeling</b> with discrete visual tokens to pre-train Vision Transformers in a self-supervised BERT-like fashion.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='MoCo v3foundation algorithms & models of understanding'></p>
    <div style="border-left: 16px solid #9ACAF0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('MoCo v3-Foundation Algorithms & Models of Understanding-details')"><i>An Empirical Study of Training Self-Supervised Vision Transformers</i></p>
    <p class="paper_detail">Xinlei Chen, Saining Xie, Kaiming He</p>
    <p class="paper_detail">Facebook AI Research (FAIR)</p>
    <p class="paper_detail">International Conference on Computer Vision (<b><font color=#404040>ICCV</font></b>), <b>2021</b></font></p>
    <p class="paper_detail"><b>Apr 05, 2021</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#C55253>MoCo v3</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2104.02057">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/facebookresearch/moco-v3">code</a></p>
    
    
    <div id='MoCo v3-Foundation Algorithms & Models of Understanding-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It introduces a random patch projection trick that <b>freezes the first ViT layer</b> to stabilize contrastive self-supervised training.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='SimSiamfoundation algorithms & models of understanding'></p>
    <div style="border-left: 16px solid #9ACAF0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('SimSiam-Foundation Algorithms & Models of Understanding-details')"><i>Exploring Simple Siamese Representation Learning</i></p>
    <p class="paper_detail">Xinlei Chen, Kaiming He</p>
    <p class="paper_detail">Facebook AI Research (FAIR)</p>
    <p class="paper_detail">Conference on Computer Vision and Pattern Recognition (<b><font color=#404040>CVPR</font></b>), <b>2021</b></font></p>
    <p class="paper_detail"><b>Nov 20, 2020</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#C55253>SimSiam</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2011.10566">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/facebookresearch/simsiam">code</a></p>
    
    
    <div id='SimSiam-Foundation Algorithms & Models of Understanding-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It introduces a simple yet effective <b>Siamese architecture</b> that learns visual representations by contrasting positive and negative pairs.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='MoCofoundation algorithms & models of understanding'></p>
    <div style="border-left: 16px solid #9ACAF0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('MoCo-Foundation Algorithms & Models of Understanding-details')"><i>Momentum Contrast for Unsupervised Visual Representation Learning</i></p>
    <p class="paper_detail">Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick</p>
    <p class="paper_detail">Facebook AI Research (FAIR)</p>
    <p class="paper_detail">Conference on Computer Vision and Pattern Recognition (<b><font color=#404040>CVPR</font></b>), <b>2020</b></font></p>
    <p class="paper_detail"><b>Nov 13, 2019</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#C55253>MoCo</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/1911.05722">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/facebookresearch/moco">code</a></p>
    <p class="paper_detail"><font color=#C55253>It advances unsupervised visual representation learning by introducing a momentum-updated encoder with a dynamic queue of negatives, enabling scalable contrastive training that rivaled supervised pretraining and shaped subsequent self-supervised learning research. It has over 17,000 citations (as of Sep 2025).</font></p>
    
    <div id='MoCo-Foundation Algorithms & Models of Understanding-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It introduces <b>momentum contrast</b> to train Vision Transformers in a self-supervised manner.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    <h2 id="Foundation Algorithms & Models of Generation-table"><a class="no_dec" href="#Foundation Algorithms & Models of Generation">Foundation Algorithms & Models of Generation</a></h2>
    <p class="little_split" id='Video Zero-shotfoundation algorithms & models of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('Video Zero-shot-Foundation Algorithms & Models of Generation-details')"><i>Video Models are Zero-Shot Learners and Reasoners</i></p>
    <p class="paper_detail">Thaddäus Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky, Been Kim, Priyank Jaini, Robert Geirhos</p>
    <p class="paper_detail">Google DeepMind</p>
    <p class="paper_detail"> <b><font color=#404040>arXiv</font></b>, <b>2025</b></font></p>
    <p class="paper_detail"><b>Sep 24, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>Video Zero-shot</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2509.20328">paper</a></p>
    
    
    <div id='Video Zero-shot-Foundation Algorithms & Models of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It proposes that <b>large-scale generative video models</b> can function as <b>zero-shot general-purpose vision foundation models</b>.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='Seedream 4.0foundation algorithms & models of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('Seedream 4.0-Foundation Algorithms & Models of Generation-details')"><i>Seedream 4.0: Toward Next-generation Multimodal Image Generation</i></p>
    <p class="paper_detail">ByteDance Seedream Team</p>
    <p class="paper_detail">ByteDance</p>
    <p class="paper_detail"> <b><font color=#404040>arXiv</font></b>, <b>2025</b></font></p>
    <p class="paper_detail"><b>Sep 24, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>Seedream 4.0</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2509.20427">paper</a></p>
    
    
    <div id='Seedream 4.0-Foundation Algorithms & Models of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It introduces an efficient scalable DiT with high-compression VAE and acceleration, unifying <b>multi-image generation and editing</b> in one framework.
</font></p>
        
        <p>
<ul>
    <li> <b>Structure.</b> It is based on DiT. 
    <li> <b>Training.</b> 512-reso pre-training => 1024-4096-reso pre-training => continue training => SFT => RLHF => prompt engineering with Seed1.5-VL
    <li> <b>Acceleration.</b> Integrate: Hyper-SD, RayFlow, APT, ADM, quantization, etc.
</ul>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='Qwen-Imagefoundation algorithms & models of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('Qwen-Image-Foundation Algorithms & Models of Generation-details')"><i>Qwen-Image Technical Report</i></p>
    <p class="paper_detail">Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, Zenan Liu</p>
    <p class="paper_detail">Qwen Team</p>
    <p class="paper_detail"> <b><font color=#404040>arXiv</font></b>, <b>2025</b></font></p>
    <p class="paper_detail"><b>Aug 04, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>Qwen-Image</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2508.02324">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/QwenLM/Qwen-Image">code</a></p>
    
    
    <div id='Qwen-Image-Foundation Algorithms & Models of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020><b>Alibaba Qwen Team</b>'s image generation foundation model, excelling in <b>complex text rendering</b> and <b>precise image editing</b>.</font></p>
        
        <p><ul>
<li> <b>Structure.</b> It employs the standard <b>MMDiT</b> structure with <b>Qwen2.5 VL</b> as the text encoder.
<li> <b>VAE.</b> It finetunes an <b>image decoder</b> and a <b>video decoder</b> upon <b>Wan-2.1-VAE</b> by optimizing only a reconstruction loss and a perceptual loss.
<li> <b>Positional embedding.</b> It introduces Multimodal Scalable RoPE (MSRoPE), a <b>diagonal</b> position encoding.
<li> <b>Data balance.</b> Nature: 55%. Design: 27%. People: 13%. Synthetic: 5%.
<li> <b>Data filtering.</b> <b>Stage 1.</b> Initial pre-training. 256p. Broken files + resolution + deduplication + NSFW. <b>Stage 2.</b> Quality improvement. Rotation + brightness + saturation + entropy + texture. <b>Stage 3.</b> Alignment improvement. Chinese CLIP + SigLIP 2 + token length. <b>Stage 4.</b> Text-rendering enhancement. Intense filter + small character filter. <b>Stage 5.</b> High-resolution refinement. 640p. Image quality + resolution + aesthetic + abnormal element. <b>Stage 6.</b> Category balance and portrait augmentation. <b>Stage 7.</b> Balanced multi-scale training. 640p and 1328p.
<li> <b>Data synthesis.</b> (1) Pure rendering in simple background. (2) Compositional rendering in contextual scenes. (3) Complex rendering in structured frames.
<li> <b>RL.</b> DPO + GRPO.
<li> <b>Editing.</b> Channel concatenation of the original image and the edited image.
</ul>
<figure>
<img data-src='resource\figs\Qwen-Image\Qwen-Image-fig2.png' width=550>
<img data-src='resource\figs\Qwen-Image\Qwen-Image-fig1.png' width=350>
<figcaption>
<b>Figure 1.</b> <b>Structure and configuration.</b>
</figcaption>
</figure></p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='Lumos-1foundation algorithms & models of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('Lumos-1-Foundation Algorithms & Models of Generation-details')"><i>Lumos-1: On Autoregressive Video Generation from a Unified Model Perspective</i></p>
    <p class="paper_detail">Hangjie Yuan, Weihua Chen, Jun Cen, Hu Yu, Jingyun Liang, Shuning Chang, Zhihui Lin, Tao Feng, Pengwei Liu, Jiazheng Xing, Hao Luo, Jiasheng Tang, Fan Wang, Yi Yang</p>
    <p class="paper_detail">DAMO Academy, Alibaba Group, Hupan Lab, Zhejiang University, Tsinghua University</p>
    <p class="paper_detail"> <b><font color=#404040>arXiv</font></b>, <b>2025</b></font></p>
    <p class="paper_detail"><b>Jul 11, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>Lumos-1</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2507.08801">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/alibaba-damo-academy/Lumos">code</a></p>
    
    
    <div id='Lumos-1-Foundation Algorithms & Models of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>It employs LLM architecture to achieve auto-regressive video generation with some improvement on RoPE and masking strategy.</font></p>
        
        <p>
<ul>
    <li><b>Structure:</b> Llama with a new RoPE strategy to model multimodal spatiotemporal dependency.
    <li><b>Tokenizer:</b> Cosmos's visual tokenizer with spatiotemporal compression rates of 8x8x4; Chameleon's text encoder.
    <li> <b>Model size:</b> 0.5B, 1B, and 3B.
</ul>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='Magi-1foundation algorithms & models of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('Magi-1-Foundation Algorithms & Models of Generation-details')"><i>MAGI-1: Autoregressive Video Generation at Scale</i></p>
    <p class="paper_detail">Hansi Teng, Hongyu Jia, Lei Sun, Lingzhi Li, Maolin Li, Mingqiu Tang, Shuai Han, Tianning Zhang, W.Q. Zhang, Weifeng Luo, Xiaoyang Kang, Yuchen Sun, Yue Cao, Yunpeng Huang, Yutong Lin, Yuxin Fang, Zewei Tao, Zheng Zhang, Zhongshu Wang, Zixun Liu, Dai Shi, Guoli Su, Hanwen Sun, Hong Pan, Jie Wang, Jiexin Sheng, Min Cui, Min Hu, Ming Yan, Shucheng Yin, Siran Zhang, Tingting Liu, Xianping Yin, Xiaoyu Yang, Xin Song, Xuan Hu, Yankai Zhang, Yuqiao Li</p>
    <p class="paper_detail">Sand AI</p>
    <p class="paper_detail"> <b><font color=#404040>arXiv</font></b>, <b>2025</b></font></p>
    <p class="paper_detail"><b>May 19, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>Magi-1</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2505.13211">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/SandAI-org/Magi-1">code</a></p>
    
    
    <div id='Magi-1-Foundation Algorithms & Models of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>It achieves <b>chunk-wise</b> auto-regressive video generation by employing transformer-based VAE, progressive-noise causal modeling with flow matching, advanced attention/distillation techniques to enable streaming-capable video generation with fixed peak inference costs regardless of video length.</font></p>
        
        <p><ul>
<li> <b>VAE training.</b> (1) Stage 1: use training data of fixed-size videos with 256x256 resolution and 16 frames; (2) Stage 2: use mixed training data of images and 16-frame videos, and use variable resolution and aspect ratio. Training loss: <i>L = L_1 + L_KL + L_LPIPS + L_GAN</i>.
<li> <b>VAE inference.</b> Use sliding window with size of 256x256 with a stride of 192 (25% overlap). Sliding windows are not applied to temporal frames.
<li> <b>Model structure.</b> It is based on DiT with some modifications: (1) Use T5 as the text encoder; (2) Use learnable 3D RoPE to encode temporal positional information; (3) Use new kernel called Flexible-Flash-Attention; (4) Replace multi-head attention by grouped-query attention; (5) Apply LayerNorm before and after FFN and use SwiGLU to stablize training; (6) Constrain scaling value of AdaLN to [-1, 1] to stablize training.
<li> <b>Guidance.</b> <i>output = (1 - w_prev) * output_current + (w_prev - w_text) * output_prev + w_text * output_prev</i> (see the paper for details).
<li> <b>Prompt enhancement</b> for inference. Use distilled MLLM to enhance prompts. (1) Stage 1: analyze and describe the image content; (2) Stage 2: predict the temporal evolution of the scene or objects in the first frame, such as actions, motion trajectories, and transitions.
</ul>
<figure>
<img data-src='resource\figs\Magi-1\Magi-1-fig2.png' width=350>
<figcaption>
<b>Figure 1.</b> <b>VAE structure.</b> Videos are compressed by 8x8 spatially and 4 temporally, generating 16-channel features.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Magi-1\Magi-1-fig3.png' width=450>
<figcaption>
<b>Figure 2.</b> <b>VAE performance.</b> Despite having the largest model size, its encoding and decoding is efficient.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Magi-1\Magi-1-fig1.png' width=800>
<figcaption>
<b>Figure 3.</b> <b>Model design.</b> It generates videos chunk-by-chunk, where a chunk (usually 24 frames) is denoised to a certain extent and the next chunk begins generation (conditioned on all preceding chunks). The earlier chunks are cleaner than later ones. It allows multiple chunks (often 4) to be precessed concurrently. It unifies text-to-video, video continuation, and image-to-video generation.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Magi-1\Magi-1-fig4.png' width=400>
<img data-src='resource\figs\Magi-1\Magi-1-fig5.png' width=400>
<figcaption>
<b>Figure 4.</b> <b>Model structure.</b>
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Magi-1\Magi-1-fig6.png' width=400>
<figcaption>
<b>Figure 5.</b> <b>Data processing pipeline.</b> (1) Video quality: DOVER technical score; (2) Aesthetics: LAION aesthetic; (3) Overexposed & underexposed: average brightness on HSI color space; (4) Motion: RAFT optical flow model with saliency detection model; (5) Camera movement stability: evaluate consistency of optical flow between adjacent frames; (6) Slides movement: if divergence of optical flow remains consistently low; (7) Border: edge detection & Hough transform; (8) Text; (9) Logo: Florence-2; (10) Corner face: face detection model; (11) Transition: use CLIP to compute similarity between adjacent keyframes; (12) Deduplication: use pairwise similarity scores from CLIP & DINOv2; (13) Complex filtering: MLLM.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Magi-1\Magi-1-fig7.png' width=400>
<figcaption>
<b>Figure 6.</b> <b>Attributes for caption instruction.</b> Captioning by two stages: answer predefined attributes & final caption based on observed attributes. Based on the advantages of auto-regressive video generation, it applies fine-grained second-by-second descrptions for each video clip.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Magi-1\Magi-1-fig8.png' width=300>
<figcaption>
<b>Figure 7.</b> <b>Data configuration.</b>
</figcaption>
</figure></p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='SimpleARfoundation algorithms & models of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('SimpleAR-Foundation Algorithms & Models of Generation-details')"><i>SimpleAR: Pushing the Frontier of Autoregressive Visual Generation through Pretraining, SFT, and RL</i></p>
    <p class="paper_detail">Junke Wang, Zhi Tian, Xun Wang, Xinyu Zhang, Weilin Huang, Zuxuan Wu, Yu-Gang Jiang</p>
    <p class="paper_detail">Fudan University, ByteDance Seed</p>
    <p class="paper_detail"> <b><font color=#404040>arXiv</font></b>, <b>2025</b></font></p>
    <p class="paper_detail"><b>Apr 15, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>SimpleAR</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2504.11455">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/wdrink/SimpleAR/">code</a></p>
    
    
    <div id='SimpleAR-Foundation Algorithms & Models of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>A vanilla, open-sourced AR model (0.5B) for 1K text-to-image generation, trained by pre-training, SFT, RL (GRPO), and acceleration.</font></p>
        
        <p>
<ul>
    <li> <b>Structure.</b> Use <i>Qwen</i> structure and taking <i>Cosmos</i> as the visual tokenizer with 64K codebook and 16 ratio downsampling.
    <li> <b>Training stages.</b> (1) Pre-training on 512 resolution; (2) SFT on 1024 resolution; (3) RL on 1024 resolution.
    <li> Use <b>LLM initialization</b> does not improve DPG-Bench performance.
    <li> Use <b>2D RoPE</b> will not improve performance, but is necessary for dynamic resolution generation.
    <li> Use <b>GRPO</b> with CLIP as the reward model improves more than using HPS v2.
</ul>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='Seedream 3.0foundation algorithms & models of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('Seedream 3.0-Foundation Algorithms & Models of Generation-details')"><i>Seedream 3.0 Technical Report</i></p>
    <p class="paper_detail">ByteDance Seed Vision Team</p>
    <p class="paper_detail">ByteDance</p>
    <p class="paper_detail"> <b><font color=#404040>arXiv</font></b>, <b>2025</b></font></p>
    <p class="paper_detail"><b>Apr 15, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>Seedream 3.0</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2504.11346">paper</a></p>
    
    
    <div id='Seedream 3.0-Foundation Algorithms & Models of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020><b>ByteDance Seed Vision Team</b>'s text-to-image generation model, improving Seedream 2.0 by representation alignment, larger reward models.</font></p>
        
        <p>
<ul>
    <li> Propose <b>defect-aware training:</b> stop gradient on watermarks, subtitles, overlaid text, mosaic pattern.
    <li> Introduce a <b>representation alignment loss</b>: cosine distance between the feature of MMDiT and DINOv2-L.
    <li> Find <b>scaling property of VLM-based reward model</b>.
    <li> Other improvements: (1) mixed-resolution training; (2) <b>cross-modality RoPE</b>; (3) diverse aesthetic captions in SFT.
</ul>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='Seaweed-7Bfoundation algorithms & models of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('Seaweed-7B-Foundation Algorithms & Models of Generation-details')"><i>Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model</i></p>
    <p class="paper_detail">ByteDance Seaweed Team</p>
    <p class="paper_detail">ByteDance</p>
    <p class="paper_detail"> <b><font color=#404040>arXiv</font></b>, <b>2025</b></font></p>
    <p class="paper_detail"><b>Apr 11, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>Seaweed-7B</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2504.08685">paper</a></p>
    
    
    <div id='Seaweed-7B-Foundation Algorithms & Models of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020><b>ByteDance Seaweed Team</b>'s text-to-video and image-to-video generation model (7B), trained on O(100M) videos using 665K H100 GPU hours.</font></p>
        
        <p><figure>
<img data-src='resource\figs\Seaweed-7B\Seaweed-7B-fig2.png' width=450>
<figcaption>
<b>Figure 1.</b> <b>VAE</b> with compression ratio of 16x16x4 (48 channels) or 8x8x4 (16 channels). Using L1 + KL + LPIPS + adversarial losses. Using an <i>image discriminator and a video discriminator</i> is better than using either one. <i>Compressing using VAE outperforms patchification in DiT, and faster</i>.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Seaweed-7B\Seaweed-7B-fig3.png' width=200>
<figcaption>
<b>Figure 2.</b> <b>VAE training stages</b> for images and videos.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Seaweed-7B\Seaweed-7B-fig4.png' width=800>
<figcaption>
<b>Figure 3.</b> Use <b>mixed resolution & durations & frame rate</b> VAE training converges slower but performs better than training on a low resolution.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Seaweed-7B\Seaweed-7B-fig6.png' width=650>
<figcaption>
<b>Figure 4.</b> <b>Full attention</b> enjoys training scalability.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Seaweed-7B\Seaweed-7B-fig5.png' width=250>
<figcaption>
<b>Figure 5.</b> <b>Figure 5.</b> The proposed <b>hybrid-stream</b> is better than dual-stream (MMDiT).
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Seaweed-7B\Seaweed-7B-fig7.png' width=500>
<figcaption>
<b>Figure 6.</b> <b>4-stage pre-training.</b> (1) <b>Multi-task pre-training:</b> text-to-video, image-to-video, video-to-video. Input features and conditions are channel-concatenated, with a binary mask indicating the condition. Ratio of image-to-video is 20% during pre-training, and increases to 50%-75% detached for fine-tuning. (2) <b>SFT:</b> use 700K good videos and 50K top videos; The semantic alignment ability drops a little. (3) <b>RLHF:</b> lr=1e-7, beta=100, select win-lose from 4 candidates. (4) <b>Distillation:</b> trajectory segmented consistency distillation + CFG distillation + adversarial training, distill to 8 steps.
</figcaption>
</figure></p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='Wanfoundation algorithms & models of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('Wan-Foundation Algorithms & Models of Generation-details')"><i>Wan: Open and Advanced Large-Scale Video Generative Models</i></p>
    <p class="paper_detail">Tongyi Wanxiang</p>
    <p class="paper_detail">Alibaba</p>
    <p class="paper_detail"> <b><font color=#404040>arXiv</font></b>, <b>2025</b></font></p>
    <p class="paper_detail"><b>Mar 26, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>Wan</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2503.20314">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/Wan-Video/Wan2.1/">code</a></p>
    
    
    <div id='Wan-Foundation Algorithms & Models of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020><b>Alibaba Tongyi Wanxiang</b>'s text-to-video and image-to-video generation models (14B) with DiT structure.</font></p>
        
        <p><p><b>Data procssing pipeline</b>. <i>Fundamental dimensions:</i> text, aesthetic, NSFW score, watermark and logo, black border, overexposure, synthetic image, blur, duration and resolution. <i>Visual quality:</i> clustering, scoring. <i>Motion quality:</i> optimal motion, medium-quality motion, static videos, camera-driven motion, low-quality motion, shaky camera footage. <i>Visual text data:</i> hundreds of millions of text-containing images by rendering Chinese characters on a pure white background and large amounts from real-world data. <i>Captions:</i> celebrities, landmarks, movie characters, object counting, OCR, camera angle and motion, categories, relational understanding, re-caption, editing instruction caption, group image description, human-annotated captions.</p>
<figure>
<img data-src='resource\figs\Wan\Wan-fig1.png' width=450>
<img data-src='resource\figs\Wan\Wan-fig2.png' width=450>
<figcaption>
<b>Figure 1.</b> <b>VAE</b> with 127M parameters and 8x8x4 compression ratio. Three-stage training: (1) Train 2D image VAE. (2) Train 3D causal VAE with 128x128x5. (3) Adversarial training with a 3D discriminator. It optimizes <i>L = L_1 loss + KL loss + L_LPIPS</i> loss. It replace all GroupNorm layers with RMSNorm layers to preserve temporal causality. It then applies <i>temporal cache</i> to save GPU memory by employing the causality.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Wan\Wan-fig3.png' width=500>
<figcaption>
<b>Figure 2.</b> <b>Architecture</b>. Text prompt encoded by umT5 is injected by cross-attention; timestep is embedded by MLP; using flow-matching loss.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Wan\Wan-fig4.png' width=550>
<figcaption>
<b>Figure 3.</b> <b>I2V framework.</b> Image condition is incorporated through channel-concat and <i>CLIP image encodings</i>.
</figcaption>
</figure></p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='Step-Video-TI2Vfoundation algorithms & models of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('Step-Video-TI2V-Foundation Algorithms & Models of Generation-details')"><i>Step-Video-TI2V Technical Report: A State-of-the-Art Text-Driven Image-to-Video Generation Model</i></p>
    <p class="paper_detail">Step-Video Team</p>
    <p class="paper_detail">StepFun</p>
    <p class="paper_detail"> <b><font color=#404040>arXiv</font></b>, <b>2025</b></font></p>
    <p class="paper_detail"><b>Mar 14, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>Step-Video-TI2V</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2503.11251">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/stepfun-ai/Step-Video-TI2V/">code</a></p>
    
    
    <div id='Step-Video-TI2V-Foundation Algorithms & Models of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020><b>StepFun</b>'s image-to-video generation model (30B), trained upon Step-Video-T2V, by incorporating conditions of motion and channel-concat image.</font></p>
        
        <p><figure>
<img data-src='resource\figs\Step-Video-TI2V\Step-Video-TI2V-fig1.png' width=600>
<figcaption>
<b>Figure 1.</b> <b>Image condition:</b> channel-concat of <i>noise-augmented</i> image condition. <b>Motion condition:</b> optical flow-based motion + timestep.
</figcaption>
</figure></p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='Seedream2.0foundation algorithms & models of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('Seedream2.0-Foundation Algorithms & Models of Generation-details')"><i>Seedream 2.0: A Native Chinese-English Bilingual Image Generation Foundation Model</i></p>
    <p class="paper_detail">ByteDance's Seed Vision Team</p>
    <p class="paper_detail">ByteDance</p>
    <p class="paper_detail"> <b><font color=#404040>arXiv</font></b>, <b>2025</b></font></p>
    <p class="paper_detail"><b>Mar 10, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>Seedream2.0</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2503.07703">paper</a></p>
    
    
    <div id='Seedream2.0-Foundation Algorithms & Models of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020><b>ByteDance Sead Vision Team</b> 's image generation model that employs MMDiT structure and has Chinese-English bilingual capability.</font></p>
        
        <p><ul>
<li> <b>Structure innovation:</b> <i>self-developed bilingual LLM</i> and ByT5 as text encoders; <i>self-developed VAE</i>; learned positional embeddings on text tokens and scaled 2D RoPE on image tokens.
<li> <b>Training stages:</b> pre-training => continue training => supervised fine-tuning => human feedback alignment.
<li> <b>Inference stages:</b> user prompt => prompt engineering => text encoding => generation => refinement => output.
<li> <b>User experience platform:</b> Doubao (豆包) & Dreamina (即梦).
</ul>
<figure>
<img data-src='resource\figs\Seedream2.0\Seedream2.0-fig3.png' width=600>
<figcaption>
<b>Figure 1.</b> <b>Model structure</b> is similar to MMDiT (SD3).
</figcaption>
</figure></p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='uEDMfoundation algorithms & models of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('uEDM-Foundation Algorithms & Models of Generation-details')"><i>Is Noise Conditioning Necessary for Denoising Generative Models?</i></p>
    <p class="paper_detail">Qiao Sun, Zhicheng Jiang, Hanhong Zhao, Kaiming He</p>
    <p class="paper_detail">MiT</p>
    <p class="paper_detail">International Conference on Machine Learning (<b><font color=#404040>ICML</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Feb 18, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>uEDM</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2502.13129">paper</a></p>
    
    
    <div id='uEDM-Foundation Algorithms & Models of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>Theoretical and empirical analysis on denoising diffusion models <b>without a timestep input</b> for image generation.</font></p>
        
        <p>
<ul>
    <li> Many denoising generative models perform <b>robustly</b> even in the absence of noise conditioning.
    <li> <b>Flow</b>-based ones can even produce <b>improved results</b> without noise conditioning.
</ul>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='Step-Video-T2Vfoundation algorithms & models of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('Step-Video-T2V-Foundation Algorithms & Models of Generation-details')"><i>Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model</i></p>
    <p class="paper_detail">Step-Video Team</p>
    <p class="paper_detail">StepFun</p>
    <p class="paper_detail"> <b><font color=#404040>arXiv</font></b>, <b>2025</b></font></p>
    <p class="paper_detail"><b>Feb 14, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>Step-Video-T2V</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2502.10248">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/stepfun-ai/Step-Video-T2V/">code</a></p>
    
    
    <div id='Step-Video-T2V-Foundation Algorithms & Models of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020><b>StepFun</b>'s open-sourced model (30B) with DiT structure for text-to-video generation.</font></p>
        
        <p><figure>
<img data-src='resource\figs\Step-Video-T2V\Step-Video-T2V-fig1.png' width=500>
<figcaption>
<b>Figure 1.</b> <b>Structure.</b> A VAE with a 8x8x4 compression ratio and 16 feature channels, bilingual text encoders (HunyuanCLIP and Step-LLM), DiT with RoPE-3D and QK-Norm, and a DPO pipeline. Text prompt conditions are incorporated into DiT by cross-attention modules.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Step-Video-T2V\Step-Video-T2V-fig2.png' width=400>
<figcaption>
<b>Figure 2.</b> <b>VAE</b> compresses videos by 16x16x8 with 16 feature channels.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Step-Video-T2V\Step-Video-T2V-fig4.png' width=500>
<figcaption>
<b>Figure 3.</b> <b>DPO framework.</b> Use training data prompts and handcrafted prompts to generate samples, which are scored through human annotation or reward models. Diffusion-DPO method is adapted here by reducing beta and increasing learning rate for achieving faster convergence.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Step-Video-T2V\Step-Video-T2V-fig5.png' width=900>
<figcaption>
<b>Figure 4.</b> <b>Data pool with 2B video-text pairs & 3.8B image-text pairs.</b> <i>Filters:</i> video segmentation, video quality assessment, aesthetic score, NSFW score, watermark detection, subtitle detection, saturation score, blur score, black border detection, video motion assessment, K-means-based concept balancing, and CLIP score alignment. <i>Video captioning:</i> short caption, dense caption, and original title.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Step-Video-T2V\Step-Video-T2V-fig3.png' width=550>
<figcaption>
<b>Figure 5.</b> <b>Pre-training stages.</b>
</figcaption>
</figure></p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='Flow Matching Guidefoundation algorithms & models of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('Flow Matching Guide-Foundation Algorithms & Models of Generation-details')"><i>Flow Matching Guide and Code</i></p>
    <p class="paper_detail">Yaron Lipman, Marton Havasi, Peter Holderrieth, Neta Shaul, Matt Le, Brian Karrer, Ricky T. Q. Chen, David Lopez-Paz, Heli Ben-Hamu, Itai Gat</p>
    <p class="paper_detail">FAIR at Meta, MIT CSAIL, Weizmann Institute of Science</p>
    <p class="paper_detail"> <b><font color=#404040>arXiv</font></b>, <b>2024</b></font></p>
    <p class="paper_detail"><b>Dec 09, 2024</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>Flow Matching Guide</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2412.06264">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/facebookresearch/flow_matching">code</a></p>
    
    
    <div id='Flow Matching Guide-Foundation Algorithms & Models of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It offers a comprehensive and self-contained <b>review of flow matching</b>, covering its mathematical foundations, design choices, and extensions.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='Infinityfoundation algorithms & models of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('Infinity-Foundation Algorithms & Models of Generation-details')"><i>Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis</i></p>
    <p class="paper_detail">Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, Xiaobing Liu</p>
    <p class="paper_detail">ByteDance</p>
    <p class="paper_detail">Conference on Computer Vision and Pattern Recognition (<b><font color=#404040>CVPR</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Dec 05, 2024</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>Infinity</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2412.04431">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/FoundationVision/Infinity/">code</a></p>
    
    
    <div id='Infinity-Foundation Algorithms & Models of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>It improves VAR by applying <b>bitwise modeling</b> that makes vocabulary "infinity" to open up new posibilities of discrete text-to-image generation.</font></p>
        
        <p><figure>
<img data-src='resource\figs\Infinity\Infinity-fig1.png' width=450>
<figcaption>
<b>Figure 1.</b> <b>Viusal tokenization and quantization.</b> Instead of predicting <i>2**d</i> indices, infinite-vocabulary classifier predicts <i>d</i> bits instead.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Infinity\Infinity-fig3.png' width=350>
<figcaption>
<b>Figure 2.</b> <b>Infinity</b> is fast and better.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Infinity\Infinity-fig4.png' width=300>
<figcaption>
<b>Figure 3.</b> <b>Tokenizer</b> outperforms continuous SD VAE.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Infinity\Infinity-fig5.png' width=450>
<figcaption>
<b>Figure 4.</b> <b>Inifinite-Vocabulary Classifier</b> needs low memory but performs better.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Infinity\Infinity-fig8.png' width=400>
<figcaption>
<b>Figure 5.</b> <b>Self-correction</b> mitigates the train-test discrepancy.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Infinity\Infinity-fig6.png' width=700>
<figcaption>
<b>Figure 6.</b> <b>Vocabulary scales well.</b>
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Infinity\Infinity-fig7.png' width=650>
<figcaption>
<b>Figure 7.</b> <b>Scaling up model size.</b> There is strong correlation between validation loss and evaluation metrics (as observed by Fluid).
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Infinity\Infinity-fig2.png' width=700>
<figcaption>
<b>Figure 8.</b> Using <b>2D RoPE</b> outperforms using APE.
</figcaption>
</figure></p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='HunyuanVideofoundation algorithms & models of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('HunyuanVideo-Foundation Algorithms & Models of Generation-details')"><i>HunyuanVideo: A Systematic Framework For Large Video Generative Models</i></p>
    <p class="paper_detail">Hunyuan Multimodal Generation Team</p>
    <p class="paper_detail">Tencent</p>
    <p class="paper_detail"> <b><font color=#404040>arXiv</font></b>, <b>2024</b></font></p>
    <p class="paper_detail"><b>Dec 03, 2024</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>HunyuanVideo</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2412.03603">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/Tencent/HunyuanVideo/">code</a></p>
    
    
    <div id='HunyuanVideo-Foundation Algorithms & Models of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020><b>Tencent Hunyuan Team</b>'s open-sourced text-to-video and image-to-video generation model (13B) with diffusion transformer (FLUX structure).</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='Movie Genfoundation algorithms & models of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('Movie Gen-Foundation Algorithms & Models of Generation-details')"><i>Movie Gen: A Cast of Media Foundation Models</i></p>
    <p class="paper_detail">Movie Gen Team</p>
    <p class="paper_detail">Meta</p>
    <p class="paper_detail"> <b><font color=#404040>arXiv</font></b>, <b>2024</b></font></p>
    <p class="paper_detail"><b>Oct 17, 2024</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>Movie Gen</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2410.13720">paper</a></p>
    
    
    <div id='Movie Gen-Foundation Algorithms & Models of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020><b>Meta Movie Gen Team</b>'s diffusion transformer-based model (30B) for 16s / 1080p / 16fps video and synchronized audio generation.</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='Fluidfoundation algorithms & models of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('Fluid-Foundation Algorithms & Models of Generation-details')"><i>Fluid: Scaling Autoregressive Text-to-image Generative Models with Continuous Tokens</i></p>
    <p class="paper_detail">Lijie Fan, Tianhong Li, Siyang Qin, Yuanzhen Li, Chen Sun, Michael Rubinstein, Deqing Sun, Kaiming He, Yonglong Tian</p>
    <p class="paper_detail">Google DeepMind, MIT</p>
    <p class="paper_detail">International Conference on Learning Representations (<b><font color=#404040>ICLR</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Oct 17, 2024</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>Fluid</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2410.13863">paper</a></p>
    
    
    <div id='Fluid-Foundation Algorithms & Models of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>It shows auto-regressive models with <b>continuous tokens beat discrete tokens counterpart</b>, and finds some empirical observations during scaling.</font></p>
        
        <p><figure>
<img data-src='resource\figs\Fluid\Fluid-fig1.png' width=450>
<figcaption>
<b>Figure 1.</b> <b>Image tokenizer:</b> discrete (VQGAN) or continuous (VAE). <b>Text tokenizer:</b> discrete (T5-XXL). <b>Model structure:</b> transformer with cross-attention modules attending to text embeddings. <b>Loss:</b> cross-entropy loss on text tokens and diffusion loss on image tokens.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Fluid\Fluid-fig2.png' width=600>
<figcaption>
<b>Figure 2.</b> <b>Scaling</b> behavior of validation loss on <b>model size</b>.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Fluid\Fluid-fig3.png' width=600>
<figcaption>
<b>Figure 3.</b> <b>Random-order masks</b> on <b>continuous image tokens</b> perform the best. Continuous prefers random order, discrete prefers raster order.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Fluid\Fluid-fig4.png' width=550>
<figcaption>
<b>Figure 4.</b> Random-order masks on continuous tokens <b>scale with training computes</b>.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Fluid\Fluid-fig5.png' width=550>
<figcaption>
<b>Figure 5.</b> Strong correlation between <b>validation loss</b> and <b>evaluation metrics</b>.
</figcaption>
</figure></p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='DiT-MoEfoundation algorithms & models of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('DiT-MoE-Foundation Algorithms & Models of Generation-details')"><i>Scaling Diffusion Transformers to 16 Billion Parameters</i></p>
    <p class="paper_detail">Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, Junshi Huang</p>
    <p class="paper_detail">Kunlun Inc.</p>
    <p class="paper_detail"> <b><font color=#404040>arXiv</font></b>, <b>2024</b></font></p>
    <p class="paper_detail"><b>Jul 16, 2024</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>DiT-MoE</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2407.11633">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/feizc/DiT-MoE/">code</a></p>
    
    
    <div id='DiT-MoE-Foundation Algorithms & Models of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>It proposes diffusion transformer (16B) with <b>Mixture-of-Experts</b> by inserting experts into DiT blocks for image generation.</font></p>
        
        <p><ul>
<li> Incorporating <b>shared expert routing</b> improves convergence and performance, but the improvement is little when using more than one.
<li> <b>Increasing experts</b> reduces loss but introduces more loss spikes.
</ul>
<figure>
<img data-src='resource\figs\DiT-MoE\DiT-MoE-fig1.png' width=550>
<figcaption>
<b>Figure 1.</b> <b>Structure.</b> It is built upon DiT and replaces MLP within Transformer blocks by sparsely activated mixture of MLPs as experts.
</figcaption>
</figure></p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='LlamaGenfoundation algorithms & models of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('LlamaGen-Foundation Algorithms & Models of Generation-details')"><i>Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation</i></p>
    <p class="paper_detail">Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, Zehuan Yuan</p>
    <p class="paper_detail">The University of Hong Kong, ByteDance</p>
    <p class="paper_detail"> <b><font color=#404040>arXiv</font></b>, <b>2024</b></font></p>
    <p class="paper_detail"><b>Jun 10, 2024</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>LlamaGen</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2406.06525">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/FoundationVision/LlamaGen/">code</a></p>
    
    
    <div id='LlamaGen-Foundation Algorithms & Models of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>It shows that applying "next-token prediction" to <b>vanilla autoregressive language models</b> can achieve good  image generation performance.</font></p>
        
        <p>
<ul>
    <li> It trains a <b>discrete visual tokenizer</b> that is competitive to the continuous ones, e.g., SD VAE, SDXL VAE, Consistency Decoder from OpenAI.
    <li> It shows that <b>vanilla autoregressive models</b>, e.g., LlaMA, without visual inductive biases can serve as the basis of image generation system.
    <li> <b>Training data.</b> 50M subset of LAION-COCO and 10M internal high aesthetics quality images.
</ul>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='VARfoundation algorithms & models of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('VAR-Foundation Algorithms & Models of Generation-details')"><i>Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction</i></p>
    <p class="paper_detail">Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, Liwei Wang</p>
    <p class="paper_detail">Peking University, Bytedance</p>
    <p class="paper_detail">Advances in Neural Information Processing Systems (<b><font color=#404040>NeurIPS</font></b>), <b>2024</b></font></p>
    <p class="paper_detail"><b>Apr 03, 2024</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#C55253>VAR</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2404.02905">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/FoundationVision/VAR/">code</a></p>
    <p class="paper_detail"><font color=#C55253>NeurIPS 2024 best paper award.</font></p>
    
    <div id='VAR-Foundation Algorithms & Models of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>It improves auto-regressive image generation on image quality, inference speed, data efficiency, and scalability, by proposing <b>next-scale prediction</b>.</font></p>
        
        <p><figure>
<img data-src='resource\figs\VAR\VAR-fig1.png' width=650>
<figcaption>
<b>Figure 1.</b> <b>Next-scale prediction.</b> Start from 1x1 token map; at each step, it predicts the next higher-resolution token map given all previous ones.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\VAR\VAR-fig3.png' width=750>
<figcaption>
<b>Figure 2.</b> <b>Training pipeline of tokenzier and VAR.</b>  Tokenzier (similar to VQ-VAE): the same architecture and training data (OpenImages), using codebook of 4096 and spatial downsample ratio of 16. VAR: the standard transformer with AdaLN; not use RoPE, SwiGLU MLP, RMS Norm.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\VAR\VAR-fig4.png' width=650>
<figcaption>
<b>Figure 3.</b> <b>Encoding & decoding of tokenizer.</b>
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\VAR\VAR-fig2.png' width=350>
<figcaption>
<b>Figure 4.</b> VAR shows good <b>scaling behavior</b>, and significantly outperforms DiT.
</figcaption>
</figure></p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='SDXLfoundation algorithms & models of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('SDXL-Foundation Algorithms & Models of Generation-details')"><i>SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis</i></p>
    <p class="paper_detail">Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, Robin Rombach</p>
    <p class="paper_detail">Stability AI</p>
    <p class="paper_detail">International Conference on Learning Representations (<b><font color=#404040>ICLR</font></b>), <b>2024</b></font></p>
    <p class="paper_detail"><b>Jul 04, 2023</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#C55253>SDXL</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2307.01952">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/Stability-AI/generative-models/">code</a></p>
    
    
    <div id='SDXL-Foundation Algorithms & Models of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>It improves older SD by employing <b>larger UNet backbone</b>, <b>resolution conditions</b>, <b>two text encoders</b>, and a <b>refinement model</b>.</font></p>
        
        <p><p><b>Architecture of SDXL:</b>.<br>
(1) It has 2.6B parameters with different transformer blocks, SD 1.4/1.5/2.0/2.1 has about 860M parameters.<br>
(2) It uses two text encoders: OpenCLIP ViT-bigG & CLIP ViT-L.<br>
(3) The embeddings of height & width and cropping top & left and bucketing heigh & width are added to timestep embeddings as conditions.<br>
(4) It improves VAE by employing EMA and a larger batchsize of 256.<br>
(5) It employs a refinement model of SDEdit to refine visual details.</p>
<p><b>Training stages:</b> (1) reso=256x256, steps=600,000, batchsize=2048; (2) reso=512x512, steps=200,000; (3) mixed resolution and aspect ratio training.</p>
<figure>
<img data-src='resource\figs\SDXL\SDXL-fig1.png' width=500>
<figcaption>
<b>Figure 1.</b> <b>SDXL Structure.</b>
</figcaption>
</figure></p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='DiTfoundation algorithms & models of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('DiT-Foundation Algorithms & Models of Generation-details')"><i>Scalable Diffusion Models with Transformers</i></p>
    <p class="paper_detail">William Peebles, Saining Xie</p>
    <p class="paper_detail">UC Berkeley, New York University</p>
    <p class="paper_detail">International Conference on Computer Vision (<b><font color=#404040>ICCV</font></b>), <b>2023</b></font></p>
    <p class="paper_detail"><b>Dec 19, 2022</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#C55253>DiT</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2212.09748">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/facebookresearch/DiT/">code</a></p>
    
    
    <div id='DiT-Foundation Algorithms & Models of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>It replaces the conventional U-Net structure with <b>transformer</b> for scalable image generation, the timestep and condition are injected by adaLN-Zero.</font></p>
        
        <p><a href="https://github.com/junkunyuan/junkunyuan.github.io/blob/main/paper_reading_list/resource/jupyters/DiT.ipynb" class="note">(see notes in jupyter)</a></p>
        
        <p><figure>
<img data-src='resource\figs\DiT\DiT-fig1.png' width=700>
<figcaption>
<b>Figure 1.</b> <b>Model structure.</b> Use <i>adaLN-Zero</i> structure to inject timestep and class condition performs better than using cross-attention or in-context.
</figcaption>
</figure></p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='Flow Matchingfoundation algorithms & models of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('Flow Matching-Foundation Algorithms & Models of Generation-details')"><i>Flow Matching for Generative Modeling</i></p>
    <p class="paper_detail">Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, Matt Le</p>
    <p class="paper_detail">Meta AI (FAIR), Weizmann Institute of Science</p>
    <p class="paper_detail">International Conference on Learning Representations (<b><font color=#404040>ICLR</font></b>), <b>2023</b></font></p>
    <p class="paper_detail"><b>Oct 06, 2022</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#C55253>Flow Matching</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2210.02747">paper</a></p>
    <p class="paper_detail"><font color=#C55253>It proposes a simple yet powerful framework for training continuous-time generative models by directly learning the vector field of an optimal transport path between data and noise, enabling fast training, stable optimization, and high sample quality. It has over 1,800 citations (as of Aug 2025).</font></p>
    
    <div id='Flow Matching-Foundation Algorithms & Models of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It trains <b>continuous normalizing flows</b> using conditional probability paths, resulting in fast training, high sample quality, and efficient sampling.
</font></p>
        
        <p><a href="https://github.com/junkunyuan/junkunyuan.github.io/blob/main/paper_reading_list/resource/jupyters/FlowMatching.ipynb" class="note">(see notes in jupyter)</a></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='Unified Perspectivefoundation algorithms & models of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('Unified Perspective-Foundation Algorithms & Models of Generation-details')"><i>Understanding Diffusion Models: A Unified Perspective</i></p>
    <p class="paper_detail">Calvin Luo</p>
    <p class="paper_detail">Google Brain</p>
    <p class="paper_detail"> <b><font color=#404040>arXiv</font></b>, <b>2022</b></font></p>
    <p class="paper_detail"><b>Aug 25, 2022</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>Unified Perspective</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2208.11970">paper</a></p>
    
    
    <div id='Unified Perspective-Foundation Algorithms & Models of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>Introduction to VAE, DDPM, score-based generative model, guidance from a <b>unified generative perspective</b>.</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='CogVideofoundation algorithms & models of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('CogVideo-Foundation Algorithms & Models of Generation-details')"><i>CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers</i></p>
    <p class="paper_detail">Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, Jie Tang</p>
    <p class="paper_detail">Tsinghua University, BAAI</p>
    <p class="paper_detail">International Conference on Learning Representations (<b><font color=#404040>ICLR</font></b>), <b>2023</b></font></p>
    <p class="paper_detail"><b>May 29, 2022</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>CogVideo</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2205.15868">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/THUDM/CogVideo/">code</a></p>
    
    
    <div id='CogVideo-Foundation Algorithms & Models of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>It proposes a transformer-based video generation model (9B) that performs <b>auto-regressive</b> frame  generation and recursive frame interpolatation</font></p>
        
        <p><figure>
<img data-src='resource\figs\CogVideo\CogVideo-fig1.png' width=450>
<figcaption>
<b>Figure 1.</b> <b>Model structure & training.</b> CogVideo is trained upon CogView2. It generates frames autoregressively and interpolates them recursively.
</figcaption>
</figure></p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='LDMfoundation algorithms & models of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('LDM-Foundation Algorithms & Models of Generation-details')"><i>High-Resolution Image Synthesis with Latent Diffusion Models</i></p>
    <p class="paper_detail">Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer</p>
    <p class="paper_detail">Heidelberg University, Runway ML</p>
    <p class="paper_detail">Conference on Computer Vision and Pattern Recognition (<b><font color=#404040>CVPR</font></b>), <b>2022</b></font></p>
    <p class="paper_detail"><b>Dec 20, 2021</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#C55253>LDM</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2112.10752">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/CompVis/latent-diffusion/">code</a></p>
    <p class="paper_detail"><font color=#C55253>It significantly advances visual generation field by enabling efficient, high-quality synthesis via latent-space diffusion. It has over 20,000 citations (as of Jul 2025).</font></p>
    
    <div id='LDM-Foundation Algorithms & Models of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>It achieves efficient high-resolution image generation by applying diffusion and denoising processes in the <b>compressed VAE latent space</b>.</font></p>
        
        <p><figure>
<img data-src='resource\figs\LDM\LDM-fig1.png' width=350>
<figcaption>
<b>Figure 1.</b> <b>Strucuture.</b> The conditions are injected by cross-attention.
</figcaption>
</figure></p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='CFGfoundation algorithms & models of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('CFG-Foundation Algorithms & Models of Generation-details')"><i>Classifier-Free Diffusion Guidance</i></p>
    <p class="paper_detail">Jonathan Ho, Tim Salimans</p>
    <p class="paper_detail">Google Research, Brain team</p>
    <p class="paper_detail">Advances in Neural Information Processing Systems (<b><font color=#404040>NeurIPS workshop</font></b>), <b>2021</b></font></p>
    <p class="paper_detail"><b>Dec 08, 2021</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#C55253>CFG</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2207.12598">paper</a></p>
    
    
    <div id='CFG-Foundation Algorithms & Models of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>It improves conditional image generation with <b>classifier-free condition guidance</b> by jointly training a conditional model and an unconditional model.</font></p>
        
        <p><a href="https://github.com/junkunyuan/junkunyuan.github.io/blob/main/paper_reading_list/resource/jupyters/CFG.ipynb" class="note">(see notes in jupyter)</a></p>
        
        <p></p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='DDIMfoundation algorithms & models of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('DDIM-Foundation Algorithms & Models of Generation-details')"><i>Denoising Diffusion Implicit Models</i></p>
    <p class="paper_detail">Jiaming Song, Chenlin Meng, Stefano Ermon</p>
    <p class="paper_detail">Stanford University</p>
    <p class="paper_detail">International Conference on Learning Representations (<b><font color=#404040>ICLR</font></b>), <b>2021</b></font></p>
    <p class="paper_detail"><b>Oct 06, 2020</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#C55253>DDIM</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2010.02502">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/ermongroup/ddim/">code</a></p>
    
    
    <div id='DDIM-Foundation Algorithms & Models of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>Accelerate sampling of diffusion models by introducing a <b>non-Markovian, deterministic process</b> that achieves high-quality results with fewer steps while preserving training consistency.</font></p>
        
        <p><a href="https://github.com/junkunyuan/junkunyuan.github.io/blob/main/paper_reading_list/resource/jupyters/DDIM.ipynb" class="note">(see notes in jupyter)</a></p>
        
        <p><figure>
<img data-src='resource\figs\DDIM\DDIM-fig1.png' width=500>
<figcaption>
<b>Figure 1.</b> Comparisons between <b>Markovian DDPM</b> (left) and <b>non-Markovian DDIM</b> (right).
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\DDIM\DDIM-fig2.png' width=250>
<figcaption>
<b>Figure 2.</b> <b>Accelerate sampling</b> by skipping time steps.
</figcaption>
</figure></p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='DDPMfoundation algorithms & models of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('DDPM-Foundation Algorithms & Models of Generation-details')"><i>Denoising Diffusion Probabilistic Models</i></p>
    <p class="paper_detail">Jonathan Ho, Ajay Jain, Pieter Abbeel</p>
    <p class="paper_detail">UC Berkeley</p>
    <p class="paper_detail">Advances in Neural Information Processing Systems (<b><font color=#404040>NeurIPS</font></b>), <b>2020</b></font></p>
    <p class="paper_detail"><b>Jun 19, 2020</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#C55253>DDPM</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2006.11239">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/hojonathanho/diffusion/">code</a></p>
    <p class="paper_detail"><font color=#C55253>It shows that a simple, theoretically grounded denoising process can rival and even surpass GANs in sample quality, sparking an explosion of diffusion-based research and aplications across images, videos, audio, and beyond. It has over 20,000 citations (as of Jul 2025).</font></p>
    
    <div id='DDPM-Foundation Algorithms & Models of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>It proposes <b>denoising diffusion probabilistic models</b> that iteratively denoises data from random noise.</font></p>
        
        <p><a href="https://github.com/junkunyuan/junkunyuan.github.io/blob/main/paper_reading_list/resource/jupyters/DDPM.ipynb" class="note">(see notes in jupyter)</a></p>
        
        <p><figure>
<img data-src='resource\figs\DDPM\DDPM-fig1.png' width=400>
<figcaption>
<b>Figure 1.</b> <b>Diffusion (forward) & denoising (reverse) processes.</b>
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\DDPM\DDPM-fig2.png' width=550>
<figcaption>
<b>Figure 2.</b> <b>Training & sampling algorithms.</b>
</figcaption>
</figure></p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='VQ-VAE-2foundation algorithms & models of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('VQ-VAE-2-Foundation Algorithms & Models of Generation-details')"><i>Generating Diverse High-Fidelity Images with VQ-VAE-2</i></p>
    <p class="paper_detail">Ali Razavi, Aaron van den Oord, Oriol Vinyals</p>
    <p class="paper_detail">DeepMind</p>
    <p class="paper_detail">Advances in Neural Information Processing Systems (<b><font color=#404040>NeurIPS</font></b>), <b>2019</b></font></p>
    <p class="paper_detail"><b>Jun 02, 2019</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#C55253>VQ-VAE-2</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/1906.00446">paper</a></p>
    
    
    <div id='VQ-VAE-2-Foundation Algorithms & Models of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>In order to generate large scale images efficiently, it improves VQ-VAE by employing a <b>hierarchical organization</b>.</font></p>
        
        <p><ul>
<li><b>Structure:</b> (1) a top-level encoder to learn top-level priors from images; (2) a bottom-level encoder to learn bottom-level priors from images and top-level priors; (3) a decoder to generate images from both top-level and bottom-level priors.</li>
<li><b>Training stage 1:</b> training the top-level encoder and the bottom-level encoder to encode images onto the two levels of discrete latent space.</li>
<li><b>Training stage 2:</b> training PixelCNN to predict bottom-level priors from top-level priors, while fixing the two encoders.</li>
<li><b>Sampling:</b> (1) sampling a top-level prior; (2) predicting bottom-level prior from the top-level prior using the trained PixelCNN; (3) generating images from both the top-level and the bottom-level priors by the trained decoder.</li>
</ul>
<figure>
<img data-src='resource\figs\VQ-VAE-2\VQ-VAE-2-fig1.png' width=850>
<figcaption>
<b>Figure 1.</b> <b>Training (left) & sampling (right) frameworks.</b>
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\VQ-VAE-2\VQ-VAE-2-fig2.png' width=500>
<figcaption>
<b>Figure 2.</b> <b>Training & sampling algorithms.</b>
</figcaption>
</figure></p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='VQ-VAEfoundation algorithms & models of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('VQ-VAE-Foundation Algorithms & Models of Generation-details')"><i>Neural Discrete Representation Learning</i></p>
    <p class="paper_detail">Aaron van den Oord, Oriol Vinyals, Koray Kavukcuoglu</p>
    <p class="paper_detail">DeepMind</p>
    <p class="paper_detail">Advances in Neural Information Processing Systems (<b><font color=#404040>NeurIPS</font></b>), <b>2017</b></font></p>
    <p class="paper_detail"><b>Nov 02, 2017</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#C55253>VQ-VAE</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/1711.00937">paper</a></p>
    
    
    <div id='VQ-VAE-Foundation Algorithms & Models of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>It proposes <b>vector quantised variational autoencoder</b> to generate discrete codes while the prior is also learned.</font></p>
        
        <p><ul>
<li><b>Posterior collapse problem:</b> a strong decoder and a strong KL constraint could make the learned posterior <i>q(z|x)</i> very close to prior <i>p(z)</i>, so that the conditional generation task collapses to an unconditional generation task.</li>
<li><b>How VQ-VAE avoids the collapse problem by employing discrete codes/latents?</b> (1) It learns <i>q(z|x)</i> by choosing one from some candidates rather than directly generating a simple prior; (2) The learned <i>q(z|x)</i> is continuous but <i>p(z)</i> is discrete, so the encoder can not be "lazy".</li>
<li><b>Optimization objectives:</b> (1) The decoder is optimized by a recontruction loss; (2) The encoder is optimized by a reconstruction loss and a matching loss; (3) The embedding is optimized by a matching loss.</li>
<li><b>How to back-propagate gradient with quantization exists? Straight-Through Estimator:</b> directly let the graident of loss to the quantized embedding equal to the gradient of loss to the embedding that before being quantized.</li>
</ul>
<figure>
<img data-src='resource\figs\VQ-VAE\VQ-VAE-fig1.png' width=900>
<figcaption>
<b>Figure 1.</b> <b>Model structure.</b>
</figcaption>
</figure></p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    <h2 id="Reinforcement Learning of Generation-table"><a class="no_dec" href="#Reinforcement Learning of Generation">Reinforcement Learning of Generation</a></h2>
    <p class="little_split" id='RDPOreinforcement learning of generation'></p>
    <div style="border-left: 16px solid #A0D0A0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('RDPO-Reinforcement Learning of Generation-details')"><i>RDPO: Real Data Preference Optimization for Physics Consistency Video Generation</i></p>
    <p class="paper_detail">Wenxu Qian, Chaoyue Wang, Hou Peng, Zhiyu Tan, Hao Li, Anxiang Zeng</p>
    <p class="paper_detail">Fudan University, Shopee Inc</p>
    <p class="paper_detail"> <b><font color=#404040>arXiv</font></b>, <b>2025</b></font></p>
    <p class="paper_detail"><b>Jun 23, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>RDPO</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2506.18655">paper</a></p>
    
    
    <div id='RDPO-Reinforcement Learning of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>It constructs positive DPO data by <b>reversing-then-denoising real data</b> to overcome the large domain gap between synthetic data and real data.</font></p>
        
        <p><ul>
<li> <b>DPO data construction.</b> Positive samples: reversing-then-denoising real data. Negative samples: denoising from noise.
<li> <b>Rejection sampling.</b> select the instance that is closest (L2 distance) to model's own sample at the same timestep.
<li> <b>Progressive training.</b> Apply reversing-then-denoising sequentially from heavy to light. 8K preference pairs.
</ul>
<figure>
<img data-src='resource\figs\RDPO\RDPO-fig1.png' width=500>
<figcaption>
<b>Figure 1.</b> <b>Construct DPO samples</b> (left) & <b>progressive training</b> (right).
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\RDPO\RDPO-fig2.png' width=650>
<figcaption>
<b>Figure 2.</b> Apply <b>SFT loss</b> to DPO loss improves performance.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\RDPO\RDPO-fig3.png' width=700>
<figcaption>
<b>Figure 3.</b> Mix <b>human-annotation data</b> improves performance.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\RDPO\RDPO-fig4.png' width=350>
<figcaption>
<b>Figure 4.</b> <b>Progressive training.</b> Stage 1 requres larger reversing/sampling steps, e.g., 42. Stage 2 requries smaller, e.g., 40.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\RDPO\RDPO-fig5.png' width=700>
<figcaption>
<b>Figure 5.</b> <b>Progressive training</b> is effective.
</figcaption>
</figure></p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='D-Fusionreinforcement learning of generation'></p>
    <div style="border-left: 16px solid #A0D0A0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('D-Fusion-Reinforcement Learning of Generation-details')"><i>D-Fusion: Direct Preference Optimization for Aligning Diffusion Models with Visually Consistent Samples</i></p>
    <p class="paper_detail">Zijing Hu, Fengda Zhang, Kun Kuang</p>
    <p class="paper_detail">Zhejiang University, Nanyang Technological University</p>
    <p class="paper_detail">International Conference on Machine Learning (<b><font color=#404040>ICML</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>May 28, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>D-Fusion</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2505.22002">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/hu-zijing/D-Fusion">code</a></p>
    
    
    <div id='D-Fusion-Reinforcement Learning of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>It constructs new winning samples that are <b>visually consistent</b> with losing samples while aligning with winning samples in <b>prompt-following</b>.</font></p>
        
        <p>
<ul>
    <li> <b>Method details.</b> (1) It extracts a mask by <b>averaging cross-attention maps</b> across all heads and blocks in the first up-sampling layer. (2) It <b>binarizes the mask</b> by applying a fixed threshold, generating segmentation of attention, i.e., the importance map for the prompt. (3) It <b>fuses the self-attention key and value</b> by applying the segmentation mask to winning samples and losing samples.
</ul>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='DanceGRPOreinforcement learning of generation'></p>
    <div style="border-left: 16px solid #A0D0A0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('DanceGRPO-Reinforcement Learning of Generation-details')"><i>DanceGRPO: Unleashing GRPO on Visual Generation</i></p>
    <p class="paper_detail">Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan Guo, Weilin Huang, Ping Luo</p>
    <p class="paper_detail">ByteDance Seed, The University of Hong Kong</p>
    <p class="paper_detail"> <b><font color=#404040>arXiv</font></b>, <b>2025</b></font></p>
    <p class="paper_detail"><b>May 12, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>DanceGRPO</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2505.07818">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/XueZeyue/DanceGRPO">code</a></p>
    
    
    <div id='DanceGRPO-Reinforcement Learning of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>It reformulates ODE sampling to <b>SDE</b>, and adapts <b>GRPO</b> to visual generation, validating on different models, tasks, and reward models.</font></p>
        
        <p><ul>
<li> It <b>omits the KL regularization</b> of GRPO because it brings little benefits.
<li> It <b>assigns shared initialization noise</b> to samples from the same prompt to solve reward hacking and training instability.
</ul>
<figure>
<img data-src='resource\figs\DanceGRPO\DanceGRPO-fig1.png' width=550>
<figcaption>
<b>Figure 1.</b> <b>Algorithm.</b>
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\DanceGRPO\DanceGRPO-fig2.png' width=600>
<figcaption>
<b>Figure 2.</b> Left: Sampling top k and bottom k with <b>best-of-N</b> improves results. Right: Training with some <b>timestep subsets</b> is potential.
</figcaption>
</figure></p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='InPOreinforcement learning of generation'></p>
    <div style="border-left: 16px solid #A0D0A0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('InPO-Reinforcement Learning of Generation-details')"><i>InPO: Inversion Preference Optimization with Reparametrized DDIM for Efficient Diffusion Model Alignment</i></p>
    <p class="paper_detail">Yunhong Lu, Qichao Wang, Hengyuan Cao, Xierui Wang, Xiaoyin Xu, Min Zhang</p>
    <p class="paper_detail">Zhejiang University, Shanghai Institute for Advanced Study-Zhejiang University</p>
    <p class="paper_detail">Conference on Computer Vision and Pattern Recognition (<b><font color=#404040>CVPR</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Mar 24, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>InPO</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2503.18454">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/JaydenLyh/InPO">code</a></p>
    
    
    <div id='InPO-Reinforcement Learning of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>Instead of maximizing accumulative rewards, it only maximizes the reward of the latent variable that have a strong correlation with the data.</font></p>
        
        <p><figure>
<img data-src='resource\figs\InPO\InPO-fig2.png' width=300>
<figcaption>
<b>Figure 1.</b> It is more <b>trainig efficient</b>.
</figcaption>
</figure></p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='Survey on Pre. Ali.reinforcement learning of generation'></p>
    <div style="border-left: 16px solid #A0D0A0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('Survey on Pre. Ali.-Reinforcement Learning of Generation-details')"><i>Preference Alignment on Diffusion Model: A Comprehensive Survey for Image Generation and Editing</i></p>
    <p class="paper_detail">Sihao Wu, Xiaonan Si, Chi Xing, Jianhong Wang, Gaojie Jin, Guangliang Cheng, Lijun Zhang, Xiaowei Huang</p>
    <p class="paper_detail">University of Liverpool, Institute of Software Chinese Academy of Sciences, University of Edinburgh, University of Bristol, University of Exeter</p>
    <p class="paper_detail"> <b><font color=#404040>arXiv</font></b>, <b>2025</b></font></p>
    <p class="paper_detail"><b>Feb 10, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>Survey on Pre. Ali.</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2502.07829">paper</a></p>
    
    
    <div id='Survey on Pre. Ali.-Reinforcement Learning of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>A survey on preference alignment of image generation and editing.</font></p>
        
        <p><figure>
<img data-src='resource\figs\Survey on Pre. Ali.\Survey on Pre. Ali.-fig1.png' width=700>
<figcaption>
<b>Figure 1.</b> <b>Preference Alignment methods for Image Generation.</b>
</figcaption>
</figure></p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='CaPOreinforcement learning of generation'></p>
    <div style="border-left: 16px solid #A0D0A0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('CaPO-Reinforcement Learning of Generation-details')"><i>Calibrated Multi-Preference Optimization for Aligning Diffusion Models</i></p>
    <p class="paper_detail">Kyungmin Lee, Xiahong Li, Qifei Wang, Junfeng He, Junjie Ke, Ming-Hsuan Yang, Irfan Essa, Jinwoo Shin, Feng Yang, Yinxiao Li1</p>
    <p class="paper_detail">Google DeepMind, KAIST, Google, Google Research, Georgia Institute of Technology</p>
    <p class="paper_detail">Conference on Computer Vision and Pattern Recognition (<b><font color=#404040>CVPR</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Feb 04, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>CaPO</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2502.02588">paper</a></p>
    
    
    <div id='CaPO-Reinforcement Learning of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>It proposes calibrated preference optimization by calculating average win-rate of each sample to other samples as the reward.</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='Flow-RWR, Flow-DPOreinforcement learning of generation'></p>
    <div style="border-left: 16px solid #A0D0A0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('Flow-RWR, Flow-DPO-Reinforcement Learning of Generation-details')"><i>Improving Video Generation with Human Feedback</i></p>
    <p class="paper_detail">Jie Liu, Gongye Liu, Jiajun Liang, Ziyang Yuan, Xiaokun Liu, Mingwu Zheng, Xiele Wu, Qiulin Wang, Wenyu Qin, Menghan Xia, Xintao Wang, Xiaohong Liu, Fei Yang, Pengfei Wan, Di Zhang, Kun Gai, Yujiu Yang, Wanli Ouyang</p>
    <p class="paper_detail">CUHK, Tsinghua University, Kuaishou Technology, Shanghai Jiao Tong University, Shanghai AI Lab</p>
    <p class="paper_detail"> <b><font color=#404040>arXiv</font></b>, <b>2025</b></font></p>
    <p class="paper_detail"><b>Jan 23, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>Flow-RWR, Flow-DPO</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2501.13918">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/KwaiVGI/VideoAlign/">code</a></p>
    
    
    <div id='Flow-RWR, Flow-DPO-Reinforcement Learning of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>It introduces a human preference video dataset, and adapts diffusion-based reinforcement learning to flow-based video generation models.</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='PPDreinforcement learning of generation'></p>
    <div style="border-left: 16px solid #A0D0A0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('PPD-Reinforcement Learning of Generation-details')"><i>Personalized Preference Fine-tuning of Diffusion Models</i></p>
    <p class="paper_detail">Meihua Dang, Anikait Singh, Linqi Zhou, Stefano Ermon, Jiaming Song</p>
    <p class="paper_detail">Stanford University, Luma AI</p>
    <p class="paper_detail">Conference on Computer Vision and Pattern Recognition (<b><font color=#404040>CVPR</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Jan 11, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>PPD</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2501.06655">paper</a></p>
    
    
    <div id='PPD-Reinforcement Learning of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>It introduces <b>personalized preference alignment</b> by injecting VLM embeddings into diffusion models through cross-attention.</font></p>
        
        <p><ul>
<li> <b>VLM.</b> Use LLaVA-OneVision to extract user preference embeddings from few-shot pairwise preference examples for each user.
<li> <b>Cross-attention.</b> Similar to IP-Adapter, it injects VLM-based user embeddin via cross-attention, and adds the embeddings to text embeddings.
</ul>
<figure>
<img data-src='resource\figs\PPD\PPD-fig1.png' width=450>
<figcaption>
<b>Figure 1.</b> <b>Structure.</b> Only the cross-attention module is optimized.
</figcaption>
</figure></p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='VideoDPOreinforcement learning of generation'></p>
    <div style="border-left: 16px solid #A0D0A0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('VideoDPO-Reinforcement Learning of Generation-details')"><i>VideoDPO: Omni-Preference Alignment for Video Diffusion Generation</i></p>
    <p class="paper_detail">Runtao Liu, Haoyu Wu, Zheng Ziqiang, Chen Wei, Yingqing He, Renjie Pi, Qifeng Chen</p>
    <p class="paper_detail">HKUST, Renmin University of China, Johns Hopkins University</p>
    <p class="paper_detail">Conference on Computer Vision and Pattern Recognition (<b><font color=#404040>CVPR</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Dec 18, 2024</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>VideoDPO</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2412.14167">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/CIntellifusion/VideoDPO">code</a></p>
    
    
    <div id='VideoDPO-Reinforcement Learning of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>It builds a metric for quality and semantic alignment evaluation, then uses the metric to build DPO data for preference alignment of video generation.</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='SPOreinforcement learning of generation'></p>
    <div style="border-left: 16px solid #A0D0A0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('SPO-Reinforcement Learning of Generation-details')"><i>Aesthetic Post-Training Diffusion Models from Generic Preferences with Step-by-step Preference Optimization</i></p>
    <p class="paper_detail">Zhanhao Liang, Yuhui Yuan, Shuyang Gu, Bohan Chen, Tiankai Hang, Mingxi Cheng, Ji Li, Liang Zheng</p>
    <p class="paper_detail">The Australian National University, University of Liverpool, Southeast University, Microsoft, Microsoft Research Asia</p>
    <p class="paper_detail">Conference on Computer Vision and Pattern Recognition (<b><font color=#404040>CVPR</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Jun 06, 2024</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>SPO</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2406.04314">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/RockeyCoss/SPO">code</a></p>
    
    
    <div id='SPO-Reinforcement Learning of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>It trains a noise-aware reward model, and contructs DPO data from noisy samples.</font></p>
        
        <p>
<ul>
    <li> <b>DPO data construcion.</b> Start from a initial noise, denoise to some steps and build DPO samples from a noise-aware reward model.
</ul>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='Curriculum DPOreinforcement learning of generation'></p>
    <div style="border-left: 16px solid #A0D0A0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('Curriculum DPO-Reinforcement Learning of Generation-details')"><i>Curriculum Direct Preference Optimization for Diffusion and Consistency Models</i></p>
    <p class="paper_detail">Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, Nicu Sebe, Mubarak Shah</p>
    <p class="paper_detail">University of Bucharest, Romania, University of Trento, Italy, University of Central Florida, US</p>
    <p class="paper_detail">Conference on Computer Vision and Pattern Recognition (<b><font color=#404040>CVPR</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>May 22, 2024</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>Curriculum DPO</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2405.13637">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/CroitoruAlin/Curriculum-DPO">code</a></p>
    
    
    <div id='Curriculum DPO-Reinforcement Learning of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>It applies <b>curriculum learning</b> to DPO by learning from win samples and lose samples with their differences from small to large.</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='InstructVideoreinforcement learning of generation'></p>
    <div style="border-left: 16px solid #A0D0A0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('InstructVideo-Reinforcement Learning of Generation-details')"><i>InstructVideo: Instructing Video Diffusion Models with Human Feedback</i></p>
    <p class="paper_detail">Hangjie Yuan, Shiwei Zhang, Xiang Wang, Yujie Wei, Tao Feng, Yining Pan, Yingya Zhang, Ziwei Liu, Samuel Albanie, Dong Ni</p>
    <p class="paper_detail">Zhejiang University, Alibaba Group, Tsinghua University, Singapore University of Technology and Design, Nanyang Technological University, University of Cambridge</p>
    <p class="paper_detail">Conference on Computer Vision and Pattern Recognition (<b><font color=#404040>CVPR</font></b>), <b>2024</b></font></p>
    <p class="paper_detail"><b>Dec 19, 2023</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>InstructVideo</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2312.12490">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/ali-vilab/VGen/">code</a></p>
    
    
    <div id='InstructVideo-Reinforcement Learning of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>It uses HPS v2 to provide reward feedback and train video generation models in an editing manner.</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='Diffusion-DPOreinforcement learning of generation'></p>
    <div style="border-left: 16px solid #A0D0A0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('Diffusion-DPO-Reinforcement Learning of Generation-details')"><i>Diffusion Model Alignment Using Direct Preference Optimization</i></p>
    <p class="paper_detail">Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, Nikhil Naik</p>
    <p class="paper_detail">Salesforce AI, Stanford University</p>
    <p class="paper_detail">Conference on Computer Vision and Pattern Recognition (<b><font color=#404040>CVPR</font></b>), <b>2024</b></font></p>
    <p class="paper_detail"><b>Nov 21, 2023</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#C55253>Diffusion-DPO</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2311.12908">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/SalesforceAIResearch/DiffusionDPO/">code</a></p>
    
    
    <div id='Diffusion-DPO-Reinforcement Learning of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>It adapts <b>Direct Preference Optimization (DPO)</b> from large language models to diffusion models.</font></p>
        
        <p><a href="https://github.com/junkunyuan/junkunyuan.github.io/blob/main/paper_reading_list/resource/jupyters/Diffusion-DPO.ipynb" class="note">(see notes in jupyter)</a></p>
        
        <p>
<ul>
    <li> <b>Model & dataset.</b> It trains SD1.5 and SDXL1.0 on <i>Pick-a-Pic</i> human preference data consisting of 850K pairs from 59K unique prompts.
    <li> <b>Evaluations</b> are performed on Pick-a-Pic validation set, Partiprompt, and HPS v2.
</ul>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='DDPOreinforcement learning of generation'></p>
    <div style="border-left: 16px solid #A0D0A0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('DDPO-Reinforcement Learning of Generation-details')"><i>Training Diffusion Models with Reinforcement Learning</i></p>
    <p class="paper_detail">Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, Sergey Levine</p>
    <p class="paper_detail">University of California, Berkeley, Massachusetts Institute of Technology</p>
    <p class="paper_detail">International Conference on Learning Representations (<b><font color=#404040>ICLR</font></b>), <b>2024</b></font></p>
    <p class="paper_detail"><b>May 22, 2023</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#C55253>DDPO</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2305.13301">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/jannerm/ddpo">code</a></p>
    
    
    <div id='DDPO-Reinforcement Learning of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>It applies policy gradient to diffusion models, the reward is estimated from a VLM, to improve its aesthetics.</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='ReFLreinforcement learning of generation'></p>
    <div style="border-left: 16px solid #A0D0A0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('ReFL-Reinforcement Learning of Generation-details')"><i>ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation</i></p>
    <p class="paper_detail">Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, Yuxiao Dong</p>
    <p class="paper_detail">Tsinghua University, Zhipu AI, Beijing University of Posts and Telecommunications</p>
    <p class="paper_detail">Advances in Neural Information Processing Systems (<b><font color=#404040>NeurIPS</font></b>), <b>2023</b></font></p>
    <p class="paper_detail"><b>Apr 12, 2023</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>ReFL</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2304.05977">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/THUDM/ImageReward/">code</a></p>
    
    
    <div id='ReFL-Reinforcement Learning of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>It trains BLIP on 137K human preference image pairs for preference evaluation and use it to tune models by Reward Feedback Learning (ReFL).</font></p>
        
        <p><figure>
<img data-src='resource\figs\ReFL\ReFL-fig1.png' width=600>
<figcaption>
<b>Figure 1.</b> <b>Training pipeline.</b> (1) Use DiffusionDB prompts to generate images; (2) Rank; (3) Train model on ranking data; (4) Tune model via ReFL.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\ReFL\ReFL-fig2.png' width=300>
<figcaption>
<b>Figure 2.</b> <b>ReFL algorithm.</b>
</figcaption>
</figure></p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='promptistreinforcement learning of generation'></p>
    <div style="border-left: 16px solid #A0D0A0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('promptist-Reinforcement Learning of Generation-details')"><i>Optimizing Prompts for Text-to-Image Generation</i></p>
    <p class="paper_detail">Yaru Hao, Zewen Chi, Li Dong, Furu Wei</p>
    <p class="paper_detail">Microsoft Research</p>
    <p class="paper_detail">Advances in Neural Information Processing Systems (<b><font color=#404040>NeurIPS</font></b>), <b>2023</b></font></p>
    <p class="paper_detail"><b>Dec 19, 2022</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>promptist</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2212.09611">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/microsoft/LMOps/">code</a></p>
    
    
    <div id='promptist-Reinforcement Learning of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>It uses <b>LLM to refine prompts</b> for preference-aligned image generation by taking relevance and aesthetics as rewards.</font></p>
        
        <p><figure>
<img data-src='resource\figs\promptist\promptist-fig1.png' width=500>
<figcaption>
<b>Figure 1.</b> <b>Training pipeline.</b> (1) Fine-tune a language model (LM) to learn to optimize prompts; (2) Further fine-tune LM with PPO.
</figcaption>
</figure></p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    <h2 id="Inference-Time Improvement of Generation-table"><a class="no_dec" href="#Inference-Time Improvement of Generation">Inference-Time Improvement of Generation</a></h2>
    <p class="little_split" id='Inference can Beat Pretraininginference-time improvement of generation'></p>
    <div style="border-left: 16px solid #F0BBCC; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('Inference can Beat Pretraining-Inference-Time Improvement of Generation-details')"><i>Ideas in Inference-time Scaling can Benefit Generative Pre-training Algorithms</i></p>
    <p class="paper_detail">Jiaming Song, Linqi Zhou</p>
    <p class="paper_detail">Luma AI</p>
    <p class="paper_detail"> <b><font color=#404040>arXiv</font></b>, <b>2025</b></font></p>
    <p class="paper_detail"><b>Mar 10, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>Inference can Beat Pretraining</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2503.07154">paper</a></p>
    
    
    <div id='Inference can Beat Pretraining-Inference-Time Improvement of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>Analyze generative pre-training from an <b>inference-first</b> idea, and scaling inference from a perspective of scaling sequence length & refinement steps.</font></p>
        
        <p>
<ul>
    <li> Pre-training algorithms should have <b>inference-scalability</b> in sequence length and refinement steps.
    <li> Algorithms should scale training efficiently by <b>reduing inference computation</b>.
    <li> One should verify whether the model has <b>enough capacity</b> to represent the target distribution during inference.
    <li> <b>Not scalable in either sequence length or refinement steps:</b> VAE, GAN, Normalizing Flows.
    <li> <b>Scalable in sequence length but not refinement steps:</b> GPT, PixelCNN, MaskGiT, VAR.
    <li> <b>Scalable in refinement steps but not in sequence length:</b> diffusion models, energy-based models, consistency models.
    <li> <b>Scalable in both, with sequence length in the outer loop:</b> AR-Diffusion, Rolling diffusion, MAR.
    <li> <b>Scalable in both, with refinement steps in the outer loop:</b> autoregression distribution smoothing.
</ul>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='PARMinference-time improvement of generation'></p>
    <div style="border-left: 16px solid #F0BBCC; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('PARM-Inference-Time Improvement of Generation-details')"><i>Can We Generate Images with CoT? Let's Verify and Reinforce Image Generation Step by Step</i></p>
    <p class="paper_detail">Ziyu Guo, Renrui Zhang, Chengzhuo Tong, Zhizheng Zhao, Peng Gao, Hongsheng Li, Pheng-Ann Heng</p>
    <p class="paper_detail">CUHK, Peking University, Shanghai AI Lab</p>
    <p class="paper_detail">Conference on Computer Vision and Pattern Recognition (<b><font color=#404040>CVPR</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Jan 23, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>PARM</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2501.13926">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/ZiyuGuo99/Image-Generation-CoT/">code</a></p>
    
    
    <div id='PARM-Inference-Time Improvement of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>It applies the idea of <b>Chain-of-Thought</b> into image generation and combines it with reinforcement learning to further improve performance.</font></p>
        
        <p><figure>
<img data-src='resource\figs\PARM\PARM-fig1.png' width=600>
<figcaption>
<b>Figure 1.</b> <b>Strategy comparisons.</b> ORM is coarse, PRM does not know when to make decision, PARM combines them.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\PARM\PARM-fig2.png' width=250>
<figcaption>
<b>Figure 2.</b> It is observed that <b>self-correction</b> also works in image generation by fine-tuning Show-o.
</figcaption>
</figure></p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='Inference-Time Scaling Analysisinference-time improvement of generation'></p>
    <div style="border-left: 16px solid #F0BBCC; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('Inference-Time Scaling Analysis-Inference-Time Improvement of Generation-details')"><i>Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps</i></p>
    <p class="paper_detail">Nanye Ma, Shangyuan Tong, Haolin Jia, Hexiang Hu, Yu-Chuan Su, Mingda Zhang, Xuan Yang, Yandong Li, Tommi Jaakkola, Xuhui Jia, Saining Xie</p>
    <p class="paper_detail">NYU, MIT, Google</p>
    <p class="paper_detail">Conference on Computer Vision and Pattern Recognition (<b><font color=#404040>CVPR</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Jan 16, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#C55253>Inference-Time Scaling Analysis</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2501.09732">paper</a></p>
    
    
    <div id='Inference-Time Scaling Analysis-Inference-Time Improvement of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>Analysis on <b>inference-time scaling</b> of diffusion models for image generation from the axes of <b>verifiers</b> and <b>algorithms</b>.</font></p>
        
        <p><ul>
<li>Use some <b>verifiers</b> to provide feedback: FID, IS, CLIP, DINO; Aesthetic Score Predictor, CLIPScore, ImageReward, Ensemble.</li>
<li>Use some <b>algorithms</b> to find better noise: Random Search, Zero-Order Search, Search Over Paths.</li>
<li><b>Random Search:</b> run using different initial random noise and select the best final result by the verifier.</li>
<li><b>Zero-Order Search:</b> run under different random noise around a pivot noise and select the best final result by the verifier, the best one is then served as a new pivot for next round search.</li>
<li><b>Search Over Paths:</b> run under different random noise to a specific step, sample noises for each noisy sample and simulate forward process, then perform denoising and select the best candiate using the verifier, continue this process until finish denoising.</li>
<li><b>Scaling through search</b> leads to substantial improvement across model sizes.</li>
<li>No single <b>verifier-algorithm configuration</b> is universally optimal.</li>
<li><b>Inference-time search</b> further improves performance of the model which has already been fine-tuned.</li>
<li><b>Fewer denoising steps but more searching iterations</b> enables efficient convergence but lower final performance.</li>
<li>With a fixed inference compute budget, performing <b>search on small models</b> can outperform larger models without search.</li>
</ul>
<figure>
<img data-src='resource\figs\Inference-Time Scaling Analysis\Inference-Time Scaling Analysis-fig1.png' width=400>
<figcaption>
<b>Figure 1.</b> <b>Scale with search</b> is more effective than scale with denoising steps.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Inference-Time Scaling Analysis\Inference-Time Scaling Analysis-fig2.png' width=400>
<figcaption>
<b>Figure 2.</b> <b>Random Search performs the best</b> because it has larger space that converges the fastest.
</figcaption>
</figure></p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='Z-Samplinginference-time improvement of generation'></p>
    <div style="border-left: 16px solid #F0BBCC; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('Z-Sampling-Inference-Time Improvement of Generation-details')"><i>Zigzag Diffusion Sampling: Diffusion Models Can Self-Improve via Self-Reflection</i></p>
    <p class="paper_detail">Lichen Bai, Shitong Shao, Zikai Zhou, Zipeng Qi, Zhiqiang Xu, Haoyi Xiong, Zeke Xie</p>
    <p class="paper_detail">The Hong Kong University of Science and Technology (Guangzhou), Mohamed bin Zayed University of Artificial Intelligence, Baidu Inc</p>
    <p class="paper_detail">International Conference on Learning Representations (<b><font color=#404040>ICLR</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Dec 14, 2024</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>Z-Sampling</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2412.10891">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/xie-lab-ml/Zigzag-Diffusion-Sampling/">code</a></p>
    
    
    <div id='Z-Sampling-Inference-Time Improvement of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>It emploits <b>guidance gap between denosing and inversion</b> by iteratively performing them for improve image generation quality.</font></p>
        
        <p><figure>
<img data-src='resource\figs\Z-Sampling\Z-Sampling-fig2.png' width=300>
<figcaption>
<b>Figure 1.</b> It capture more <b>semantics</b> by denoising more times.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Z-Sampling\Z-Sampling-fig1.png' width=300>
<figcaption>
<b>Figure 2.</b> It is more <b>efficient & effective</b> than common denoising.
</figcaption>
</figure></p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    <h2 id="Acceleration of Generation-table"><a class="no_dec" href="#Acceleration of Generation">Acceleration of Generation</a></h2>
    <p class="little_split" id='MeanFlowacceleration of generation'></p>
    <div style="border-left: 16px solid #9ACAF0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('MeanFlow-Acceleration of Generation-details')"><i>Mean Flows for One-step Generative Modeling</i></p>
    <p class="paper_detail">Zhengyang Geng, Mingyang Deng, Xingjian Bai, J. Zico Kolter, Kaiming He</p>
    <p class="paper_detail">CMU, MIT</p>
    <p class="paper_detail"> <b><font color=#404040>arXiv</font></b>, <b>2025</b></font></p>
    <p class="paper_detail"><b>May 19, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#C55253>MeanFlow</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2505.13447">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/Gsunshine/meanflow">code</a></p>
    
    
    <div id='MeanFlow-Acceleration of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It introduces a <b>one-step</b> generative framework that learns an <b>average velocity field</b> via MeanFlow Identity, without distillation or curriculum learning.
</font></p>
        
        <p>
Some observations in experiments:
<ul>
    <li> <b>25% of sampling \( r\neq t \)</b> performs the best; while 0% (standard Flow Matching) performs much worse.
    <li> <b>A correct JVP</b> is important, i.e., \( \mathrm{jvp}=(v, 0, 1) \).
    <li> <b>\( u_{\theta}(z, r, t) \) takes (t, t-r)</b> as the positional embedding performs the best.
</ul>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='SD3-Turboacceleration of generation'></p>
    <div style="border-left: 16px solid #9ACAF0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('SD3-Turbo-Acceleration of Generation-details')"><i>Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation</i></p>
    <p class="paper_detail">Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, Robin Rombach</p>
    <p class="paper_detail">Stability AI</p>
    <p class="paper_detail"> <b><font color=#404040>SIGGRAPH Asia</font></b>, <b>2024</b></font></p>
    <p class="paper_detail"><b>Mar 18, 2024</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#C55253>SD3-Turbo</font></b></p>
    <p class="paper_detail"><a href="https://export.arxiv.org/pdf/2403.12015">paper</a></p>
    
    
    <div id='SD3-Turbo-Acceleration of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>It performs distillation of diffusion models in <b>latent space</b> using <b>teacher-synthetic data</b> and optimizing adversarial loss with <b>teacher as discriminator</b>.</font></p>
        
        <p><figure>
<img data-src='resource\figs\SD3-Turbo\SD3-Turbo-fig1.png' width=500>
<figcaption>
<b>Figure 1.</b> <b>ADD:</b> (1) An adversarial loss for deceiving a discriminator (DINO v2); (2) A distillation loss for matching denoised output to that of a teacher. <b>The proposed LADD:</b> (1) Use <i>teacher-generated images</i> as the student input; (2) Use <i>the teacher</i> as the discrinimator. <b>Advantages:</b> (1) It is <i>efficient</i> to distill model in latent space; (2) Diffusion model as the discriminator provides <i>noise-level feedback</i>, handles <i>multi-aspect ratio data</i>.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\SD3-Turbo\SD3-Turbo-fig2.png' width=700>
<figcaption>
<b>Figure 2.</b> (1) Training on <b>synthetic data</b> works better than real data. (2) Training on synthetic data only needs the <b>adversarial loss</b>. CS: CLIPScore.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\SD3-Turbo\SD3-Turbo-fig3.png' width=700>
<figcaption>
<b>Figure 3.</b> Training using <b>LADD performs better than LCM</b>.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\SD3-Turbo\SD3-Turbo-fig4.png' width=700>
<figcaption>
<b>Figure 4.</b> <b>Student model size</b> significant impacts performance, while the benefits of teacher models and data quality plateau.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\SD3-Turbo\SD3-Turbo-fig5.png' width=500>
<figcaption>
<b>Figure 5.</b> Use LoRA for DPO-traning, and apply <b>DPO-LoRA</b> after LADD training.
</figcaption>
</figure></p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    <h2 id="Datasets & Evaluation of Generation-table"><a class="no_dec" href="#Datasets & Evaluation of Generation">Datasets & Evaluation of Generation</a></h2>
    <p class="little_split" id='HPSv3datasets & evaluation of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('HPSv3-Datasets & Evaluation of Generation-details')"><i>HPSv3: Towards Wide-Spectrum Human Preference Score</i></p>
    <p class="paper_detail">Yuhang Ma, Yunhao Shui, Xiaoshi Wu, Keqiang Sun, Hongsheng Li</p>
    
    <p class="paper_detail">International Conference on Computer Vision (<b><font color=#404040>ICCV</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Aug 05, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>HPSv3</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2508.03789">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/MizzenAI/HPSv3">code</a></p>
    
    
    <div id='HPSv3-Datasets & Evaluation of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It introduces a human-preference dataset with <b>1.08M text-image pairs</b> and <b>1.17M pairwise comparisons</b>, which are used to fine-tune <b>Qwen2VL-7B</b>.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='UnifiedRewarddatasets & evaluation of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('UnifiedReward-Datasets & Evaluation of Generation-details')"><i>Unified Reward Model for Multimodal Understanding and Generation</i></p>
    <p class="paper_detail">Yibin Wang, Yuhang Zang, Hao Li, Cheng Jin, Jiaqi Wang</p>
    <p class="paper_detail">Fudan University, Shanghai Innovation Institute, Shanghai AI Lab, Shanghai Academy of Artificial Intelligence for Science</p>
    <p class="paper_detail"> <b><font color=#404040>arXiv</font></b>, <b>2025</b></font></p>
    <p class="paper_detail"><b>Mar 07, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>UnifiedReward</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2503.05236">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/CodeGoat24/UnifiedReward/">code</a></p>
    
    
    <div id='UnifiedReward-Datasets & Evaluation of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>It fine-tunes LLaVA-OneVision 7B for both <b>multimodal understanding & generation evaluation</b> by pairwise ranking & pointwise scoring.</font></p>
        
        <p></p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='VisionRewarddatasets & evaluation of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('VisionReward-Datasets & Evaluation of Generation-details')"><i>VisionReward: Fine-Grained Multi-Dimensional Human Preference Learning for Image and Video Generation</i></p>
    <p class="paper_detail">Jiazheng Xu, Yu Huang, Jiale Cheng, Yuanming Yang, Jiajun Xu, Yuan Wang, Wenbo Duan, Shen Yang, Qunlin Jin, Shurun Li, Jiayan Teng, Zhuoyi Yang, Wendi Zheng, Xiao Liu, Ming Ding, Xiaohan Zhang, Xiaotao Gu, Shiyu Huang, Minlie Huang, Jie Tang, Yuxiao Dong</p>
    
    <p class="paper_detail"> <b><font color=#404040>arXiv</font></b>, <b>2024</b></font></p>
    <p class="paper_detail"><b>Dec 30, 2024</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>VisionReward</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2412.21059">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/zai-org/VisionReward">code</a></p>
    
    
    <div id='VisionReward-Datasets & Evaluation of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It disentangles human preference into <b>64 binary questions</b> and learns an <b>interpretable linear reward</b> via multi-dimensional optimization.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='T2V-CompBenchdatasets & evaluation of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('T2V-CompBench-Datasets & Evaluation of Generation-details')"><i>T2V-CompBench: A Comprehensive Benchmark for Compositional Text-to-video Generation</i></p>
    <p class="paper_detail">Kaiyue Sun, Kaiyi Huang, Xian Liu, Yue Wu, Zihan Xu, Zhenguo Li, Xihui Liu</p>
    <p class="paper_detail">The University of Hong Kong, The Chinese University of Hong Kong, Huawei Noah's Ark Lab</p>
    <p class="paper_detail"> <b><font color=#404040>T2V-CompBench</font></b></b></font></p>
    <p class="paper_detail"><b>Jul 19, 2024</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>T2V-CompBench</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2407.14505">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/KaiyueSun98/T2V-CompBench/">code</a></p>
    
    
    <div id='T2V-CompBench-Datasets & Evaluation of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>Evaluate compositional video generation capability: consistent attribute, dynamic attribute, spatial relationships, motion, action, object interations, numeracy.</font></p>
        
        <p><ul>
<li> Find nouns and verbs by identifying them using WordNet from <b>Pika Discord channels</b>, used to generate prompts by GPT-4.
<li> <b>Consistent attribute binding:</b> two objects, two attributes, and at least one active verb from color, shape, texture, and human-related attributes.
<li> <b>Dynamic attribute binding:</b> color and light change, shape and size change, texture change, combined change.
<li> <b>Spatial relationships:</b> two objects with spatial relationships like "on the left of".
<li> <b>Motion binding:</b> one or two objects with specified moving direction like "leftwards".
<li> <b>Action binding:</b> bind actions to corresponding objects.
<li> <b>Object interactions:</b> dynamic interactions like pysical interactions.
<li> <b>Generative numeracy:</b> a specific number of objects.
<li> <b>Video LLM-based metrics (Grid-LLaVa)</b> is used for evaluating consistent attribute binding, action binding, object interactions.
<li> <b>Image LLM-based metrics (LLaVa)</b> is used for evaluating dynamic attribute binding.
<li> <b>Grounding DINO</b> is used for evaluating spatial relationships and numeracy.
<li> <b>Grounding SAM + DOT</b> is used for evaluating motion binding.
</ul>
<figure>
<img data-src='resource\figs\T2V-CompBench\T2V-CompBench-fig1.png' width=800>
<figcaption>
<b>Figure 1.</b> <b>Categories (left), evaluation methods (middle), and benchmarking model performance (right).</b>
</figcaption>
</figure></p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='VQAScoredatasets & evaluation of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('VQAScore-Datasets & Evaluation of Generation-details')"><i>Evaluating Text-to-Visual Generation with Image-to-Text Generation</i></p>
    <p class="paper_detail">Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, Deva Ramanan</p>
    <p class="paper_detail">Crnegie Mellon University, Meta</p>
    <p class="paper_detail">European Conference on Computer Vision (<b><font color=#404040>ECCV</font></b>), <b>2024</b></font></p>
    <p class="paper_detail"><b>Apr 01, 2024</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>VQAScore</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2404.01291">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/linzhiqiu/t2v_metrics/">code</a></p>
    
    
    <div id='VQAScore-Datasets & Evaluation of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>VQAScore: alignment probability of "yes" answer from a VQA model (CLIP-FlanT5); GenAI-Bench: 1600 prompts for image generation evaluation.</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='Vbenchdatasets & evaluation of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('Vbench-Datasets & Evaluation of Generation-details')"><i>VBench: Comprehensive Benchmark Suite for Video Generative Models</i></p>
    <p class="paper_detail">Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, Ziwei Liu</p>
    <p class="paper_detail">Nanyang Technological University, Shanghai Artificial Intelligence Laboratory, The Chinese University of Hong Kong, Nanjing University</p>
    <p class="paper_detail">Conference on Computer Vision and Pattern Recognition (<b><font color=#404040>CVPR</font></b>), <b>2024</b></font></p>
    <p class="paper_detail"><b>Nov 29, 2023</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#C55253>Vbench</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2311.17982">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/Vchitect/VBench/">code</a></p>
    
    
    <div id='Vbench-Datasets & Evaluation of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>It evaluates video generation from 16 dimensions within the perspectives of video quality and video-prompt consistency.</font></p>
        
        <p>
<ul>
    <li> <b>Content Categories:</b> animal, architecture, food, human, lifestyle, plant, scenary, vehicles.
    <li> <b>Temporal quality-subject consistency:</b> DINO feature similarity across frames.
    <li> <b>Temporal quality-background consistency:</b> CLIP feature similarity across frames.
    <li> <b>Temporal quality-temporal flickering:</b> mean absolute difference across frames.
    <li> <b>Temporal quality-motion smoothness:</b> use video frame interpolation model to evaluate motion smoothness.
    <li> <b>Temporal quality-dynamic degree:</b> use RAFT to estimate degree of dynamics.
    <li> <b>Frame-wise quality-aesthetic quality:</b> use LAION aesthetic predictor.
    <li> <b>Frame-wise quality-imaging quality:</b> use MUSIQ image quality predictor.
    <li> <b>Semantics-object class:</b> use GRiT to detect classes.
    <li> <b>Semantics-multiple objects:</b> detect success rate of generating all objects.
    <li> <b>Semantics-human action:</b> use UMT to detect specific actions.
    <li> <b>Semantics-color:</b> use GRiT for color captioning.
    <li> <b>Semantics-spatial relationship:</b> use rule-based evaluation.
    <li> <b>Semantics-scene:</b> use Tag2Text for scene captioning.
    <li> <b>Style-appearance style:</b> use CLIP feature similarity.
    <li> <b>Style-temporal style:</b> use ViCLIP to calculate video feature and temporal style description feature similarity.
    <li> <b>Overall consistency:</b> use ViCLIP to evaluate overall semantics and style consistency.
</ul>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='GenEvaldatasets & evaluation of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('GenEval-Datasets & Evaluation of Generation-details')"><i>GenEval: An Object-Focused Framework for Evaluating Text-to-Image Alignment</i></p>
    <p class="paper_detail">Dhruba Ghosh, Hanna Hajishirzi, Ludwig Schmidt</p>
    <p class="paper_detail">University of Washington, Allen Institute for AI, LAION</p>
    <p class="paper_detail">Advances in Neural Information Processing Systems (<b><font color=#404040>NeurIPS</font></b>), <b>2023</b></font></p>
    <p class="paper_detail"><b>Oct 17, 2023</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>GenEval</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2310.11513">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/djghosh13/geneval/">code</a></p>
    
    
    <div id='GenEval-Datasets & Evaluation of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>An <b>object-focused</b> framework for image generation evaluation.</font></p>
        
        <p><figure>
<img data-src='resource\figs\GenEval\GenEval-fig1.png' width=600>
<figcaption>
<b>Figure 1.</b> <b>GenEval pipeline.</b> Detect objects using Mask2Former detector and evaluate attributes of them.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\GenEval\GenEval-fig2.png' width=550>
<figcaption>
<b>Figure 2.</b> <b>Evaluation perspectives.</b>
</figcaption>
</figure></p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='T2I-CompBenchdatasets & evaluation of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('T2I-CompBench-Datasets & Evaluation of Generation-details')"><i>T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation</i></p>
    <p class="paper_detail">Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, Xihui Liu</p>
    <p class="paper_detail">The University of Hong Kong, Huawei Noah's Ark Lab</p>
    <p class="paper_detail">Advances in Neural Information Processing Systems (<b><font color=#404040>NeurIPS</font></b>), <b>2023</b></font></p>
    <p class="paper_detail"><b>Jul 12, 2023</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>T2I-CompBench</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2307.06350">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/Karine-Huang/T2I-CompBench/">code</a></p>
    
    
    <div id='T2I-CompBench-Datasets & Evaluation of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>It uses 6000 prompts to evaluate model capability on compositional generation, including attribute binding, object relationship, complex compositions.</font></p>
        
        <p><ul>
<li> <b>Attribute binding prompts:</b> at least two objects with two attributes from color, shape, texture.
<li> <b>Object relationship prompts:</b> at least two objects with spatial relationship or non-spatial relationship.
<li> <b>Complex compositions prompts:</b> more than two objects or more than two sub-categories.
</ul>
<figure>
<img data-src='resource\figs\T2I-CompBench\T2I-CompBench-fig1.png' width=700>
<figcaption>
<b>Figure 1.</b> <b>Evaluation methods.</b> Use disentangled BLIP-VQA to evaluate attribute binding, UniDet-based metric to evaluate spatial relationship, CLIPScore to evaluate non-spatial relationship, and 3-in-1 metric (average score of the three metrics) to evaluate complex compositions.
</figcaption>
</figure></p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='HPS v2datasets & evaluation of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('HPS v2-Datasets & Evaluation of Generation-details')"><i>Human Preference Score v2: A Solid Benchmark for Evaluating Human Preferences of Text-to-Image Synthesis</i></p>
    <p class="paper_detail">Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, Hongsheng Li</p>
    <p class="paper_detail">CUHK, SenseTime Research, Shanghai Jiao Tong University, Centre for Perceptual and Interactive Intelligence</p>
    <p class="paper_detail"> <b><font color=#404040>arXiv</font></b>, <b>2023</b></font></p>
    <p class="paper_detail"><b>Jun 15, 2023</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#C55253>HPS v2</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2306.09341">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/tgxs002/HPSv2/">code</a></p>
    
    
    <div id='HPS v2-Datasets & Evaluation of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>It proposes HPD v2: 798K human preferences on 433K pairs of images; HPS v2: fine-tuned CLIP on HPD v2 for image generation evaluation.</font></p>
        
        <p><figure>
<img data-src='resource\figs\HPS v2\HPS v2-fig1.png' width=750>
<figcaption>
<b>Figure 1.</b> <b>Training pipeline.</b> (1) Clean prompts from COCO captions and DiffusionDB by ChatGPT; (2) Generate images using 9 image generation models; (3) Rank and annotate each pair of images by humans; (4) Finetune CLIP and obtain a preference model to give HPS v2 evaluation score.
</figcaption>
</figure></p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='PickScoredatasets & evaluation of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('PickScore-Datasets & Evaluation of Generation-details')"><i>Pick-a-Pic: An Open Dataset of User Preferences for Text-to-Image Generation</i></p>
    <p class="paper_detail">Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, Omer Levy</p>
    <p class="paper_detail">Tel Aviv University, Stability AI</p>
    <p class="paper_detail">Advances in Neural Information Processing Systems (<b><font color=#404040>NeurIPS</font></b>), <b>2023</b></font></p>
    <p class="paper_detail"><b>May 02, 2023</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#C55253>PickScore</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2305.01569">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/yuvalkirstain/PickScore/">code</a></p>
    
    
    <div id='PickScore-Datasets & Evaluation of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>Pick-a-Pic: use a web app to collect user preferences; PickScore: train a CLIP-based model on preference data for image generation evaluation.</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='ImageRewarddatasets & evaluation of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('ImageReward-Datasets & Evaluation of Generation-details')"><i>ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation</i></p>
    <p class="paper_detail">Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, Yuxiao Dong</p>
    <p class="paper_detail">Tsinghua University, Zhipu AI, Beijing University of Posts and Telecommunications</p>
    <p class="paper_detail">Advances in Neural Information Processing Systems (<b><font color=#404040>NeurIPS</font></b>), <b>2023</b></font></p>
    <p class="paper_detail"><b>Apr 12, 2023</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#C55253>ImageReward</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2304.05977">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/THUDM/ImageReward/">code</a></p>
    
    
    <div id='ImageReward-Datasets & Evaluation of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>It trains BLIP on 137K human preference image pairs for preference evaluation and use it to tune models by Reward Feedback Learning (ReFL).</font></p>
        
        <p><figure>
<img data-src='resource\figs\ImageReward\ImageReward-fig1.png' width=600>
<figcaption>
<b>Figure 1.</b> <b>Training pipeline.</b> (1) Use DiffusionDB prompts to generate images; (2) Rank; (3) Train model on ranking data; (4) Tune model via ReFL.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\ImageReward\ImageReward-fig2.png' width=300>
<figcaption>
<b>Figure 2.</b> <b>ReFL algorithm.</b>
</figcaption>
</figure></p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='HPSdatasets & evaluation of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('HPS-Datasets & Evaluation of Generation-details')"><i>Human Preference Score: Better Aligning Text-to-Image Models with Human Preference</i></p>
    <p class="paper_detail">Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, Hongsheng Li</p>
    <p class="paper_detail">CUHK, SenseTime Research, Shanghai Jiao Tong University, Centre for Perceptual and Interactive Intelligence, Shanghai AI Lab</p>
    <p class="paper_detail">International Conference on Computer Vision (<b><font color=#404040>ICCV</font></b>), <b>2023</b></font></p>
    <p class="paper_detail"><b>Mar 25, 2023</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>HPS</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2303.14420">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/tgxs002/align_sd/">code</a></p>
    
    
    <div id='HPS-Datasets & Evaluation of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>It fine-tunes CLIP on annotated 98K SD generated images from 25K prompts for image generation evaluation.</font></p>
        
        <p><figure>
<img data-src='resource\figs\HPS\HPS-fig1.png' width=650>
<figcaption>
<b>Figure 1.</b> <b>Train score model:</b> the same as CLIP except for the sample with the highest preference is taken as the positive; <b>Finetune image generation model using the score model:</b> append a special token to the prompts of worse images for training; remove that token during inference.
</figcaption>
</figure></p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='CLIPScoredatasets & evaluation of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('CLIPScore-Datasets & Evaluation of Generation-details')"><i>CLIPScore: A Reference-free Evaluation Metric for Image Captioning</i></p>
    <p class="paper_detail">Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, Yejin Choi</p>
    <p class="paper_detail">Allen Institute for AI, University of Washington</p>
    <p class="paper_detail"> <b><font color=#404040>EMNLP</font></b>, <b>2021</b></font></p>
    <p class="paper_detail"><b>Apr 18, 2021</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#C55253>CLIPScore</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2104.08718">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/jmhessel/clipscore">code</a></p>
    
    
    <div id='CLIPScore-Datasets & Evaluation of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>It proposes a reference-free metric mainly focusing on semantic alignment for image generation evaluation.</font></p>
        
        <p>
<ul>
    <li> It calculates the <b>cosine similarity between a caption and an image</b>, multiplying the result by 2.5 (some use 1.).
    <li> It is <b>sensitive</b> to adversarially constructed image captions.
    <li> It <b>generalizes well</b> on never-before-seen images.
    <li> It frees from the shortcomings of <b>n-gram matching</b> that disfavors good captions with new words and favors captions with familiar words.
</ul>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='FVDdatasets & evaluation of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('FVD-Datasets & Evaluation of Generation-details')"><i>FVD: A new Metric for Video Generation</i></p>
    <p class="paper_detail">Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphaël Marinier, Marcin Michalski, Sylvain Gelly</p>
    <p class="paper_detail">Johannes Kepler University, IDSIA, Google Brain</p>
    <p class="paper_detail">International Conference on Learning Representations (<b><font color=#404040>ICLR workshop</font></b>), <b>2019</b></font></p>
    <p class="paper_detail"><b>May 04, 2019</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>FVD</font></b></p>
    <p class="paper_detail"><a href="https://openreview.net/pdf?id=rylgEULtdN">paper</a></p>
    
    
    <div id='FVD-Datasets & Evaluation of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>Extend FID for video generation evaluation by replacing 2D InceptionNet with pre-trained Inflated 3D convnet.</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='FIDdatasets & evaluation of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('FID-Datasets & Evaluation of Generation-details')"><i>GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium</i></p>
    <p class="paper_detail">Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Sepp Hochreiter</p>
    <p class="paper_detail">Johannes Kepler University Linz</p>
    <p class="paper_detail">Advances in Neural Information Processing Systems (<b><font color=#404040>NeurIPS</font></b>), <b>2017</b></font></p>
    <p class="paper_detail"><b>Jun 26, 2017</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#C55253>FID</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/1706.08500">paper</a></p>
    
    
    <div id='FID-Datasets & Evaluation of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>Calculate <b>Fréchet distance</b> between Gaussian distributions of InceptionNet features of real-world and synthetic data for image generation evaluation.</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='Inception Scoredatasets & evaluation of generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('Inception Score-Datasets & Evaluation of Generation-details')"><i>Improved Techniques for Training GANs</i></p>
    <p class="paper_detail">Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen</p>
    <p class="paper_detail">OpenAI</p>
    <p class="paper_detail">Advances in Neural Information Processing Systems (<b><font color=#404040>NeurIPS</font></b>), <b>2016</b></font></p>
    <p class="paper_detail"><b>Jun 10, 2016</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#C55253>Inception Score</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/1606.03498">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/openai/improved-gan/">code</a></p>
    
    
    <div id='Inception Score-Datasets & Evaluation of Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>Calculate <b>KL divergence between p(y|x) and p(y)</b> that aims to minimize the entropy across predictions and maximize the entropy across predictions of classes for image generation evaluation.</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    <h2 id="Controllable Generation-table"><a class="no_dec" href="#Controllable Generation">Controllable Generation</a></h2>
    <p class="little_split" id='Follow-Your-Emojicontrollable generation'></p>
    <div style="border-left: 16px solid #A0D0A0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('Follow-Your-Emoji-Controllable Generation-details')"><i>Follow-Your-Emoji: Fine-Controllable and Expressive Freestyle Portrait Animation</i></p>
    <p class="paper_detail">Yue Ma, Hongyu Liu, Hongfa Wang, Heng Pan, Yingqing He, Junkun Yuan, Ailing Zeng, Chengfei Cai, Heung-Yeung Shum, Wei Liu, Qifeng Chen</p>
    
    <p class="paper_detail">ACM SIGGRAPH Annual Conference in Asia (<b><font color=#404040>SIGGRAPH-Asia</font></b>), <b>2024</b></font></p>
    <p class="paper_detail"><b>Jun 04, 2024</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>Follow-Your-Emoji</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2406.01900">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/mayuelala/FollowYourEmoji">code</a></p>
    
    
    <div id='Follow-Your-Emoji-Controllable Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020></font></p>
        
        <p>
<ul>
    <li>
</ul>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='ControlNetcontrollable generation'></p>
    <div style="border-left: 16px solid #A0D0A0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('ControlNet-Controllable Generation-details')"><i>Adding Conditional Control to Text-to-Image Diffusion Models</i></p>
    <p class="paper_detail">Lvmin Zhang, Anyi Rao, Maneesh Agrawala</p>
    <p class="paper_detail">Stanford University</p>
    <p class="paper_detail">International Conference on Computer Vision (<b><font color=#404040>ICCV</font></b>), <b>2023</b></font></p>
    <p class="paper_detail"><b>Feb 10, 2023</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#C55253>ControlNet</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2302.05543">paper</a></p>
    <p class="paper_detail"><font color=#C55253>It introduces a scalable method to condition diffusion models with additional spatial or semantic inputs, enabling precise and flexible control over image generation without retraining the base model. It has over 5,000 citations (as of Sep 2025).</font></p>
    
    <div id='ControlNet-Controllable Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It introduces a zero-convolution bypass architecture that adds spatial conditioning to frozen diffusion models for controllable generation.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    <h2 id="Editing & Inpainting & Outpainting Generation-table"><a class="no_dec" href="#Editing & Inpainting & Outpainting Generation">Editing & Inpainting & Outpainting Generation</a></h2>
    <p class="little_split" id='Trans-Adapterediting & inpainting & outpainting generation'></p>
    <div style="border-left: 16px solid #F0BBCC; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('Trans-Adapter-Editing & Inpainting & Outpainting Generation-details')"><i>Trans-Adapter: A Plug-and-Play Framework for Transparent Image Inpainting</i></p>
    <p class="paper_detail">Yuekun Dai, Haitian Li, Shangchen Zhou, Chen Change Loy</p>
    <p class="paper_detail">Nanyang Technological University</p>
    <p class="paper_detail">International Conference on Computer Vision (<b><font color=#404040>ICCV</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Aug 01, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>Trans-Adapter</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2508.01098">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/ykdai/Trans-Adapter">code</a></p>
    
    
    <div id='Trans-Adapter-Editing & Inpainting & Outpainting Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
 It proposes a <b>plug-and-play adapter</b> that inflates any diffusion inpainting model to generate aligned RGB and alpha channels for <b>transparent images</b>.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='MTADiffusionediting & inpainting & outpainting generation'></p>
    <div style="border-left: 16px solid #F0BBCC; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('MTADiffusion-Editing & Inpainting & Outpainting Generation-details')"><i>MTADiffusion: Mask Text Alignment Diffusion Model for Object Inpainting</i></p>
    <p class="paper_detail">Jun Huang, Ting Liu, Yihang Wu, Xiaochao Qu, Luoqi Liu, Xiaolin Hu</p>
    <p class="paper_detail">Meitu, National University of Singapore, Tsinghua University</p>
    <p class="paper_detail">Conference on Computer Vision and Pattern Recognition (<b><font color=#404040>CVPR</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Jun 30, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>MTADiffusion</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2506.23482">paper</a></p>
    
    
    <div id='MTADiffusion-Editing & Inpainting & Outpainting Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It leverages <b>25 million</b> fine-grained mask-text pairs and <b>multi-task edge-guided</b> training with Gram-style loss to learn image inpainting.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='HomoGenediting & inpainting & outpainting generation'></p>
    <div style="border-left: 16px solid #F0BBCC; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('HomoGen-Editing & Inpainting & Outpainting Generation-details')"><i>HomoGen: Enhanced Video Inpainting via Homography Propagation and Diffusion</i></p>
    <p class="paper_detail">Ding Ding, Yueming Pan, Ruoyu Feng, Qi Dai, Kai Qiu, Jianmin Bao, Chong Luo, Zhenzhong Chen</p>
    <p class="paper_detail">Wuhan University, Xi'an Jiaotong University, University of Science and Technology of China, Microsoft Research Asia</p>
    <p class="paper_detail">Conference on Computer Vision and Pattern Recognition (<b><font color=#404040>CVPR</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Jun 11, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>HomoGen</font></b></p>
    <p class="paper_detail"><a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Ding_HomoGen_Enhanced_Video_Inpainting_via_Homography_Propagation_and_Diffusion_CVPR_2025_paper.pdf">paper</a></p>
    
    
    <div id='HomoGen-Editing & Inpainting & Outpainting Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It employs homography-based pixel propagation to supply semantically coherent priors and a content-adaptive model for video inpainting.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='VideoRepainterediting & inpainting & outpainting generation'></p>
    <div style="border-left: 16px solid #F0BBCC; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('VideoRepainter-Editing & Inpainting & Outpainting Generation-details')"><i>Keyframe-Guided Creative Video Inpainting</i></p>
    <p class="paper_detail">Yuwei Guo, Ceyuan Yang, Anyi Rao, Chenlin Meng, Omer Bar-Tal, Shuangrui Ding, Maneesh Agrawala, Dahua Lin, Bo Dai</p>
    <p class="paper_detail">CUHK, Shanghai AI Laboratory, Pika Labs, ByteDance, CPII under InnoHK, Stanford, HKUST, HKU, Feeling AI</p>
    <p class="paper_detail">Conference on Computer Vision and Pattern Recognition (<b><font color=#404040>CVPR</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Jun 11, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>VideoRepainter</font></b></p>
    <p class="paper_detail"><a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Guo_Keyframe-Guided_Creative_Video_Inpainting_CVPR_2025_paper.pdf">paper</a></p>
    
    
    <div id='VideoRepainter-Editing & Inpainting & Outpainting Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It introduces a <b>keyframe-guided</b> two-stage pipeline that repurposes an <b>image-to-video model</b> with mask conditioning for creative video inpainting.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='Step1X-Editediting & inpainting & outpainting generation'></p>
    <div style="border-left: 16px solid #F0BBCC; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('Step1X-Edit-Editing & Inpainting & Outpainting Generation-details')"><i>Step1X-Edit: A Practical Framework for General Image Editing</i></p>
    <p class="paper_detail">Step1X-Image Team</p>
    <p class="paper_detail">StepFun</p>
    <p class="paper_detail"> <b><font color=#404040>arXiv</font></b>, <b>2025</b></font></p>
    <p class="paper_detail"><b>Apr 24, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>Step1X-Edit</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2504.17761">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/stepfun-ai/Step1X-Edit/">code</a></p>
    
    
    <div id='Step1X-Edit-Editing & Inpainting & Outpainting Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>It uses a <b>MLLM to generate condition embedding</b> of the reference image and instructions for image generation editing.</font></p>
        
        <p><ul>
<li> <b>Training date:</b> 1M images & 20M instruction-image data.
<li><b>Data construction.</b> (1) Subject addition and removal; (2) Subject replacement and background change; (3) Color Alteration and material modification; (4) Text modification; (5) Motion change; (6) Portrait editing; (7) Style transfer; (8) Tone transformation.
<li><b>Caption strategy.</b> Redundancy-enhanced annotation: multi-round annotation strategy. Stylized annotation via contextual examples: use style-aligned examples as contextual references. Use GPT-4o to annotate data for training in-house annotators. Bilingual: Chinese and English.
</ul>
<figure>
<img data-src='resource\figs\Step1X-Edit\Step1X-Edit-fig1.png' width=600>
<figcaption>
<b>Figure 1.</b> <b>Structure.</b> <b>Multimodal large language model (Qwen-VL)</b> is used to generate embeddings of instruction and reference images.
</figcaption>
</figure></p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='ATAediting & inpainting & outpainting generation'></p>
    <div style="border-left: 16px solid #F0BBCC; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('ATA-Editing & Inpainting & Outpainting Generation-details')"><i>ATA: Adaptive Transformation Agent for Text-Guided Subject-Position Variable Background Inpainting</i></p>
    <p class="paper_detail">Yizhe Tang, Zhimin Sun, Yuzhen Du, Ran Yi, Guangben Lu, Teng Hu, Luying Li, Lizhuang Ma, Fangyuan Zou</p>
    <p class="paper_detail">Shanghai Jiao Tong University, Tencent</p>
    <p class="paper_detail">Conference on Computer Vision and Pattern Recognition (<b><font color=#404040>CVPR</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Apr 02, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>ATA</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2504.01603">paper</a></p>
    
    
    <div id='ATA-Editing & Inpainting & Outpainting Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It predicts subject displacement via hierarchical reverse transforms to enable text-guided, position-variable background inpainting.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='TurboFillediting & inpainting & outpainting generation'></p>
    <div style="border-left: 16px solid #F0BBCC; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('TurboFill-Editing & Inpainting & Outpainting Generation-details')"><i>TurboFill: Adapting Few-step Text-to-image Model for Fast Image Inpainting</i></p>
    <p class="paper_detail">Liangbin Xie, Daniil Pakhomov, Zhonghao Wang, Zongze Wu, Ziyan Chen, Yuqian Zhou, Haitian Zheng, Zhifei Zhang, Zhe Lin, Jiantao Zhou, Chao Dong</p>
    <p class="paper_detail">University of Macau, Shenzhen University of Advanced Technology, Adobe, Chinese Academy of Sciences</p>
    <p class="paper_detail">Conference on Computer Vision and Pattern Recognition (<b><font color=#404040>CVPR</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Apr 01, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>TurboFill</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2504.00996">paper</a></p>
    
    
    <div id='TurboFill-Editing & Inpainting & Outpainting Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It trains a <b>ControlNet-style adapter</b> directly on a <b>distilled</b> text-to-image model via a novel 3-step adversarial training scheme.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='OmniPaintediting & inpainting & outpainting generation'></p>
    <div style="border-left: 16px solid #F0BBCC; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('OmniPaint-Editing & Inpainting & Outpainting Generation-details')"><i>OmniPaint: Mastering Object-Oriented Editing via Disentangled Insertion-Removal Inpainting</i></p>
    <p class="paper_detail">Yongsheng Yu, Ziyun Zeng, Haitian Zheng, Jiebo Luo</p>
    <p class="paper_detail">University of Rochester, Adobe Research</p>
    <p class="paper_detail">International Conference on Computer Vision (<b><font color=#404040>ICCV</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Mar 11, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>OmniPaint</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2503.08677">paper</a></p>
    
    
    <div id='OmniPaint-Editing & Inpainting & Outpainting Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It proposes a unified framework that reconceptualizes object removal and insertion as <b>interdependent inverse tasks</b>.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='SAGIediting & inpainting & outpainting generation'></p>
    <div style="border-left: 16px solid #F0BBCC; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('SAGI-Editing & Inpainting & Outpainting Generation-details')"><i>SAGI: Semantically Aligned and Uncertainty Guided AI Image Inpainting</i></p>
    <p class="paper_detail">Paschalis Giakoumoglou, Dimitrios Karageorgiou, Symeon Papadopoulos, Panagiotis C. Petrantonakis</p>
    <p class="paper_detail">Aristotle University of Thessaloniki, CERTH</p>
    <p class="paper_detail">International Conference on Computer Vision (<b><font color=#404040>ICCV</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Feb 10, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>SAGI</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2502.06593">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/mever-team/SAGI">code</a></p>
    
    
    <div id='SAGI-Editing & Inpainting & Outpainting Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It proposes a pipeline to combine <b>semantically-aligned prompt generation</b> and <b>uncertainty-guided realism filtering</b> for image inpainting.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='BVINetediting & inpainting & outpainting generation'></p>
    <div style="border-left: 16px solid #F0BBCC; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('BVINet-Editing & Inpainting & Outpainting Generation-details')"><i>BVINet: Unlocking Blind Video Inpainting with Zero Annotations</i></p>
    <p class="paper_detail">Zhiliang Wu, Kerui Chen, Kun Li, Hehe Fan, Yi Yang</p>
    <p class="paper_detail">Zhejiang University</p>
    <p class="paper_detail">International Conference on Computer Vision (<b><font color=#404040>ICCV</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Feb 03, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>BVINet</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2502.01181">paper</a></p>
    
    
    <div id='BVINet-Editing & Inpainting & Outpainting Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It proposes the first end-to-end blind video inpainting framework that jointly learns mask prediction and inpainting <b>without any manual annotations</b>.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='RADediting & inpainting & outpainting generation'></p>
    <div style="border-left: 16px solid #F0BBCC; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('RAD-Editing & Inpainting & Outpainting Generation-details')"><i>RAD: Region-Aware Diffusion Models for Image Inpainting</i></p>
    <p class="paper_detail">Sora Kim, Sungho Suh, Minsik Lee</p>
    <p class="paper_detail">Hanyang University, Korea University, DFKI, Hanyang University ERICA</p>
    <p class="paper_detail">Conference on Computer Vision and Pattern Recognition (<b><font color=#404040>CVPR</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Dec 12, 2024</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>RAD</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2412.09191">paper</a></p>
    
    
    <div id='RAD-Editing & Inpainting & Outpainting Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It proposes a <b>pixel-wise spatially-varying</b> diffusion schedule that asynchronously denoises masked regions.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='Pincoediting & inpainting & outpainting generation'></p>
    <div style="border-left: 16px solid #F0BBCC; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('Pinco-Editing & Inpainting & Outpainting Generation-details')"><i>Pinco: Position-induced Consistent Adapter for Diffusion Transformer in Foreground-conditioned Inpainting</i></p>
    <p class="paper_detail">Guangben Lu, Yuzhen Du, Zhimin Sun, Ran Yi, Yifan Qi, Yizhe Tang, Tianyi Wang, Lizhuang Ma, Fangyuan Zou</p>
    <p class="paper_detail">Shanghai Jiao Tong University, Tencent</p>
    <p class="paper_detail">International Conference on Computer Vision (<b><font color=#404040>ICCV</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Dec 05, 2024</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>Pinco</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2412.03812">paper</a></p>
    
    
    <div id='Pinco-Editing & Inpainting & Outpainting Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
 It proposes a <b>foreground-conditioned inpainting adapter</b> that injects subject-aware attention into the self-attention layer.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='OmniEditediting & inpainting & outpainting generation'></p>
    <div style="border-left: 16px solid #F0BBCC; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('OmniEdit-Editing & Inpainting & Outpainting Generation-details')"><i>OmniEdit: Building Image Editing Generalist Models Through Specialist Supervision</i></p>
    <p class="paper_detail">Cong Wei, Zheyang Xiong, Weiming Ren, Xinrun Du, Ge Zhang, Wenhu Chen</p>
    <p class="paper_detail">University of Waterloo, University of Wisconsin-Madison, Vector Institute, M-A-P</p>
    <p class="paper_detail">International Conference on Learning Representations (<b><font color=#404040>ICLR</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Nov 11, 2024</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#C55253>OmniEdit</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2411.07199">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/TIGER-AI-Lab/OmniEdit">code</a></p>
    
    
    <div id='OmniEdit-Editing & Inpainting & Outpainting Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It introduces a diffusion editor that employs <b>seven task-specific experts</b>, GPT-4o-driven importance sampling and an EditNet transformer.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='TD-Paintediting & inpainting & outpainting generation'></p>
    <div style="border-left: 16px solid #F0BBCC; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('TD-Paint-Editing & Inpainting & Outpainting Generation-details')"><i>TD-Paint: Faster Diffusion Inpainting Through Time Aware Pixel Conditioning</i></p>
    <p class="paper_detail">Tsiry Mayet, Pourya Shamsolmoali, Simon Bernard, Eric Granger, Romain Hérault, Clement Chatelain</p>
    <p class="paper_detail">INSA Rouen Normandie, University of York, Universit'e Rouen Normandie, LIVIA, Universit'e Caen Normandie,</p>
    <p class="paper_detail">International Conference on Learning Representations (<b><font color=#404040>ICLR</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Oct 11, 2024</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>TD-Paint</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2410.09306">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/MaugrimEP/td-paint">code</a></p>
    
    
    <div id='TD-Paint-Editing & Inpainting & Outpainting Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It introduces a <b>pixel-wise time-conditioning</b> scheme that allows cutting sampling steps by an order of magnitude without architectural changes.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='CAT-Diffusionediting & inpainting & outpainting generation'></p>
    <div style="border-left: 16px solid #F0BBCC; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('CAT-Diffusion-Editing & Inpainting & Outpainting Generation-details')"><i>Improving Text-guided Object Inpainting with Semantic Pre-inpainting</i></p>
    <p class="paper_detail">Yifu Chen, Jingwen Chen, Yingwei Pan, Yehao Li, Ting Yao, Zhineng Chen, Tao Mei</p>
    <p class="paper_detail">Fudan University, Shanghai Collaborative Innovation Center of Intelligent Visual Computing, HiDream.ai Inc.</p>
    <p class="paper_detail">European Conference on Computer Vision (<b><font color=#404040>ECCV</font></b>), <b>2024</b></font></p>
    <p class="paper_detail"><b>Sep 12, 2024</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>CAT-Diffusion</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2409.08260">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/Nnn-s/CATdiffusion">code</a></p>
    
    
    <div id='CAT-Diffusion-Editing & Inpainting & Outpainting Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It presents a cascaded Transformer-diffusion that <b>semantically pre-inpaints object features in CLIP space</b> and injects them via a reference adapter.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='Follow-Your-Canvasediting & inpainting & outpainting generation'></p>
    <div style="border-left: 16px solid #F0BBCC; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('Follow-Your-Canvas-Editing & Inpainting & Outpainting Generation-details')"><i>Follow-Your-Canvas: Higher-Resolution Video Outpainting with Extensive Content Generation</i></p>
    <p class="paper_detail">Qihua Chen, Yue Ma, Hongfa Wang, Junkun Yuan, Wenzhe Zhao, Qi Tian, Hongmei Wang, Shaobo Min, Qifeng Chen, Wei Liu</p>
    <p class="paper_detail">Tencent, HKUST, USTC, Tsinghua University</p>
    <p class="paper_detail">AAAI Conference on Artificial Intelligence (<b><font color=#404040>AAAI</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Sep 02, 2024</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>Follow-Your-Canvas</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2409.01055">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/mayuelala/FollowYourCanvas">code</a></p>
    
    
    <div id='Follow-Your-Canvas-Editing & Inpainting & Outpainting Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>It enables <b>higher-resolution</b> video outpainting with <b>extensive content</b> generation through sliding window and source video layout injection.</font></p>
        
        <p><figure>
<img data-src='resource\figs\Follow-Your-Canvas\Follow-Your-Canvas-fig1.png' width=600>
<figcaption>
<b>Figure 1.</b> <b>Training framework.</b> An anchor window and a target window are randomly sampled, mimicking the "source video" and "region to perform outpaint" for inference. The anchor window is injected into the model through a layout encoder, as well as a relative region embedding calculated by the positional relation between the anchor window and the target window, aligning generated layout of the target window with the anchor window.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Follow-Your-Canvas\Follow-Your-Canvas-fig2.png' width=600>
<figcaption>
<b>Figure 2.</b> <b>Inference framework.</b> The given source video is covered by N spatial windows. During each denoising step t, outpainting is performed within each window in parallel on separate GPUs to accelerate inference. The windows are then merged through Gaussian weights to get outcome at step t-1. These windows may cover layer upon layer, allowing outpainting any videos to a higher resolution without being limited by GPU memory.
</figcaption>
</figure></p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='Brush2Promptediting & inpainting & outpainting generation'></p>
    <div style="border-left: 16px solid #F0BBCC; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('Brush2Prompt-Editing & Inpainting & Outpainting Generation-details')"><i>Brush2Prompt: Contextual Prompt Generator for Object Inpainting</i></p>
    <p class="paper_detail">Mang Tik Chiu, Yuqian Zhou, Lingzhi Zhang, Zhe Lin, Connelly Barnes, Sohrab Amirghodsi, Eli Shechtman, Humphrey Shi</p>
    <p class="paper_detail">UIUC, Adobe, University of Oregon</p>
    <p class="paper_detail">Conference on Computer Vision and Pattern Recognition (<b><font color=#404040>CVPR</font></b>), <b>2024</b></font></p>
    <p class="paper_detail"><b>Jun 17, 2024</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>Brush2Prompt</font></b></p>
    <p class="paper_detail"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Chiu_Brush2Prompt_Contextual_Prompt_Generator_for_Object_Inpainting_CVPR_2024_paper.pdf">paper</a></p>
    
    
    <div id='Brush2Prompt-Editing & Inpainting & Outpainting Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It proposes a <b>prompt generator</b> that translates masked-image CLIP embeddings into diverse object labels or captions without user text input.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='Paint by Inpaintediting & inpainting & outpainting generation'></p>
    <div style="border-left: 16px solid #F0BBCC; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('Paint by Inpaint-Editing & Inpainting & Outpainting Generation-details')"><i>Paint by Inpaint: Learning to Add Image Objects by Removing Them First</i></p>
    <p class="paper_detail">Navve Wasserman, Noam Rotstein, Roy Ganz, Ron Kimmel</p>
    <p class="paper_detail">Weizmann Institute of Science, Technion - Israel Institute of Technology</p>
    <p class="paper_detail">Conference on Computer Vision and Pattern Recognition (<b><font color=#404040>CVPR</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Apr 28, 2024</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>Paint by Inpaint</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2404.18212">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/RotsteinNoam/Paint-by-Inpaint">code</a></p>
    
    
    <div id='Paint by Inpaint-Editing & Inpainting & Outpainting Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It inverts large-scale inpainting pipelines to <b>synthesize real object-addition pairs</b>, setting new SOTA for text-guided object insertion & general editing.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='StrDiffusionediting & inpainting & outpainting generation'></p>
    <div style="border-left: 16px solid #F0BBCC; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('StrDiffusion-Editing & Inpainting & Outpainting Generation-details')"><i>Structure Matters: Tackling the Semantic Discrepancy in Diffusion Models for Image Inpainting</i></p>
    <p class="paper_detail">Haipeng Liu, Yang Wang, Biao Qian, Meng Wang, Yong Rui</p>
    <p class="paper_detail">Hefei University of Technology, Lenovo Research</p>
    <p class="paper_detail">Conference on Computer Vision and Pattern Recognition (<b><font color=#404040>CVPR</font></b>), <b>2024</b></font></p>
    <p class="paper_detail"><b>Mar 29, 2024</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>StrDiffusion</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2403.19898">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/htyjers/StrDiffusion">code</a></p>
    
    
    <div id='StrDiffusion-Editing & Inpainting & Outpainting Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It progressively <b>injects sparser structural semantics</b> to bridge the semantic gap between masked and unmasked regions.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='Latent Codesediting & inpainting & outpainting generation'></p>
    <div style="border-left: 16px solid #F0BBCC; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('Latent Codes-Editing & Inpainting & Outpainting Generation-details')"><i>Don't Look into the Dark: Latent Codes for Pluralistic Image Inpainting</i></p>
    <p class="paper_detail">Haiwei Chen, Yajie Zhao</p>
    <p class="paper_detail">University of Southern California, USC Institute for Creative Technologies</p>
    <p class="paper_detail">Conference on Computer Vision and Pattern Recognition (<b><font color=#404040>CVPR</font></b>), <b>2024</b></font></p>
    <p class="paper_detail"><b>Mar 27, 2024</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>Latent Codes</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2403.18186">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/nintendops/latent-code-inpainting">code</a></p>
    
    
    <div id='Latent Codes-Editing & Inpainting & Outpainting Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It <b>encodes visible regions</b>, <b>infers missing tokens</b>, and <b>fuses them</b> with partial-image priors to achieve inpainting under extreme masks.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='BrushNetediting & inpainting & outpainting generation'></p>
    <div style="border-left: 16px solid #F0BBCC; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('BrushNet-Editing & Inpainting & Outpainting Generation-details')"><i>BrushNet: A Plug-and-Play Image Inpainting Model with Decomposed Dual-Branch Diffusion</i></p>
    <p class="paper_detail">Xuan Ju, Xian Liu, Xintao Wang, Yuxuan Bian, Ying Shan, Qiang Xu </p>
    <p class="paper_detail">Tencent PCG ARC Lab, The Chinese University of Hong Kong</p>
    <p class="paper_detail">European Conference on Computer Vision (<b><font color=#404040>ECCV</font></b>), <b>2024</b></font></p>
    <p class="paper_detail"><b>Mar 11, 2024</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#C55253>BrushNet</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2403.06976">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/TencentARC/BrushNet">code</a></p>
    
    
    <div id='BrushNet-Editing & Inpainting & Outpainting Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>It introduces a <b>plug-and-play</b> dual-branch model and a segmentation-based inpainting training dataset <b>BrushData</b> and a benchmark <b>BrushBench</b>.</font></p>
        
        <p><figure>
<img data-src='resource\figs\BrushNet\BrushNet-fig1.png' width=550>
<figcaption>
<b>Figure 1.</b> <b>Comparisons.</b> (a) lacks knowledge of mask boundaries. (b) struggles to obtain pure masked image features due to the text's influence.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\BrushNet\BrushNet-fig2.png' width=550>
<figcaption>
<b>Figure 2.</b> <b>Structure.</b> (1) Use VAE to process masked image to preserve original details. (2) Masked image, mask, noisy masked image are concatenated as the input. (3) Adopt hierarchical approach to incorporate UNet features. (4) Remove text cross-attention. (5) Use blurred blending.
</figcaption>
</figure></p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='ROVIediting & inpainting & outpainting generation'></p>
    <div style="border-left: 16px solid #F0BBCC; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('ROVI-Editing & Inpainting & Outpainting Generation-details')"><i>Towards Language-Driven Video Inpainting via Multimodal Large Language Models</i></p>
    <p class="paper_detail">Jianzong Wu, Xiangtai Li, Chenyang Si, Shangchen Zhou, Jingkang Yang, Jiangning Zhang, Yining Li, Kai Chen, Yunhai Tong, Ziwei Liu, Chen Change Loy</p>
    <p class="paper_detail">Peking University, Nanyang Technological University, Shanghai AI Laboratory, PKU-Wuhan Institute for Artificial Intelligence, Zhejiang University</p>
    <p class="paper_detail">Conference on Computer Vision and Pattern Recognition (<b><font color=#404040>CVPR</font></b>), <b>2024</b></font></p>
    <p class="paper_detail"><b>Jan 18, 2024</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>ROVI</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2401.10226">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/jianzongwu/Language-Driven-Video-Inpainting">code</a></p>
    
    
    <div id='ROVI-Editing & Inpainting & Outpainting Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It introduces language-driven video inpainting, a new task that <B>replaces binary masks with natural language instructions</b>.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='HD-Painterediting & inpainting & outpainting generation'></p>
    <div style="border-left: 16px solid #F0BBCC; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('HD-Painter-Editing & Inpainting & Outpainting Generation-details')"><i>HD-Painter: High-Resolution and Prompt-Faithful Text-Guided Image Inpainting with Diffusion Models</i></p>
    <p class="paper_detail">Hayk Manukyan, Andranik Sargsyan, Barsegh Atanyan, Zhangyang Wang, Shant Navasardyan, Humphrey Shi</p>
    <p class="paper_detail">Picsart AI Research (PAIR), UT Austin, Georgia Tech</p>
    <p class="paper_detail">International Conference on Learning Representations (<b><font color=#404040>ICLR</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Dec 21, 2023</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>HD-Painter</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2312.14091">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/Picsart-AI-Research/HD-Painter">code</a></p>
    
    
    <div id='HD-Painter-Editing & Inpainting & Outpainting Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It introduces plug-and-play PAIntA and RASG to make text-guided inpainting <b>prompt-faithful</b>, <b>high-resolution</b>, and <b>training-free</b>.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='ASUKAediting & inpainting & outpainting generation'></p>
    <div style="border-left: 16px solid #F0BBCC; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('ASUKA-Editing & Inpainting & Outpainting Generation-details')"><i>Towards Enhanced Image Inpainting: Mitigating Unwanted Object Insertion and Preserving Color Consistency</i></p>
    <p class="paper_detail">Yikai Wang, Chenjie Cao, Junqiu Yu, Ke Fan, Xiangyang Xue, Yanwei Fu</p>
    <p class="paper_detail">Fudan University, Nanyang Technological University, Alibaba DAMO Academy, Hupan Lab</p>
    <p class="paper_detail">Conference on Computer Vision and Pattern Recognition (<b><font color=#404040>CVPR</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Dec 08, 2023</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>ASUKA</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2312.04831">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/Yikai-Wang/asuka-misato">code</a></p>
    
    
    <div id='ASUKA-Editing & Inpainting & Outpainting Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It proposes to align MAE prior and fine-tune a local-harmonization VAE decoder to suppress <b>object hallucination & color inconsistency</b> in inpainting.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='AVIDediting & inpainting & outpainting generation'></p>
    <div style="border-left: 16px solid #F0BBCC; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('AVID-Editing & Inpainting & Outpainting Generation-details')"><i>AVID: Any-Length Video Inpainting with Diffusion Model</i></p>
    <p class="paper_detail">Zhixing Zhang, Bichen Wu, Xiaoyan Wang, Yaqiao Luo, Luxin Zhang, Yinan Zhao, Peter Vajda, Dimitris Metaxas, Licheng Yu</p>
    <p class="paper_detail">Rutgers University, Meta</p>
    <p class="paper_detail">Conference on Computer Vision and Pattern Recognition (<b><font color=#404040>CVPR</font></b>), <b>2024</b></font></p>
    <p class="paper_detail"><b>Dec 06, 2023</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>AVID</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2312.03816">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/zhang-zx/AVID">code</a></p>
    
    
    <div id='AVID-Editing & Inpainting & Outpainting Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It combines motion modules, adjustable structure guidance, and MultiDiffusion sampler with middle-frame attention to <b>inpaint videos of any length</b>.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='PowerPaintediting & inpainting & outpainting generation'></p>
    <div style="border-left: 16px solid #F0BBCC; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('PowerPaint-Editing & Inpainting & Outpainting Generation-details')"><i>A Task is Worth One Word: Learning with Task Prompts for High-Quality Versatile Image Inpainting</i></p>
    <p class="paper_detail">Junhao Zhuang, Yanhong Zeng, Wenran Liu, Chun Yuan, Kai Chen</p>
    <p class="paper_detail">Tsinghua University, Shanghai Artificial Intelligence Laboratory</p>
    <p class="paper_detail">European Conference on Computer Vision (<b><font color=#404040>ECCV</font></b>), <b>2024</b></font></p>
    <p class="paper_detail"><b>Dec 06, 2023</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#C55253>PowerPaint</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2312.03594">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/open-mmlab/PowerPaint">code</a></p>
    
    
    <div id='PowerPaint-Editing & Inpainting & Outpainting Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It presents <b>unifies multiple tasks</b> through <b>learnable task prompts</b>, achieving SOTA results in object synthesis, removal, and outpainting.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='TPMediting & inpainting & outpainting generation'></p>
    <div style="border-left: 16px solid #F0BBCC; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('TPM-Editing & Inpainting & Outpainting Generation-details')"><i>Image Inpainting via Tractable Steering of Diffusion Models</i></p>
    <p class="paper_detail">Anji Liu, Mathias Niepert, Guy Van den Broeck</p>
    <p class="paper_detail">University of California, Los Angeles, University of Stuttgart</p>
    <p class="paper_detail">International Conference on Learning Representations (<b><font color=#404040>ICLR</font></b>), <b>2024</b></font></p>
    <p class="paper_detail"><b>Nov 28, 2023</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>TPM</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2401.03349">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/UCLA-StarAI/Tiramisu">code</a></p>
    
    
    <div id='TPM-Editing & Inpainting & Outpainting Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
</font></p>
        
        <p>
It introduces the first framework that steers diffusion models via exact yet efficient <b>probalistic circuits-computed conditional distributions</b>.
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='SmartBrushediting & inpainting & outpainting generation'></p>
    <div style="border-left: 16px solid #F0BBCC; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('SmartBrush-Editing & Inpainting & Outpainting Generation-details')"><i>SmartBrush: Text and Shape Guided Object Inpainting with Diffusion Model</i></p>
    <p class="paper_detail">Shaoan Xie, Zhifei Zhang, Zhe Lin, Tobias Hinz, Kun Zhang</p>
    <p class="paper_detail">Carnegie Mellon University</p>
    <p class="paper_detail">Conference on Computer Vision and Pattern Recognition (<b><font color=#404040>CVPR</font></b>), <b>2023</b></font></p>
    <p class="paper_detail"><b>Dec 09, 2022</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#C55253>SmartBrush</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2212.05034">paper</a></p>
    
    
    <div id='SmartBrush-Editing & Inpainting & Outpainting Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It jointly leverages text prompts and <b>multi-precision object masks</b>, together with a self-predicted foreground mask, to achieve high-fidelity inpainting.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='PSMediting & inpainting & outpainting generation'></p>
    <div style="border-left: 16px solid #F0BBCC; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('PSM-Editing & Inpainting & Outpainting Generation-details')"><i>Image Inpainting via Iteratively Decoupled Probabilistic Modeling</i></p>
    <p class="paper_detail">Wenbo Li, Xin Yu, Kun Zhou, Yibing Song, Zhe Lin, Jiaya Jia</p>
    <p class="paper_detail">Huawei Noah's Ark Lab, HKU, CUHK (SZ), Alibaba DAMO Academy, Adobe Research</p>
    <p class="paper_detail">International Conference on Learning Representations (<b><font color=#404040>ICLR</font></b>), <b>2024</b></font></p>
    <p class="paper_detail"><b>Dec 06, 2022</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>PSM</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2212.02963">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/fenglinglwb/PSM">code</a></p>
    
    
    <div id='PSM-Editing & Inpainting & Outpainting Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It alternates between <b>adversarially-optimized mean prediction</b> and <b>Gaussian-modeled uncertainty</b>, achieving efficient large-hole inpainting.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    <h2 id="Stylization Generation-table"><a class="no_dec" href="#Stylization Generation">Stylization Generation</a></h2>
    <p class="little_split" id='OmniStyle2stylization generation'></p>
    <div style="border-left: 16px solid #9ACAF0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('OmniStyle2-Stylization Generation-details')"><i>OmniStyle2: Scalable and High Quality Artistic Style Transfer Data Generation via Destylization</i></p>
    <p class="paper_detail">Ye Wang, Zili Yi, Yibo Zhang, Peng Zheng, Xuping Xie, Jiang Lin, Yilin Wang, Rui Ma</p>
    <p class="paper_detail">Jilin University, Nanjing University, Shanghai Innovation Institute, Adobe, Engineering Research Center of Knowledge-Driven Human-Machine Intelligence</p>
    <p class="paper_detail"> <b><font color=#404040>arXiv</font></b>, <b>2025</b></font></p>
    <p class="paper_detail"><b>Sep 07, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>OmniStyle2</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2509.05970">paper</a></p>
    
    
    <div id='OmniStyle2-Stylization Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It introduces destylization to reverse style transfer and creates the <b>100K-pair DST-100K dataset</b>, enabling a simple <b>FLUX-based</b> model.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='SCFlowstylization generation'></p>
    <div style="border-left: 16px solid #9ACAF0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('SCFlow-Stylization Generation-details')"><i>SCFlow: Implicitly Learning Style and Content Disentanglement with Flow Models</i></p>
    <p class="paper_detail">Pingchuan Ma, Xiaopei Yang, Yusong Li, Ming Gui, Felix Krause, Johannes Schusterbauer, Björn Ommer</p>
    <p class="paper_detail">LMU Munich, Munich Center for Machine Learning</p>
    <p class="paper_detail">International Conference on Computer Vision (<b><font color=#404040>ICCV</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Aug 05, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>SCFlow</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2508.03402">paper</a></p>
    
    
    <div id='SCFlow-Stylization Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It implicitly disentangles style and content by learning an <b>invertible flow</b> between entangled and disentangled latent distributions.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='AIComposerstylization generation'></p>
    <div style="border-left: 16px solid #9ACAF0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('AIComposer-Stylization Generation-details')"><i>AIComposer: Any Style and Content Image Composition via Feature Integration</i></p>
    <p class="paper_detail">Haowen Li, Zhenfeng Fan, Zhang Wen, Zhengzhou Zhu, Yunjin Li</p>
    <p class="paper_detail">Peking University, Beijing Yuanli Science and Technology</p>
    <p class="paper_detail">International Conference on Computer Vision (<b><font color=#404040>ICCV</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Jul 28, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>AIComposer</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2507.20721">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/sherlhw/AIComposer">code</a></p>
    
    
    <div id='AIComposer-Stylization Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It achieves text-prompt-free stylization by <b>linearly separating and re-fusing content/style CLIP features</b>, guiding a single-branch 10-step diffusion.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='CSD-VARstylization generation'></p>
    <div style="border-left: 16px solid #9ACAF0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('CSD-VAR-Stylization Generation-details')"><i>CSD-VAR: Content-Style Decomposition in Visual Autoregressive Models</i></p>
    <p class="paper_detail">Quang-Binh Nguyen, Minh Luu, Quang Nguyen, Anh Tran, Khoi Nguyen</p>
    <p class="paper_detail">Qualcomm AI Research, MovianAI</p>
    <p class="paper_detail">International Conference on Computer Vision (<b><font color=#404040>ICCV</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Jul 18, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>CSD-VAR</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2507.13984">paper</a></p>
    
    
    <div id='CSD-VAR-Stylization Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It pioneers <b>VAR</b>-based content-style decomposition by scale-aware alternating optimization, SVD rectification, and augmented K-V memories.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='DGPSTstylization generation'></p>
    <div style="border-left: 16px solid #9ACAF0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('DGPST-Stylization Generation-details')"><i>Domain Generalizable Portrait Style Transfer</i></p>
    <p class="paper_detail">Xinbo Wang, Wenju Xu, Qing Zhang, Wei-Shi Zheng</p>
    <p class="paper_detail">Sun Yat-sen University, AMAZON, Key Laboratory of Machine Intelligence and Advanced Computing</p>
    <p class="paper_detail">International Conference on Computer Vision (<b><font color=#404040>ICCV</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Jul 08, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>DGPST</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2507.04243">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/wangxb29/DGPST">code</a></p>
    
    
    <div id='DGPST-Stylization Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It unifies dense semantic correspondence, AdaIN-Wavelet latent fusion, and dual-conditional diffusion to enable portrait style transfer.</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='OmniStylestylization generation'></p>
    <div style="border-left: 16px solid #9ACAF0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('OmniStyle-Stylization Generation-details')"><i>OmniStyle: Filtering High Quality Style Transfer Data at Scale</i></p>
    <p class="paper_detail">Ye Wang, Ruiqi Liu, Jiang Lin, Fei Liu, Zili Yi, Yilin Wang, Rui Ma</p>
    <p class="paper_detail">Jilin University, Nanjing University, ByteDance, Adobe, Engineering Research Center of Knowledge-Driven Human-Machine Intelligence, MOE, China</p>
    <p class="paper_detail">Conference on Computer Vision and Pattern Recognition (<b><font color=#404040>CVPR</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>May 20, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>OmniStyle</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2505.14028">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/wangyePHD/OmniStyle">code</a></p>
    
    
    <div id='OmniStyle-Stylization Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It introduces a dataset consisting of <b>150K</b> content-style-stylized image triplets across <b>1,000 styles</b> with textual discriptions and instruction prompts.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='DuoLoRAstylization generation'></p>
    <div style="border-left: 16px solid #9ACAF0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('DuoLoRA-Stylization Generation-details')"><i>DuoLoRA : Cycle-consistent and Rank-disentangled Content-Style Personalization</i></p>
    <p class="paper_detail">Aniket Roy, Shubhankar Borse, Shreya Kadambi, Debasmit Das, Shweta Mahajan, Risheek Garrepalli, Hyojin Park, Ankita Nayak, Rama Chellappa, Munawar Hayat, Fatih Porikli</p>
    <p class="paper_detail">Johns Hopkins University, Qualcomm AI Research</p>
    <p class="paper_detail">International Conference on Computer Vision (<b><font color=#404040>ICCV</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Apr 15, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>DuoLoRA</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2504.13206">paper</a></p>
    
    
    <div id='DuoLoRA-Stylization Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It disentangles content and style in diffusion LoRA merging by rank-dimension masking, layer priors, and cycle-consistency loss.
</font></p>
        
        <p>
<ul>
    <li>
</ul>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='Semantixstylization generation'></p>
    <div style="border-left: 16px solid #9ACAF0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('Semantix-Stylization Generation-details')"><i>Semantix: An Energy Guided Sampler for Semantic Style Transfer</i></p>
    <p class="paper_detail">Huiang He, Minghui Hu, Chuanxia Zheng, Chaoyue Wang, Tat-Jen Cham</p>
    <p class="paper_detail">South China University of Technology, Nanyang Technological University, University of Oxford, The University of Sydney, </p>
    <p class="paper_detail">International Conference on Learning Representations (<b><font color=#404040>ICLR</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Mar 28, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>Semantix</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2503.22344">paper</a></p>
    
    
    <div id='Semantix-Stylization Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It is a training-free, <b>energy-guided sampler</b> that performs semantic style and appearance transfer for both images and videos.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='SaMamstylization generation'></p>
    <div style="border-left: 16px solid #9ACAF0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('SaMam-Stylization Generation-details')"><i>SaMam: Style-aware State Space Model for Arbitrary Image Style Transfer</i></p>
    <p class="paper_detail">Hongda Liu, Longguang Wang, Ye Zhang, Ziru Yu, Yulan Guo</p>
    <p class="paper_detail">Sun Yat-Sen University, Aviation University of Air Force</p>
    <p class="paper_detail">Conference on Computer Vision and Pattern Recognition (<b><font color=#404040>CVPR</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Mar 20, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>SaMam</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2503.15934">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/Chernobyllight/SaMam">code</a></p>
    
    
    <div id='SaMam-Stylization Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
Introduce <b>Mamba model</b> into style transfer to improve efficiency.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='V-Styliststylization generation'></p>
    <div style="border-left: 16px solid #9ACAF0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('V-Stylist-Stylization Generation-details')"><i>V-Stylist: Video Stylization via Collaboration and Reflection of MLLM Agents</i></p>
    <p class="paper_detail">Zhengrong Yue, Shaobin Zhuang, Kunchang Li, Yanbo Ding, Yali Wang</p>
    <p class="paper_detail">Shanghai Jiao Tong University, Shanghai AI Laboratory, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, University of Chinese Academy of Sciences</p>
    <p class="paper_detail">Conference on Computer Vision and Pattern Recognition (<b><font color=#404040>CVPR</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Mar 15, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>V-Stylist</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2503.12077">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/ZhengrongYue/V-Stylist">code</a></p>
    
    
    <div id='V-Stylist-Stylization Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It enables video stylization with <b>open style prompt</b> by searching a style tree and asigning the obtained weights to ControlNets.
</font></p>
        
        <p>
<ul>
    <li> <b>Video parser</b> splits the input video into shots and generates their text prompts.
    <li> <b>Style parser</b> search the matched style ControlNet model combination from a style tree.
    <li> <b>Style artist</b> renders the video shots by applying the matched style ControlNet models.
</ul>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='SMSstylization generation'></p>
    <div style="border-left: 16px solid #9ACAF0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('SMS-Stylization Generation-details')"><i>Balanced Image Stylization with Style Matching Score</i></p>
    <p class="paper_detail">Yuxin Jiang, Liming Jiang, Shuai Yang, Jia-Wei Liu, Ivor Tsang, Mike Zheng Shou</p>
    <p class="paper_detail">National University of Singapore, Technology and Research (A*STAR), Nanyang Technological University, Peking University</p>
    <p class="paper_detail">International Conference on Computer Vision (<b><font color=#404040>ICCV</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Mar 10, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>SMS</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2503.07601">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/showlab/SMS">code</a></p>
    
    
    <div id='SMS-Stylization Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It reframes stylization as <b>style-distribution matching</b> with LoRA priors, regularizing in frequency domain and semantically refining gradients.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='SCSAstylization generation'></p>
    <div style="border-left: 16px solid #9ACAF0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('SCSA-Stylization Generation-details')"><i>SCSA: A Plug-and-Play Semantic Continuous-Sparse Attention for Arbitrary Semantic Style Transfer</i></p>
    <p class="paper_detail">Chunnan Shang, Zhizhong Wang, Hongwei Wang, Xiangming Meng</p>
    
    <p class="paper_detail">Conference on Computer Vision and Pattern Recognition (<b><font color=#404040>CVPR</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Mar 06, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>SCSA</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2503.04119">paper</a></p>
    
    
    <div id='SCSA-Stylization Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It augments attention-based training-free arbitrary style transfer with semantic-aware continuous sparse attention.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='K-LoRAstylization generation'></p>
    <div style="border-left: 16px solid #9ACAF0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('K-LoRA-Stylization Generation-details')"><i>K-LoRA: Unlocking Training-Free Fusion of Any Subject and Style LoRAs</i></p>
    <p class="paper_detail">Ziheng Ouyang, Zhen Li, Qibin Hou</p>
    <p class="paper_detail">Nankai University</p>
    <p class="paper_detail">Conference on Computer Vision and Pattern Recognition (<b><font color=#404040>CVPR</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Feb 25, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>K-LoRA</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2502.18461">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/HVision-NKU/K-LoRA">code</a></p>
    
    
    <div id='K-LoRA-Stylization Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It introduces <b>training-free LoRA fusion</b> that compares Top-K elements in LoRAs to be fused and determines which LoRA to select for optimal fusion.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='MaskSTstylization generation'></p>
    <div style="border-left: 16px solid #9ACAF0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('MaskST-Stylization Generation-details')"><i>Less is More: Masking Elements in Image Condition Features Avoids Content Leakages in Style Transfer Diffusion Models</i></p>
    <p class="paper_detail">Lin Zhu, Xinbing Wang, Chenghu Zhou, Qinying Gu, Nanyang Ye</p>
    <p class="paper_detail">Shanghai Jiao Tong University, Chinese Academy of Sciences, Shanghai Artificial Intelligence Laboratory</p>
    <p class="paper_detail">International Conference on Learning Representations (<b><font color=#404040>ICLR</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Feb 11, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>MaskST</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2502.07466">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/LinLLLL/MaskST">code</a></p>
    
    
    <div id='MaskST-Stylization Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It <b>masks content-correlated entries</b> in the style-reference feature to achieve training-free, leakage-free text-driven style transfer.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='HSIstylization generation'></p>
    <div style="border-left: 16px solid #9ACAF0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('HSI-Stylization Generation-details')"><i>HSI: A Holistic Style Injector for Arbitrary Style Transfer</i></p>
    <p class="paper_detail">Shuhao Zhang, Hui Kang, Yang Liu, Fang Mei, Hongjuan Li</p>
    <p class="paper_detail">Jilin University, Jilin University of Arts</p>
    <p class="paper_detail">Conference on Computer Vision and Pattern Recognition (<b><font color=#404040>CVPR</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Feb 05, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>HSI</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2502.04369">paper</a></p>
    
    
    <div id='HSI-Stylization Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It replaces attention with element-wise <b>global-statistic injection</b>, achieving high-quality arbitrary style transfer.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='StyleSSPstylization generation'></p>
    <div style="border-left: 16px solid #9ACAF0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('StyleSSP-Stylization Generation-details')"><i>StyleSSP: Sampling StartPoint Enhancement for Training-free Diffusion-based Method for Style Transfer</i></p>
    <p class="paper_detail">Ruojun Xu, Weijie Xi, Xiaodi Wang, Yongbo Mao, Zach Cheng</p>
    <p class="paper_detail">Zhejiang University, Dcar</p>
    <p class="paper_detail">Conference on Computer Vision and Pattern Recognition (<b><font color=#404040>CVPR</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Jan 20, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>StyleSSP</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2501.11319">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/bytedance/StyleSSP">code</a></p>
    
    
    <div id='StyleSSP-Stylization Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It employs DDIM reversion of content images with low frequency component removed to obtain a <b>better initial noise</b> for training-free style transfer.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='IntroStylestylization generation'></p>
    <div style="border-left: 16px solid #9ACAF0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('IntroStyle-Stylization Generation-details')"><i>IntroStyle: Training-Free Introspective Style Attribution using Diffusion Features</i></p>
    <p class="paper_detail">Anand Kumar, Jiteng Mu, Nuno Vasconcelos</p>
    <p class="paper_detail">University of California, San Diego</p>
    <p class="paper_detail">International Conference on Computer Vision (<b><font color=#404040>ICCV</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Dec 19, 2024</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>IntroStyle</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2412.14432">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://anandk27.github.io/IntroStyle/">code</a></p>
    
    
    <div id='IntroStyle-Stylization Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It proposes a training-free style attribution framework that leverages only <b>internal statistics</b> of pre-trained diffusion features for style similarity retrieval.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='StyleStudiostylization generation'></p>
    <div style="border-left: 16px solid #9ACAF0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('StyleStudio-Stylization Generation-details')"><i>StyleStudio: Text-Driven Style Transfer with Selective Control of Style Elements</i></p>
    <p class="paper_detail">Mingkun Lei, Xue Song, Beier Zhu, Hao Wang, Chi Zhang</p>
    <p class="paper_detail">Westlake University, Fudan University, Nanyang Technological University, The Hong Kong University of Science and Technology (Guangzhou)</p>
    <p class="paper_detail">Conference on Computer Vision and Pattern Recognition (<b><font color=#404040>CVPR</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Dec 11, 2024</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>StyleStudio</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2412.08503">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/Westlake-AGI-Lab/StyleStudio">code</a></p>
    
    
    <div id='StyleStudio-Stylization Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It improves text-driven style transfer by <b>cross-modal AdaIN</b>, teacher model guidance, and <b>style CFG</b> with negative style images.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='StyleMasterstylization generation'></p>
    <div style="border-left: 16px solid #9ACAF0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('StyleMaster-Stylization Generation-details')"><i>StyleMaster: Stylize Your Video with Artistic Generation and Translation</i></p>
    <p class="paper_detail">Zixuan Ye, Huijuan Huang, Xintao Wang, Pengfei Wan, Di Zhang, Wenhan Luo</p>
    <p class="paper_detail">Hong Kong University of Science and Technology, KuaiShou Technology</p>
    <p class="paper_detail">Conference on Computer Vision and Pattern Recognition (<b><font color=#404040>CVPR</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Dec 10, 2024</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>StyleMaster</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2412.07744">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/KwaiVGI/StyleMaster">code</a></p>
    
    
    <div id='StyleMaster-Stylization Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
</font></p>
        
        <p><figure>
<img data-src='resource\figs\StyleMaster\StyleMaster-fig1.png' width=900>
<figcaption>
<b>Figure 1.</b> <b>Structure.</b> (1) <b>CLIP</b> extracts patch features and embeddings. (2) <b>Patch features</b> with less similarity with text prompt are employed to remove content infomation. (3) <b>Global projection</b>, trained by contrastive data, extracts global style from <b>CLIP embeddings</b>. (4) <b>Local and global features</b> are injected into model by cross-attention. (5) <b>Gray tile ControlNet</b> injects gray information while removing style information.
</figcaption>
</figure></p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='LoRA.rarstylization generation'></p>
    <div style="border-left: 16px solid #9ACAF0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('LoRA.rar-Stylization Generation-details')"><i>LoRA.rar: Learning to Merge LoRAs via Hypernetworks for Subject-Style Conditioned Image Generation</i></p>
    <p class="paper_detail">Donald Shenaj, Ondrej Bohdal, Mete Ozay, Pietro Zanuttigh, Umberto Michieli</p>
    <p class="paper_detail">Samsung R&D Institute UK, University of Padova</p>
    <p class="paper_detail">International Conference on Computer Vision (<b><font color=#404040>ICCV</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Dec 06, 2024</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>LoRA.rar</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2412.05148">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/donaldssh/LoRA.rar">code</a></p>
    
    
    <div id='LoRA.rar-Stylization Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It trains a lightweight hypernetwork to predict <b>adaptive merging coefficients</b> for any unseen subject-style LoRA pair, achieving real-time generation.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='UnZipLoRAstylization generation'></p>
    <div style="border-left: 16px solid #9ACAF0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('UnZipLoRA-Stylization Generation-details')"><i>UnZipLoRA: Separating Content and Style from a Single Image</i></p>
    <p class="paper_detail">Chang Liu, Viraj Shah, Aiyu Cui, Svetlana Lazebnik</p>
    <p class="paper_detail">University of Illinois, Urbana-Champaign</p>
    <p class="paper_detail">International Conference on Computer Vision (<b><font color=#404040>ICCV</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Dec 05, 2024</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>UnZipLoRA</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2412.04465">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/changLiu77/UnZipLoRA">code</a></p>
    
    
    <div id='UnZipLoRA-Stylization Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It decomposes a single image into two orthogonal yet compatible <b>content / style LoRAs</b> via prompt-, column- and block-separation.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='VarInvstylization generation'></p>
    <div style="border-left: 16px solid #9ACAF0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('VarInv-Stylization Generation-details')"><i>Content-Style Learning from Unaligned Domains: Identifiability under Unknown Latent Dimensions</i></p>
    <p class="paper_detail">Sagar Shrestha, Xiao Fu</p>
    <p class="paper_detail">Mohamed bin Zayed University of Artificial Intelligence, New York University Shanghai, Carnegie Mellon University</p>
    <p class="paper_detail">International Conference on Learning Representations (<b><font color=#404040>ICLR</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Nov 06, 2024</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>VarInv</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2411.03755">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/Irislucent/variance-versus-invariance">code</a></p>
    
    
    <div id='VarInv-Stylization Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It shows that content and style can be identified from <b>unaligned multi-domain data</b> without knowing their dimensions by distribution matching.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='CompReverstylization generation'></p>
    <div style="border-left: 16px solid #9ACAF0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('CompRever-Stylization Generation-details')"><i>Towards Compact Reversible Image Representations for Neural Style Transfer</i></p>
    <p class="paper_detail">Xiyao Liu, Siyu Yang, Xunli Fan, Jian Zhang, Songtao Wu, Gerald Schaefer, Hui Fang</p>
    <p class="paper_detail">Central South University, Loughborough University, Hunan Embroidery Research Institute, Northwest University, Sony R&D Center China</p>
    <p class="paper_detail">European Conference on Computer Vision (<b><font color=#404040>ECCV</font></b>), <b>2024</b></font></p>
    <p class="paper_detail"><b>Sep 29, 2024</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>CompRever</font></b></p>
    <p class="paper_detail"><a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/08321.pdf">paper</a></p>
    
    
    <div id='CompRever-Stylization Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It enforces <b>information-theoretic redundancy reduction</b> within a reversible flow to achieve compact yet expressive representations.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='FineStylestylization generation'></p>
    <div style="border-left: 16px solid #9ACAF0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('FineStyle-Stylization Generation-details')"><i>FineStyle: Fine-grained Controllable Style Personalization for Text-to-image Models</i></p>
    <p class="paper_detail">Gong Zhang, Kihyuk Sohn, Meera Hahn, Humphrey Shi, Irfan Essa</p>
    <p class="paper_detail">Georgia Tech, Google DeepMind, Meta Reality Labs</p>
    <p class="paper_detail">Advances in Neural Information Processing Systems (<b><font color=#404040>NeurIPS</font></b>), <b>2024</b></font></p>
    <p class="paper_detail"><b>Sep 26, 2024</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>FineStyle</font></b></p>
    <p class="paper_detail"><a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/5edb0d52b6959d946afac7600f9f1e0c-Abstract-Conference.html">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/SHI-Labs/FineStyle">code</a></p>
    
    
    <div id='FineStyle-Stylization Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It disentangles style from a reference via concept-oriented data scaling and KV-adapter tuning, enabling leakage-free, controllable stylization.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='ACFunstylization generation'></p>
    <div style="border-left: 16px solid #9ACAF0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('ACFun-Stylization Generation-details')"><i>ACFun: Abstract-Concrete Fusion Facial Stylization</i></p>
    <p class="paper_detail">Jiapeng Ji, Kun Wei, Ziqi Zhang, Cheng Deng</p>
    <p class="paper_detail">Xidian University</p>
    <p class="paper_detail">Advances in Neural Information Processing Systems (<b><font color=#404040>NeurIPS</font></b>), <b>2024</b></font></p>
    <p class="paper_detail"><b>Sep 26, 2024</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>ACFun</font></b></p>
    <p class="paper_detail"><a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/e10a6a906ef323efaf708f76cf3c1d1e-Abstract-Conference.html">paper</a></p>
    
    
    <div id='ACFun-Stylization Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It disentangles <b>abstract and concrete style features</b> via CLIP-guided fusion, achieving one-shot, high-fidelity, and controllable facial stylization.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='StyleTokenizerstylization generation'></p>
    <div style="border-left: 16px solid #9ACAF0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('StyleTokenizer-Stylization Generation-details')"><i>StyleTokenizer: Defining Image Style by a Single Instance for Controlling Diffusion Models</i></p>
    <p class="paper_detail">Wen Li, Muyuan Fang, Cheng Zou, Biao Gong, Ruobing Zheng, Meng Wang, Jingdong Chen, Ming Yang</p>
    <p class="paper_detail">Ant Group, Hangzhou, China</p>
    <p class="paper_detail">European Conference on Computer Vision (<b><font color=#404040>ECCV</font></b>), <b>2024</b></font></p>
    <p class="paper_detail"><b>Sep 04, 2024</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>StyleTokenizer</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2409.02543">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/alipay/style-tokenizer">code</a></p>
    
    
    <div id='StyleTokenizer-Stylization Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It aligns a single-image <b>style embedding with the textual embedding space</b> via a style tokenizer, enabling zero-shot, disentangled style control.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='Style-Editorstylization generation'></p>
    <div style="border-left: 16px solid #9ACAF0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('Style-Editor-Stylization Generation-details')"><i>Style-Editor: Text-driven Object-centric Style Editing</i></p>
    <p class="paper_detail">Jihun Park, Jongmin Gim, Kyoungmin Lee, Seunghun Lee, Sunghoon Im</p>
    <p class="paper_detail">Republic of Korea</p>
    <p class="paper_detail">Conference on Computer Vision and Pattern Recognition (<b><font color=#404040>CVPR</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Aug 16, 2024</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>Style-Editor</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2408.08461">paper</a></p>
    
    
    <div id='Style-Editor-Stylization Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It achieves <b>object-level text-driven style editing</b> by identifying an object patches using CLIP and editing its style while preserving the background.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='RB-Modulationstylization generation'></p>
    <div style="border-left: 16px solid #9ACAF0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('RB-Modulation-Stylization Generation-details')"><i>RB-Modulation: Training-Free Personalization of Diffusion Models using Stochastic Optimal Control</i></p>
    <p class="paper_detail">Litu Rout, Yujia Chen, Nataniel Ruiz, Abhishek Kumar, Constantine Caramanis, Sanjay Shakkottai, Wen-Sheng Chu</p>
    
    <p class="paper_detail">International Conference on Learning Representations (<b><font color=#404040>ICLR</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>May 27, 2024</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>RB-Modulation</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2405.17401">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/google/RB-Modulation">code</a></p>
    
    
    <div id='RB-Modulation-Stylization Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It introduces a <b>stochastic optimal control</b> for test-time, training-free personalization via style control and a cross-attention disentanglement module.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='B-LoRAstylization generation'></p>
    <div style="border-left: 16px solid #9ACAF0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('B-LoRA-Stylization Generation-details')"><i>Implicit Style-Content Separation using B-LoRA</i></p>
    <p class="paper_detail">Yarden Frenkel, Yael Vinker, Ariel Shamir, Daniel Cohen-Or</p>
    <p class="paper_detail">Tel Aviv University, Reichman University</p>
    <p class="paper_detail">European Conference on Computer Vision (<b><font color=#404040>ECCV</font></b>), <b>2024</b></font></p>
    <p class="paper_detail"><b>Mar 21, 2024</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>B-LoRA</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2403.14572">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/yardenfren1996/B-LoRA">code</a></p>
    
    
    <div id='B-LoRA-Stylization Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It implicitly disentangles style and content from an image by training two LoRA adapters on <b>specific SDXL blocks</b>, enabling plug-and-play stylization.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='ZipLoRAstylization generation'></p>
    <div style="border-left: 16px solid #9ACAF0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('ZipLoRA-Stylization Generation-details')"><i>ZipLoRA: Any Subject in Any Style by Effectively Merging LoRAs</i></p>
    <p class="paper_detail">Viraj Shah, Nataniel Ruiz, Forrester Cole, Erika Lu, Svetlana Lazebnik, Yuanzhen Li, Varun Jampani</p>
    <p class="paper_detail">Google Research, UIUC</p>
    <p class="paper_detail">European Conference on Computer Vision (<b><font color=#404040>ECCV</font></b>), <b>2024</b></font></p>
    <p class="paper_detail"><b>Nov 22, 2023</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>ZipLoRA</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2311.13600">paper</a></p>
    
    
    <div id='ZipLoRA-Stylization Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It learns <b>column-wise mixing coefficients</b> to orthogonally merge content and style LoRAs.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='InstaStylestylization generation'></p>
    <div style="border-left: 16px solid #9ACAF0; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('InstaStyle-Stylization Generation-details')"><i>InstaStyle: Inversion Noise of a Stylized Image is Secretly a Style Adviser</i></p>
    <p class="paper_detail">Xing Cui, Zekun Li, Pei Pei Li, Huaibo Huang, Xuannan Liu, Zhaofeng He</p>
    <p class="paper_detail">Beijing University of Posts and Telecommunications, University of California, Santa Barbara, Chinese Academy of Sciences</p>
    <p class="paper_detail">European Conference on Computer Vision (<b><font color=#404040>ECCV</font></b>), <b>2024</b></font></p>
    <p class="paper_detail"><b>Nov 05, 2023</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>InstaStyle</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2311.15040">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/cuixing100876/InstaStyle">code</a></p>
    
    
    <div id='InstaStyle-Stylization Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It leverages the <b>inversion noise of a stylized image</b> and refines a learnable style token, enabling one-shot stylized generation.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    <h2 id="Interactive Generation-table"><a class="no_dec" href="#Interactive Generation">Interactive Generation</a></h2>
    <p class="little_split" id='Matrix-Game 2.0interactive generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('Matrix-Game 2.0-Interactive Generation-details')"><i>Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive World Model</i></p>
    <p class="paper_detail">Xianglong He, Chunli Peng, Zexiang Liu, Boyang Wang, Yifan Zhang, Qi Cui, Fei Kang, Biao Jiang, Mengyin An, Yangyang Ren, Baixin Xu, Hao-Xiang Guo, Kaixiong Gong, Cyrus Wu, Wei Li, Xuchen Song, Yang Liu, Eric Li, Yahui Zhou</p>
    <p class="paper_detail">Skywork AI</p>
    <p class="paper_detail"> <b><font color=#404040>arXiv</font></b>, <b>2025</b></font></p>
    <p class="paper_detail"><b>Aug 18, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>Matrix-Game 2.0</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2508.13009">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/SkyworkAI/Matrix-Game/tree/main/Matrix-Game-2">code</a></p>
    
    
    <div id='Matrix-Game 2.0-Interactive Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It introduces a <b>causal few-step auto-regressive diffusion</b> framework distilled via <b>Self-Forcing</b> that enables minute-long, 25 fps videos.
</font></p>
        
        <p>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='Yaninteractive generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('Yan-Interactive Generation-details')"><i>Yan: Foundational Interactive Video Generation</i></p>
    <p class="paper_detail">Deheng Ye, Fangyun Zhou, Jiacheng Lv, Jianqi Ma, Jun Zhang, Junyan Lv, Junyou Li, Minwen Deng, Mingyu Yang, Qiang Fu, Wei Yang, Wenkai Lv, Yangbin Yu, Yewen Wang, Yonghang Guan, Zhihao Hu, Zhongbin Fang, Zhongqian Sun</p>
    <p class="paper_detail">Tencent</p>
    <p class="paper_detail"> <b><font color=#404040>arXiv</font></b>, <b>2025</b></font></p>
    <p class="paper_detail"><b>Aug 12, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>Yan</font></b></p>
    <p class="paper_detail"><a href="https://www.arxiv.org/pdf/2508.08601">paper</a></p>
    
    
    <div id='Yan-Interactive Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
</font></p>
        
        <p><ul>
<li> <b>AAAI-level simulation (Yan-Sim).</b> Design a highly-compressed, low-latency 3D-VAE (32x32x2-16chan with a light decoder) coupled with a KV-cache-based shift-window denoising inference process, achieving real-time 1080P/60FPS interative simulation.
<li> <b>Multi-modal generation (Yan-Gen).</b> Use autoregressive caption method to inject game-specific knowledge into open-domain, multimodal, interative video diffusion models.
<li> <b>Multi-granularity editing (Yan-Edit).</b> Disentangle interative mechanics simulation from visual rendering, enabling mutli-granularity video content editing during interaction through text.
<li><b>Data pipeline.</b> Use agent to collect and clean data (action & image pair) in the game environment of a renowned modern 3D game (Yuanmeng Star). Use VLM and depth model to obtain prompt and depth. Both labeled and unlabled data are used for training.
</ul>
<figure>
<img data-src='resource\figs\Yan\Yan-fig1.png' width=500>
<figcaption>
<b>Figure 1.</b> <b>Dataset comparisons.</b>
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Yan\Yan-fig2.png' width=550>
<figcaption>
<b>Figure 2.</b> <b>Yan-Sim structure.</b>
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Yan\Yan-fig3.png' width=550>
<figcaption>
<b>Figure 3.</b> <b>Yan-Gen structure.</b>
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Yan\Yan-fig4.png' width=550>
<figcaption>
<b>Figure 4.</b> <b>Yan-Edit structure.</b>
</figcaption>
</figure></p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='Matrix-Gameinteractive generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('Matrix-Game-Interactive Generation-details')"><i>Matrix-Game: Interactive World Foundation Model</i></p>
    <p class="paper_detail">Yifan Zhang, Chunli Peng, Boyang Wang, Puyi Wang, Qingcheng Zhu, Fei Kang, Biao Jiang, Zedong Gao, Eric Li, Yang Liu, Yahui Zhou</p>
    <p class="paper_detail">Skywork AI</p>
    <p class="paper_detail"> <b><font color=#404040>arXiv</font></b>, <b>2025</b></font></p>
    <p class="paper_detail"><b>Jun 23, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>Matrix-Game</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2506.18701">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/SkyworkAI/Matrix-Game/tree/main/Matrix-Game-1">code</a></p>
    
    
    <div id='Matrix-Game-Interactive Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It introduces an <b>image-to-world</b> diffusion model (17B) that learns from <b>3,700 h of Minecraft data</b> to generate game videos from a reference frame.
</font></p>
        
        <p><ul>
<li><b>Dataset.</b> Propose <b>Matrix-Game-MC</b>, a Minecraft dataset comprising over 2,700 hours of unlabled gameplay video clips (720p, 17 & 33 & 65-frame) and 1,200 hours of high-quality labeled clips (720p, 33-frame, balanced scenes) with keyboard and mouse action annotations.
<li><b>Training stage 1.</b> Large-scale unlabled pre-training for environment understanding.
<li><b>Training stage 2.</b> Action-labeled training for interactive video generation.
</ul>
<figure>
<img data-src='resource\figs\Matrix-Game\Matrix-Game-fig1.png' width=800>
<figcaption>
<b>Figure 1.</b> <b>Structure.</b> It adopts auto-regressive: the last few frames of each generated clip are used as motion conditions for generating the next clip.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Matrix-Game\Matrix-Game-fig2.png' width=800>
<figcaption>
<b>Figure 2.</b> <b>Model blocks.</b>
</figcaption>
</figure></p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='GameFactoryinteractive generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('GameFactory-Interactive Generation-details')"><i>GameFactory: Creating New Games with Generative Interactive Videos</i></p>
    <p class="paper_detail">Jiwen Yu, Yiran Qin, Xintao Wang, Pengfei Wan, Di Zhang, Xihui Liu</p>
    <p class="paper_detail">The University of Hong Kong, Kuaishou Technology</p>
    <p class="paper_detail">International Conference on Computer Vision (<b><font color=#404040>ICCV</font></b>), <b>2025</b></font></p>
    <p class="paper_detail"><b>Jan 14, 2025</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#404040>GameFactory</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2501.08325">paper</a> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <a href="https://github.com/KwaiVGI/GameFactory">code</a></p>
    
    
    <div id='GameFactory-Interactive Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It proposes a <b>decoupled-style</b> training pipeline that plugs action-control modules into a pre-trained video diffusion model to create games.
</font></p>
        
        <p>
<ul>
    <li><b>Training.</b> (1) Pre-train a video generation model. (2) Fine-tune with LoRA for game video data to capture style. (3) Train an action control module to learn style-agnoistic control. (4) Disgard the style LoRA and use the action control module for inference.
</ul>
</p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <p class="little_split" id='Genieinteractive generation'></p>
    <div style="border-left: 16px solid #F9D0A5; padding-left: 10px">
    <div style="height: 0.3em;"></div>
    <p class="paper_title" onclick="toggleTable('Genie-Interactive Generation-details')"><i>Genie: Generative Interactive Environments</i></p>
    <p class="paper_detail">Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, Yusuf Aytar, Sarah Bechtle, Feryal Behbahani, Stephanie Chan, Nicolas Heess, Lucy Gonzalez, Simon Osindero, Sherjil Ozair, Scott Reed, Jingwei Zhang, Konrad Zolna, Jeff Clune, Nando de Freitas, Satinder Singh, Tim Rocktäschel</p>
    <p class="paper_detail">Google DeepMind, University of British Columbia</p>
    <p class="paper_detail">International Conference on Machine Learning (<b><font color=#404040>ICML</font></b>), <b>2024</b></font></p>
    <p class="paper_detail"><b>Feb 23, 2024</b> &nbsp;&nbsp;<font color=#BBBBBB>|</font>&nbsp;&nbsp; <b><font color=#C55253>Genie</font></b></p>
    <p class="paper_detail"><a href="https://arxiv.org/pdf/2402.15391">paper</a></p>
    
    
    <div id='Genie-Interactive Generation-details' class="info_detail">
        <hr class="dashed">
        <p><font color=#202020>
It proposes a foundation world model (11B), comprising of a <b>video tokenizer</b>, an <b>autoregressive dynamics world</b>, and a <b>latent action model</b>. It is trained on 200K hours of Internet <b>gaming videos</b> without action or text labels, is controllable on frame-by-frame via a learned latent action space.
</font></p>
        
        <p><ul>
<li> <b>Data.</b> It is trained on 200K hours of Internet gaming videos <b>without action or text annotations</b>.
<li> <b>Training pipeline.</b> (1) Train the video tokenizer. (2) Co-train the latent action model and the dynamics model.
</ul>
<figure>
<img data-src='resource\figs\Genie\Genie-fig1.png' width=700>
<figcaption>
<b>Figure 1.</b> <b>Structure.</b>
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Genie\Genie-fig2.png' width=400>
<figcaption>
<b>Figure 2.</b> <b>Latent action model</b> infers the latent action between each pair of frames. It is a VQ-VAE with discrete set of codes equal the possible actions (e.g., 8). The <b>encoder</b> takes an previous frames and the next frame and outputs latent actions. The <b>decoder</b> takes all previous frames and latent actions and predicts the next frame. <b>Inference:</b> the entire LAM is discarded apart from the VQ codebook, and is replaced with user actions.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Genie\Genie-fig3.png' width=400>
<figcaption>
<b>Figure 3.</b> <b>Video tokenizer</b> converts each frame of raw video into discrete tokens.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Genie\Genie-fig4.png' width=200>
<figcaption>
<b>Figure 4.</b> <b>Dynamics model</b> takes latent action and past frame tokens and predicts the next frame. It is a decoder-only MaskGIT transformer.
</figcaption>
</figure></p>
    </div>
    <div style="height: 0.05em;"></div>
    </div>
    <p class="little_split"></p>
    
    <script>
        function toggleTable(tableId) {
            const container = document.getElementById(tableId);
            const isVisible = window.getComputedStyle(container).display !== 'none';
            if (!isVisible) {
                const images = container.querySelectorAll('img');
                images.forEach(img => {
                    if (img.dataset.src !== '') {img.src = img.dataset.src;}
                });
                container.style.display = 'block';
            } else {container.style.display = 'none';}
        }
    </script>
    
<button id="backToTop" title="back to top">↑</button>
<script>
    const button = document.getElementById("backToTop");
    window.addEventListener("scroll", () => {
        if (document.documentElement.scrollTop > 300) {
            button.style.display = "block";
        } else {
            button.style.display = "none";
        }
    });

    function updateButtonPosition() {
        const bodyRect = document.body.getBoundingClientRect();
        const windowWidth = window.innerWidth;
        const rightOffset = Math.max((windowWidth - bodyRect.width) / 2, 10);
        button.style.right = rightOffset + "px";
    }

    window.addEventListener("resize", updateButtonPosition);
    window.addEventListener("load", updateButtonPosition);

    button.addEventListener("click", () => {
        window.scrollTo({ top: 0, behavior: 'smooth' });
    });
</script>
</body>
</html>