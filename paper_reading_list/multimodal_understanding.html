
<!DOCTYPE html>
<html>
<head>
    <link rel="stylesheet" href="resource/html.css" type="text/css">
    <link rel="shortcut icon" href="resource/my_photo.jpg">
    <title>Paper Reading List</title>
    <meta name="description" content="Paper Reading List">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <div id="layout-content" style="margin-top:25px">
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.24.1/prism.min.js"></script>
    <link href="https://cdn.jsdelivr.net/npm/prismjs@1.24.1/themes/prism.css" rel="stylesheet">
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.24.1/components/prism-python.min.js"></script>
<body>

<h1 id="top">Multimodal Understanding</h1>
<p class="larger"><b><font color='#D93053'>11</font> papers on  Multimodal Understanding.</b></p>
<p>Curated by <a href="https://junkunyuan.github.io/">Junkun Yuan</a>.</p>
<p>Click <a href="paper_reading_list.html">here</a> to go back to main contents.</p>
<p><font color=#B0B0B0>Last updated on July 16, 2025 at 17:14 (UTC+8).</font></p>
<hr><p id='table' class="larger"><b>Table of contents:</b></p><ul><li><a class="no_dec" id="Foundation Algorithms & Models" href="#Foundation Algorithms & Models-table"><b>Foundation Algorithms & Models:</b></a> <a class="no_dec" href="#CLIP"><font color=#B0B0B0>CLIP</font></a> <font color=#B0B0B0>&nbsp;|&nbsp;</font> <a class="no_dec" href="#BLIP"><font color=#B0B0B0>BLIP</font></a> <font color=#B0B0B0>&nbsp;|&nbsp;</font> <a class="no_dec" href="#Flamingo"><font color=#B0B0B0>Flamingo</font></a> <font color=#B0B0B0>&nbsp;|&nbsp;</font> <a class="no_dec" href="#BLIP-2"><font color=#B0B0B0>BLIP-2</font></a> <font color=#B0B0B0>&nbsp;|&nbsp;</font> <a class="no_dec" href="#LLaVA"><font color=#B0B0B0>LLaVA</font></a> <font color=#B0B0B0>&nbsp;|&nbsp;</font> <a class="no_dec" href="#MiniGPT-4"><font color=#B0B0B0>MiniGPT-4</font></a> <font color=#B0B0B0>&nbsp;|&nbsp;</font> <a class="no_dec" href="#Qwen-VL"><font color=#B0B0B0>Qwen-VL</font></a> <font color=#B0B0B0>&nbsp;|&nbsp;</font> <a class="no_dec" href="#MiniGPT-v2"><font color=#B0B0B0>MiniGPT-v2</font></a> <font color=#B0B0B0>&nbsp;|&nbsp;</font> <a class="no_dec" href="#BLIP-3"><font color=#B0B0B0>BLIP-3</font></a> <font color=#B0B0B0>&nbsp;|&nbsp;</font> <a class="no_dec" href="#Qwen2-VL"><font color=#B0B0B0>Qwen2-VL</font></a> <font color=#B0B0B0>&nbsp;|&nbsp;</font> <a class="no_dec" href="#Qwen2.5-VL"><font color=#B0B0B0>Qwen2.5-VL</font></a></li></ul><h2 id="Foundation Algorithms & Models-table"><a class="no_dec" href="#Foundation Algorithms & Models">Foundation Algorithms & Models</a></h2>
            <p class="little_split" id='Qwen2.5-VL'></p>
            <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('Qwen2.5-VL-Foundation Algorithms & Models-details')"><i>Qwen2.5-VL Technical Report</i></p>
            <p class="paper_detail">Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, Junyang Lin</p>
            <p class="paper_detail">Alibaba Group</p>
            <p class="paper_detail"><b>Feb 19, 2025 &nbsp; Qwen2.5-VL</b> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/QwenLM/Qwen2.5-VL">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2502.13923">arXiv 2025</a> &nbsp; <font color=#D0D0D0>arXiv preprint</font></p>
            
            
            <div id='Qwen2.5-VL-Foundation Algorithms & Models-details' class="info_detail">
                <hr class="dashed">
                <p><font color=#202020>It makes improvement to Qwen2-VL by employing window attention, native dynamic resolution, absoute time encoding, more and high-quality data.</font></p>
                
                <p><ul>
<li> <b>Visual encoder.</b> The used ViT is trained from scratch. It employs self-attention + window attention to improve efficiency. It employs MRoPE as position embedding. Images and videos are sampled at native resolutions and dynamic frame rates.
<li> <b>Vision-Language Merger.</b> Group adjacent four visual patches, concat them along feature dimensions, and project them using a two-layer MLP.
<li> <b>Language model.</b> Qwen2.5 LLM.
<li> <b>Pre-training stages.</b> (1) Stage 1: ViT is trained to learn visual knowledge; (2) Stage 2: all model parameters are optimized to learn diverse knowledge and tasks; (3) Stage 3: all model parameters are optimized to learn long sequences by incorpoating video and agent-based data.
<li> <b>Post-training stages.</b> SFT and DPO are employed to optimize the language model.
<li> <b>Sparkling capabilities.</b> Omni-document parsing, precise object grounding (based on real resolution), ultra-long video understanding and grounding, and enhanced agent functionality.
</ul>
<figure>
<img data-src='resource\figs\Qwen2.5-VL\Qwen2.5-VL-fig1.png' width=700>
<figcaption>
<b>Figure 1.</b> <b>Model structure.</b>
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Qwen2.5-VL\Qwen2.5-VL-fig2.png' width=450>
<figcaption>
<b>Figure 2.</b> <b>Model structure details.</b>
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Qwen2.5-VL\Qwen2.5-VL-fig3.png' width=500>
<figcaption>
<b>Figure 3.</b> <b>Pre-training data.</b>
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Qwen2.5-VL\Qwen2.5-VL-fig4.png' width=700>
<figcaption>
<b>Figure 4.</b> <b>Performance on multimodal benchmarks.</b>
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Qwen2.5-VL\Qwen2.5-VL-fig5.png' width=450>
<figcaption>
<b>Figure 5.</b> <b>Performance on pure text benchmarks.</b>
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Qwen2.5-VL\Qwen2.5-VL-fig6.png' width=550>
<figcaption>
<b>Figure 6.</b> <b>Performance on OCR, chart, document understanding benchmarks.</b>
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Qwen2.5-VL\Qwen2.5-VL-fig7.png' width=500>
<figcaption>
<b>Figure 7.</b> <b>Performance on grounding benchmarks.</b>
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Qwen2.5-VL\Qwen2.5-VL-fig8.png' width=500>
<figcaption>
<b>Figure 8.</b> <b>Performance on counting benchmarks.</b>
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Qwen2.5-VL\Qwen2.5-VL-fig9.png' width=500>
<figcaption>
<b>Figure 9.</b> <b>Performance on video benchmarks.</b>
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Qwen2.5-VL\Qwen2.5-VL-fig10.png' width=550>
<figcaption>
<b>Figure 10.</b> <b>Performance on GUI agent benchmarks.</b>
</figcaption>
</figure></p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split" id='Qwen2-VL'></p>
            <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('Qwen2-VL-Foundation Algorithms & Models-details')"><i>Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution</i></p>
            <p class="paper_detail">Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, Junyang Lin</p>
            <p class="paper_detail">Alibaba Group</p>
            <p class="paper_detail"><b>Sep 18, 2024 &nbsp; Qwen2-VL</b> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/QwenLM/Qwen2.5-VL">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2409.12191">arXiv 2024</a> &nbsp; <font color=#D0D0D0>arXiv preprint</font></p>
            
            
            <div id='Qwen2-VL-Foundation Algorithms & Models-details' class="info_detail">
                <hr class="dashed">
                <p><font color=#202020>It improves Qwen-VL by using a <b>naive dynamic resolution</b> mechanism with <b>multimodal RoPE</b>, and a <b>unified image-video processing</b> paradigm.</font></p>
                
                <p><ul>
<li> <b>Visual encoder (675M).</b> Use self-developed ViT. Employ Naive Dynamic Resolution with 2D-RoPE to provide a variable number of visual tokens for images or videos with different resolution and frame number. Compress visual tokens by 2x2 using MLP.
<li> <b>Language model (1.5B, 7.6B, 72B).</b> Qwen series.
<li> <b>Unified image and video processing:</b> (1) Sample each video at two frames per second; (2) Compress video inputs by 4x using 3D convs; (3) Each image is treated as two identical frames for consistency. (4) Limit tokens per video are set to 16384 by adjusting the resolution.
<li> <b>Three-stage training (same as Qwen-VL).</b> (1) Pre-training on 600B tokens by optimizing ViT; (2) Multi-task pre-raining on 600B + 800B tokens by optimizing all model parameters; (3) Instruction tuning on instructuion-following data (ChatML format) by optimizing LLM.
<li> <b>Three model sizes:</b> Qwen2-VL-2B (on-device), Qwen2-VL-7B (performance-optimized), Qwen2-VL-72B (most capable).
<li> <b>Capabilities:</b> general chat, multilingual image text understanding, formula recognition, function calling, UI interation, long document understanding, code/math reasoning, video understanding, grounding, live chat, and agent potential.
</ul>
<figure>
<img data-src='resource\figs\Qwen2-VL\Qwen2-VL-fig1.png' width=600>
<figcaption>
<b>Figure 1.</b> <b>Qwen2-VL Structure.</b> It discards the multimodal connector module used in Qwen-VL.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Qwen2-VL\Qwen2-VL-fig2.png' width=600>
<figcaption>
<b>Figure 2.</b> <b>Unified Multimodal Rotary Position Embedding (M-RoPE)</b> for text, images, and videos.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Qwen2-VL\Qwen2-VL-fig3.png' width=500>
<img data-src='resource\figs\Qwen2-VL\Qwen2-VL-fig4.png' width=500>
<img data-src='resource\figs\Qwen2-VL\Qwen2-VL-fig5.png' width=500>
<figcaption>
<b>Figure 3.</b> <b>Dataset format</b> example.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Qwen2-VL\Qwen2-VL-fig6.png' width=700>
<img data-src='resource\figs\Qwen2-VL\Qwen2-VL-fig7.png' width=700>
<img data-src='resource\figs\Qwen2-VL\Qwen2-VL-fig8.png' width=600>
<figcaption>
<b>Figure 4.</b> Comparisons with other state-of-the-art on <b>image-based</b> (top), <b>video-based</b> (middle), and <b>agent-based</b> (bottom) multimodal tasks.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Qwen2-VL\Qwen2-VL-fig9.png' width=550>
<figcaption>
<b>Figure 5.</b> <b>Dynamic resolution</b> achieves competitive results while consuming fewer tokens.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Qwen2-VL\Qwen2-VL-fig10.png' width=550>
<figcaption>
<b>Figure 6.</b> Some perceptual tasks enjoy a reasonable range of <b>min resolution size</b> which is used to scale small images.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Qwen2-VL\Qwen2-VL-fig11.png' width=550>
<img data-src='resource\figs\Qwen2-VL\Qwen2-VL-fig12.png' width=550>
<figcaption>
<b>Figure 7.</b> <b>M-RoPE</b> improves performance and shows great extrapolation capability.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Qwen2-VL\Qwen2-VL-fig13.png' width=650>
<figcaption>
<b>Figure 8.</b> <b>Scaling performance</b> of model size and consuming tokens.
</figcaption>
</figure></p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split" id='BLIP-3'></p>
            <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('BLIP-3-Foundation Algorithms & Models-details')"><i>xGen-MM (BLIP-3): A Family of Open Large Multimodal Models</i></p>
            <p class="paper_detail">Le Xue, Manli Shu, Anas Awadalla, Jun Wang, An Yan, Senthil Purushwalkam, Honglu Zhou, Viraj Prabhu, Yutong Dai, Michael S Ryoo, Shrikant Kendre, Jieyu Zhang, Shaoyen Tseng, Gustavo A Lujan-Moreno, Matthew L Olson, Musashi Hinck, David Cobbley, Vasudev Lal, Can Qin, Shu Zhang, Chia-Chih Chen, Ning Yu, Juntao Tan, Tulika Manoj Awalgaonkar, Shelby Heinecke, Huan Wang, Yejin Choi, Ludwig Schmidt, Zeyuan Chen, Silvio Savarese, Juan Carlos Niebles, Caiming Xiong, Ran Xu</p>
            <p class="paper_detail">Salesforce AI Research, Intel Labs, University of Washington</p>
            <p class="paper_detail"><b>Aug 16, 2024 &nbsp; BLIP-3</b> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/salesforce/LAVIS/tree/xgen-mm">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2408.08872">arXiv 2024</a> &nbsp; <font color=#D0D0D0>arXiv preprint</font></p>
            
            
            <div id='BLIP-3-Foundation Algorithms & Models-details' class="info_detail">
                <hr class="dashed">
                <p><font color=#202020>It improves BLIP-2 by introducing interleaved multimodal data, unified training objective, and visual resampler.</font></p>
                
                <p><ul>
<li> <b>Training.</b> (1) Stage 1: base resolution pre-training on 100B tokens with 384x384 visual resolution; (2) Stage 2: high resolution pre-training on high-quality data; (3) Stage 3: SFT on single-image instruction-following data; (4) Stage 4: SFT on multi-image interleaved data.
</ul>
<figure>
<img data-src='resource\figs\BLIP-3\BLIP-3-fig1.png' width=700>
<figcaption>
<b>Figure 1.</b> BLIP-3 improves BLIP-2 by introducing <b>interleaved data</b>, using <b>unified training objective</b>, and <b>fine-grained training stages</b>.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\BLIP-3\BLIP-3-fig2.png' width=400>
<figcaption>
<b>Figure 2.</b> <b>Structure.</b> It replaces Q-Former in BLIP-2 by a <b>sampler</b> (inspired by Flamingo). Only the sampler and the LLM (Phi-3) are trained.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\BLIP-3\BLIP-3-fig4.png' width=650>
<figcaption>
<b>Figure 3.</b> Performance on <b>single-image</b> benchmarks.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\BLIP-3\BLIP-3-fig3.png' width=300>
<figcaption>
<b>Figure 4.</b> Performance on <b>multi-image</b> benchmarks.
</figcaption>
</figure></p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split" id='MiniGPT-v2'></p>
            <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('MiniGPT-v2-Foundation Algorithms & Models-details')"><i>MiniGPT-v2: Large Language Model as a Unified Interface for Vision-Language Multi-Task Learning</i></p>
            <p class="paper_detail">Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, Mohamed Elhoseiny</p>
            <p class="paper_detail">King Abdullah University of Science and Technology (KAUST), Meta AI Research</p>
            <p class="paper_detail"><b>Oct 14, 2023 &nbsp; MiniGPT-v2</b> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/Vision-CAIR/MiniGPT-4">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2310.09478v1">arXiv 2024</a> &nbsp; <font color=#D0D0D0>arXiv preprint</font></p>
            
            
            <div id='MiniGPT-v2-Foundation Algorithms & Models-details' class="info_detail">
                <hr class="dashed">
                <p><font color=#202020>It makes the model learn to tackle 6 tasks with different <b>task identifiers</b> through three-stage training (maybe inspired by Qwen-VL).</font></p>
                
                <p><ul>
<li> <b>Visual structure.</b> Use ViT-G/14 from EVA-CLIP with a Q-Former (same as MiniGPT-4). Image resolution is increased from 224x224 to 448x448, and every four neighboring visual tokens are concatenated into a single token to save compute by reducing tokens.
<li> <b>Language structure.</b> Language model is upgraded from Vicuna to LLaMA2-chat (7B).
<li> <b>Task identifiers</b> are used by the model to identify tasks. VQA: [vqa]; captioning: [caption]; grounded captioning: [grounding]; referring expression comprehension: [refer]; referring expression generation: [identify]; object parsing and grounding: [detection].
<li> The <b>grounding task</b> is introduced to improve MiniGPT (maybe inspired by Qwen-VL).
</ul>
<figure>
<img data-src='resource\figs\MiniGPT-v2\MiniGPT-v2-fig1.png' width=500>
<figcaption>
<b>Figure 1.</b> <b>Training data.</b>
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\MiniGPT-v2\MiniGPT-v2-fig2.png' width=500>
<figcaption>
<b>Figure 2.</b> Performance on <b>VQA tasks.</b>
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\MiniGPT-v2\MiniGPT-v2-fig3.png' width=500>
<figcaption>
<b>Figure 3.</b> Performance on <b>referring expression comprehension tasks.</b>
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\MiniGPT-v2\MiniGPT-v2-fig4.png' width=500>
<figcaption>
<b>Figure 4.</b> Ablation studies on <b>task identifiers.</b>
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\MiniGPT-v2\MiniGPT-v2-fig5.png' width=300>
<figcaption>
<b>Figure 5.</b> Performance on <b>hallucination problem.</b>
</figcaption>
</figure></p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split" id='Qwen-VL'></p>
            <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('Qwen-VL-Foundation Algorithms & Models-details')"><i>Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities</i></p>
            <p class="paper_detail">Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, Jingren Zhou</p>
            <p class="paper_detail">Alibaba Group</p>
            <p class="paper_detail"><b>Aug 24, 2023 &nbsp; Qwen-VL</b> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/QwenLM/Qwen-VL">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2308.12966">arXiv 2023</a> &nbsp; <font color=#D0D0D0>arXiv preprint</font></p>
            
            
            <div id='Qwen-VL-Foundation Algorithms & Models-details' class="info_detail">
                <hr class="dashed">
                <p><font color=#202020>Built upon the language model Qwen-7B, it makes Qwen-VL to learn image description, QA, <b>grounding</b>, and <b>text-reading</b> by three-stage training.</font></p>
                
                <p><ul>
<li> <b>Visual encoder (1.9B):</b> ViT (Openclip's ViT-bigG).
<li> <b>Vision-language adapter (0.08B):</b> Q-Former with 2D absolute positional encodings to produce 256 visual tokens.
<li> <b>LLM (7.7B):</b> Qwen-7B.
<li> <b>Special tokens:</b> `&lt;img&gt; &lt;/img&gt;`: images; `&lt;box&gt; &lt;/box&gt;`: normalized bounding box; `&lt;ref&gt; &lt;/ref&gt;`: the content referred by bounding box.
<li> <b>Stage 1 (pre-training):</b> large-scale, weakly labeled, web-crawled image-text pairs. 5B data, 1.4B cleaned data (77% English and 23% Chinese). Freeze LLM and optimize the vision encoder and VL adapter. Train 50K steps with batchsize of 30720, consume 1.5B samples. Image: 224x224.
<li> <b>Stage 2 (multi-task pre-training).</b> Captioning, VQA, grounding, ref grounding, grounded captioning, OCR, pure-text autoregression. Image: 448x448. Train the whole model.
<li> <b>Stage 3 (instruction tuning).</b> Use 350K instruction tuning data. Freeze visual encoder and optimize the LLM and adapter.
<li> <b>Capabilities:</b> multi-lingual, multi-image, and multi-round conversation.
</ul>
<figure>
<img data-src='resource\figs\Qwen-VL\Qwen-VL-fig3.png' width=500>
<figcaption>
<b>Figure 1.</b> <b>Three-stage Training.</b>
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Qwen-VL\Qwen-VL-fig4.png' width=350>
<figcaption>
<b>Figure 2.</b> Data for training <b>stage 1</b>.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Qwen-VL\Qwen-VL-fig5.png' width=500>
<figcaption>
<b>Figure 3.</b> Data for training <b>stage 2</b>.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Qwen-VL\Qwen-VL-fig6.png' width=600>
<figcaption>
<b>Figure 4.</b> Performance on <b>image captioning & VQA</b>.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Qwen-VL\Qwen-VL-fig7.png' width=500>
<figcaption>
<b>Figure 5.</b> Performance on <b>text-oriented VQA</b>.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Qwen-VL\Qwen-VL-fig8.png' width=500>
<figcaption>
<b>Figure 6.</b> Performance on <b>referring expression comprehension</b>.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Qwen-VL\Qwen-VL-fig9.png' width=400>
<figcaption>
<b>Figure 7.</b> Performance on <b>instruction-following</b> benchmarks.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Qwen-VL\Qwen-VL-fig1.png' width=400>
<figcaption>
<b>Figure 8.</b> <b>Overall performance.</b>
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Qwen-VL\Qwen-VL-fig2.png' width=900>
<figcaption>
<b>Figure 9.</b> <b>Showcases</b> of Qwen-VL capability.
</figcaption>
</figure></p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split" id='MiniGPT-4'></p>
            <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('MiniGPT-4-Foundation Algorithms & Models-details')"><i>MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models</i></p>
            <p class="paper_detail">Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny</p>
            <p class="paper_detail">King Abdullah University of Science and Technology</p>
            <p class="paper_detail"><b>Apr 20, 2023 &nbsp; MiniGPT-4</b> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/Vision-CAIR/MiniGPT-4">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2304.10592">ICLR 2024</a> &nbsp; <font color=#D0D0D0>International Conference on Learning Representations</font></p>
            
            
            <div id='MiniGPT-4-Foundation Algorithms & Models-details' class="info_detail">
                <hr class="dashed">
                <p><font color=#202020>It aligns a frozen visual encoder with a frozen LLM (Vicuna) using <b>one projection layer</b>.</font></p>
                
                <p><ul>
<li> <b>Structure.</b> The same pretrained vision module as BLIP-2: ViT-G/14 from EVA-CLIP with a Q-Former. Language model: Vicuna. Connector: a single projection layer.
<li> <b>Training.</b> Pre-training + instruction-tuning. It only fine-tunes the projection layer.
<li> <b>Training data.</b> Pre-training: LAION, Conceptual Captions, SBU. Instruction-tuning: 3500 images from Conceptual Caption with captions generated by the pre-trained model (cleaned by ChatGPT).
</ul>
<figure>
<img data-src='resource\figs\MiniGPT-4\MiniGPT-4-fig1.png' width=350>
<img data-src='resource\figs\MiniGPT-4\MiniGPT-4-fig2.png' width=240>
<img data-src='resource\figs\MiniGPT-4\MiniGPT-4-fig3.png' width=250>
<figcaption>
<b>Figure 1.</b> Performance on <b>advanced vision-language tasks</b> (left), <b>COCO captioning</b> (middle), <b>detailed captioning & poem generation</b> (right).
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\MiniGPT-4\MiniGPT-4-fig4.png' width=300>
<img data-src='resource\figs\MiniGPT-4\MiniGPT-4-fig5.png' width=280>
<figcaption>
<b>Figure 2.</b> <b>Ablation</b> studies (left), <b>hallucination</b> evaluation (right).
</figcaption>
</figure></p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split" id='LLaVA'></p>
            <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('LLaVA-Foundation Algorithms & Models-details')"><i>Visual Instruction Tuning</i></p>
            <p class="paper_detail">Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee</p>
            <p class="paper_detail">University of Wisconsin-Madison, Microsoft Research, Columbia University</p>
            <p class="paper_detail"><b>Apr 17, 2023 &nbsp; LLaVA</b> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/haotian-liu/LLaVA">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2304.08485">NeurIPS 2023</a> &nbsp; <font color=#D0D0D0>Advances in Neural Information Processing Systems</font></p>
            
            
            <div id='LLaVA-Foundation Algorithms & Models-details' class="info_detail">
                <hr class="dashed">
                <p><font color=#202020>It makes the first attempt to use GPT-4 to generate multimodal instruction-following data and performs multimodal <b>instruction fine-tuning</b>.</font></p>
                
                <p><ul>
<li> <b>Structure.</b> (1) Vision encoder: pre-trained CLIP; (2) Connector: a linear layer; (3) Lanauge mode: Vicuna.
<li> <b>Instruction-following data.</b> 158K: 25K conversations + 23K detailed description + 77K complex reasoning.
<li> <b>Training.</b> (1) Stage 1: train connector on CC3M instruction-following data; (2) Stage 2: train connector & LLM on 158K instruction-following data.
</ul>
<figure>
<img data-src='resource\figs\LLaVA\LLaVA-fig2.png' width=350>
<figcaption>
<b>Figure 1.</b> <b>Structure.</b>
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\LLaVA\LLaVA-fig1.png' width=500>
<figcaption>
<b>Figure 2.</b> Use the context to build <b>instruction-following data</b> by promting GPT.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\LLaVA\LLaVA-fig3.png' width=400>
<figcaption>
<b>Figure 3.</b> <b>Instruction-following</b> performance.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\LLaVA\LLaVA-fig4.png' width=500>
<figcaption>
<b>Figure 4.</b> <b>Science QA</b> performance.
</figcaption>
</figure></p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split" id='BLIP-2'></p>
            <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('BLIP-2-Foundation Algorithms & Models-details')"><i>BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</i></p>
            <p class="paper_detail">Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi</p>
            <p class="paper_detail">Salesforce Research</p>
            <p class="paper_detail"><b>Jan 30, 2023 &nbsp; BLIP-2</b> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/salesforce/LAVIS/tree/main/projects/blip2">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2301.12597">ICML 2023</a> &nbsp; <font color=#D0D0D0>International Conference on Machine Learning</font></p>
            
            
            <div id='BLIP-2-Foundation Algorithms & Models-details' class="info_detail">
                <hr class="dashed">
                <p><font color=#202020></font></p>
                
                <p><ul>
<li> <b>Vision encoder.</b> ViT-L/14 from CLIP or ViT-G/14 from EVA-CLIP.
<li> <b>Language model.</b> OPT model or FlanT5.
<li> <b>Querying Transformer (Q-Former).</b> An image transformer & a text transformer, they are initialized from BERT and share the self-attention layer.
<li> <b>Training.</b> (1) Stage 1 (250K steps): learn vision-langauge representations from a frozen image encoder by optimizing the three losses used in BLIP; (2) Stage 2 (80K steps): learn vision-to-language generation from a frozen LLM.
<li> <b>Data.</b> Basically same as BLIP. Only the Q-Former is trained.
</ul>
<figure>
<img data-src='resource\figs\BLIP-2\BLIP-2-fig1.png' width=400>
<figcaption>
<b>Figure 1.</b> <b>Overall structure</b>. Visual query embeddings are projected and prepended to the input text embeddings as Q-Former output & LLM input.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\BLIP-2\BLIP-2-fig2.png' width=700>
<figcaption>
<b>Figure 2.</b> <b>Q-Former</b> (left) with 32 query tokens, and <b>self-attention masking strategy</b> (right) for different training tasks.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\BLIP-2\BLIP-2-fig3.png' width=650>
<figcaption>
<b>Figure 3.</b> <b>Zero-shot</b> performance on vision-language tasks.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\BLIP-2\BLIP-2-fig4.png' width=500>
<figcaption>
<b>Figure 4.</b> <b>Zero-shot VQA</b> performance.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\BLIP-2\BLIP-2-fig5.png' width=640>
<img data-src='resource\figs\BLIP-2\BLIP-2-fig6.png' width=260>
<figcaption>
<b>Figure 5.</b> <b>Fine-tuning captioning (left) & VQA (right)</b> performance.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\BLIP-2\BLIP-2-fig7.png' width=700>
<figcaption>
<b>Figure 6.</b> <b>Fine-tuning image-text retrieval</b> performance.
</figcaption>
</figure></p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split" id='Flamingo'></p>
            <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('Flamingo-Foundation Algorithms & Models-details')"><i>Flamingo: a Visual Language Model for Few-Shot Learning</i></p>
            <p class="paper_detail">Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, Karen Simonyan</p>
            <p class="paper_detail">DeepMind</p>
            <p class="paper_detail"><b>Apr 29, 2022 &nbsp; Flamingo</b>  &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2204.14198">NeurIPS 2022</a> &nbsp; <font color=#D0D0D0>Advances in Neural Information Processing Systems</font></p>
            
            
            <div id='Flamingo-Foundation Algorithms & Models-details' class="info_detail">
                <hr class="dashed">
                <p><font color=#202020>It achieves <b>few-shot in-context learning</b> ability by brideging vision and language models and trains on inter-leaved visual and textual data.</font></p>
                
                <p><ul>
<li> <b>Visual encoder.</b> Use pre-trained and frozen Normalizer-Free ResNet, and pre-train it using contrastive loss. Images and videos (sample_fps=1) are compressed to spatio-temporal grid of features.
<li> <b>Perceiver resampler (Q-Former).</b> It processes a variable number of image or video tokens and produces a fixed number of visual tokens (64).
<li> <b>Gated xattn-dense layers.</b> They are inserted to the pre-trained, frozen language model (Chinchilla) and are trained from scratch.
<li> <b>Model size.</b> Flamingo-3B, Flamingo-9B, and Flamingo-80B.
</ul>
<figure>
<img data-src='resource\figs\Flamingo\Flamingo-fig2.png' width=500>
<img data-src='resource\figs\Flamingo\Flamingo-fig3.png' width=600>
<figcaption>
<b>Figure 1.</b> <b>Overall structure</b> (top) and <b>gated xattn-dense layers</b> (bottom).
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Flamingo\Flamingo-fig1.png' width=600>
<figcaption>
<b>Figure 2.</b> <b>Few-shot (no fine-tuning)</b> performance. More shots and larger model size lead to better performance.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Flamingo\Flamingo-fig4.png' width=700>
<figcaption>
<b>Figure 3.</b> Performance on image and video understanding tasks with <b>few-shot learning</b>.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\Flamingo\Flamingo-fig5.png' width=700>
<figcaption>
<b>Figure 4.</b> Performance on image and video understanding tasks with <b>fine-tuning</b>.
</figcaption>
</figure></p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split" id='BLIP'></p>
            <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('BLIP-Foundation Algorithms & Models-details')"><i>BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</i></p>
            <p class="paper_detail">Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi</p>
            <p class="paper_detail">Salesforce Research</p>
            <p class="paper_detail"><b>Jan 28, 2022 &nbsp; BLIP</b> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/salesforce/BLIP">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2201.12086">ICML 2022</a> &nbsp; <font color=#D0D0D0>International Conference on Machine Learning</font></p>
            
            
            <div id='BLIP-Foundation Algorithms & Models-details' class="info_detail">
                <hr class="dashed">
                <p><font color=#202020>It enables both vision-language <b>understanding & generation</b> by multi-task learning with a unified framework, as well as a data bootstrapping strategy.</font></p>
                
                <p><figure>
<img data-src='resource\figs\BLIP\BLIP-fig1.png' width=650>
<figcaption>
<b>Figure 1.</b> <b>Structure.</b> (1) Unimodal encoder is trained with an image-text contrastive (ITC) loss; (2) Image-grounded text encoder uses cross-attention layers, trained with an image-text matching (ITM) loss; (3) Image-grounded text decoder is trained with a language modeling (LM) loss.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\BLIP\BLIP-fig2.png' width=550>
<img data-src='resource\figs\BLIP\BLIP-fig3.png' width=300>
<figcaption>
<b>Figure 2.</b> Performance comparisons on <b>image-text retrival</b> by fine-tuning (left) or zero-shot (right).
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\BLIP\BLIP-fig4.png' width=550>
<img data-src='resource\figs\BLIP\BLIP-fig5.png' width=290>
<figcaption>
<b>Figure 3.</b> Performance comparisons on <b>image captioning</b> (left) and <b>VQA & NLVR</b> (right).
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\BLIP\BLIP-fig6.png' width=300>
<img data-src='resource\figs\BLIP\BLIP-fig7.png' width=290>
<figcaption>
<b>Figure 4.</b> Performance comparisons on <b>text-to-video retrival</b> (left) and <b>video question answering</b> (right).
</figcaption>
</figure></p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split" id='CLIP'></p>
            <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('CLIP-Foundation Algorithms & Models-details')"><i>Learning Transferable Visual Models From Natural Language Supervision</i></p>
            <p class="paper_detail">Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever</p>
            <p class="paper_detail">OpenAI</p>
            <p class="paper_detail"><b>Feb 26, 2021 &nbsp; CLIP</b> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/OpenAI/CLIP">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2103.00020">ICML 2021</a> &nbsp; <font color=#D0D0D0>International Conference on Machine Learning</font></p>
            <p class="paper_detail"><font color=#FF000>CLIP shifts computer vision research from high-quality, crowd-labeled data with pre-defined labels, e.g., ImageNet, to web-scale data with natural language supervision. CLIP generalizes well on visual benchmarks, and spurs research on multimodal foundation models. It has over 30,000 citations (as of Jul 2025).</font></p>
            
            <div id='CLIP-Foundation Algorithms & Models-details' class="info_detail">
                <hr class="dashed">
                <p><font color=#202020>By training on 400M internet text-image pairs through contrastive learning, it shows great generalization on visual benchmarks.</font></p>
                
                <p><figure>
<img data-src='resource\figs\CLIP\CLIP-fig1.png' width=650>
<figcaption>
<b>Figure 1.</b> <b>Training and inference pipelines.</b>
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\CLIP\CLIP-fig2.png' width=300>
<figcaption>
<b>Figure 2.</b> <b>Pseudocode for training CLIP.</b>
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\CLIP\CLIP-fig3.png' width=400>
<figcaption>
<b>Figure 3.</b> <b>Zero-shot CLIP</b> outperforms few-shot probes of SoTA visual models.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\CLIP\CLIP-fig4.png' width=500>
<figcaption>
<b>Figure 4.</b> <b>Linear probe CLIP</b> outperforms SoTA visual models.
</figcaption>
</figure>
<figure>
<img data-src='resource\figs\CLIP\CLIP-fig5.png' width=700>
<figcaption>
<b>Figure 5.</b> CLIP is much more robust to <b>distribution shift</b>.
</figcaption>
</figure></p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
    <script>
        function toggleTable(tableId) {
            const container = document.getElementById(tableId);
            const button = container.previousElementSibling;
            const isVisible = window.getComputedStyle(container).display !== 'none';
            if (!isVisible) {
                const images = container.querySelectorAll('img');
                images.forEach(img => {
                    if (img.dataset.src !== '') {
                        img.src = img.dataset.src;
                    }
                });
                container.style.display = 'block';
            } else {
                container.style.display = 'none';
                
            }
        }
    </script>

    <button id="backToTop" title="back to top">↑</button>
    <script>
        const button = document.getElementById("backToTop");
        window.addEventListener("scroll", () => {
        if (document.documentElement.scrollTop > 300) {
            button.style.display = "block";
        } else {
            button.style.display = "none";
        }
        });

        function updateButtonPosition() {
        const bodyRect = document.body.getBoundingClientRect();
        const windowWidth = window.innerWidth;
        const rightOffset = Math.max((windowWidth - bodyRect.width) / 2, 10);
        button.style.right = rightOffset + "px";
        }

        window.addEventListener("resize", updateButtonPosition);
        window.addEventListener("load", updateButtonPosition);

        button.addEventListener("click", () => {
        window.scrollTo({ top: 0, behavior: 'smooth' });
        });
    </script>
    
</body>
</html>
