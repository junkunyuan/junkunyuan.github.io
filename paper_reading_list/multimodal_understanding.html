
<!DOCTYPE html>
<html>
<head>
    <link rel="stylesheet" href="resource/html.css" type="text/css">
    <link rel="shortcut icon" href="resource/my_photo.jpg">
    <title>Paper Reading List</title>
    <meta name="description" content="Paper Reading List">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <div id="layout-content" style="margin-top:25px">
<body>

<h1 id="top">Multimodal Understanding</h1>
<p class="larger"><b><font color='#D93053'>4</font> papers on Multimodal Understanding.</b></p>
<p>Curated by <a href="https://junkunyuan.github.io/">Junkun Yuan</a>.</p>
<p>Click <a href="paper_reading_list.html">here</a> to go back to main contents.</p>
<p><font color=#B0B0B0>Last updated on July 07, 2025 at 17:03 (UTC+8).</font></p>
<hr><p id='table' class="larger"><b>Table of contents:</b></p><ul><li><a class="no_dec" href="#Foundation Algorithms & Models-table">Foundation Algorithms & Models</a></li></ul><h2 id="Foundation Algorithms & Models-table"><a class="no_dec" href="#top">Foundation Algorithms & Models</a></h2>
            <p class="little_split"></p>
            <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('Qwen2.5-VL-Foundation Algorithms & Models-details')"><i>Qwen2.5-VL Technical Report</i></p>
            <p class="paper_detail">Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, Junyang Lin</p>
            <p class="paper_detail">Alibaba Group</p>
            <p class="paper_detail"><b><font color=#202020>Feb 19, 2025 &nbsp; Qwen2.5-VL</font></b> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/QwenLM/Qwen2.5-VL">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2502.13923">arXiv 2025</a> &nbsp; <font color=#D0D0D0>arXiv preprint</font></p>
            
            
            <div id='Qwen2.5-VL-Foundation Algorithms & Models-details' class="info_detail">
                <p class="summary">It makes improvement to Qwen2-VL by employing window attention, native dynamic resolution, absoute time encoding, more and high-quality data.</p>
                
                <p><ul>
<li> <b>Visual encoder.</b> The used ViT is trained from scratch. It employs self-attention + window attention to improve efficiency. It employs MRoPE as position embedding. Images and videos are sampled at native resolutions and dynamic frame rates.
<li> <b>Vision-Language Merger.</b> Group adjacent four visual patches, concat them along feature dimensions, and project them using a two-layer MLP.
<li> <b>Language model.</b> Qwen2.5 LLM.
<li> <b>Pre-training stages.</b> (1) Stage 1: ViT is trained to learn visual knowledge; (2) Stage 2: all model parameters are optimized to learn diverse knowledge and tasks; (3) Stage 3: all model parameters are optimized to learn long sequences by incorpoating video and agent-based data.
<li> <b>Post-training stages.</b> SFT and DPO are employed to optimize the language model.
<li> <b>Sparkling capabilities.</b> Omni-document parsing, precise object grounding (based on real resolution), ultra-long video understanding and grounding, and enhanced agent functionality.
</ul>
<figure>
    <img src='resource/figs/Qwen2.5-VL/Qwen2.5-VL-fig1.png' width=700>
    <figcaption>
    <b>Figure 1.</b> <b>Model structure.</b>
    </figcaption>
</figure>
<figure>
    <img src='resource/figs/Qwen2.5-VL/Qwen2.5-VL-fig2.png' width=450>
    <figcaption>
    <b>Figure 2.</b> <b>Model structure details.</b>
    </figcaption>
</figure>
<figure>
    <img src='resource/figs/Qwen2.5-VL/Qwen2.5-VL-fig3.png' width=500>
    <figcaption>
    <b>Figure 3.</b> <b>Pre-training data.</b>
    </figcaption>
</figure>
<figure>
    <img src='resource/figs/Qwen2.5-VL/Qwen2.5-VL-fig4.png' width=700>
    <figcaption>
    <b>Figure 4.</b> <b>Performance on multimodal benchmarks.</b>
    </figcaption>
</figure>
<figure>
    <img src='resource/figs/Qwen2.5-VL/Qwen2.5-VL-fig5.png' width=450>
    <figcaption>
    <b>Figure 5.</b> <b>Performance on pure text benchmarks.</b>
    </figcaption>
</figure>
<figure>
    <img src='resource/figs/Qwen2.5-VL/Qwen2.5-VL-fig6.png' width=550>
    <figcaption>
    <b>Figure 6.</b> <b>Performance on OCR, chart, document understanding benchmarks.</b>
    </figcaption>
</figure>
<figure>
    <img src='resource/figs/Qwen2.5-VL/Qwen2.5-VL-fig7.png' width=500>
    <figcaption>
    <b>Figure 7.</b> <b>Performance on grounding benchmarks.</b>
    </figcaption>
</figure>
<figure>
    <img src='resource/figs/Qwen2.5-VL/Qwen2.5-VL-fig8.png' width=500>
    <figcaption>
    <b>Figure 8.</b> <b>Performance on counting benchmarks.</b>
    </figcaption>
</figure>
<figure>
    <img src='resource/figs/Qwen2.5-VL/Qwen2.5-VL-fig9.png' width=500>
    <figcaption>
    <b>Figure 9.</b> <b>Performance on video benchmarks.</b>
    </figcaption>
</figure>
<figure>
    <img src='resource/figs/Qwen2.5-VL/Qwen2.5-VL-fig10.png' width=550>
    <figcaption>
    <b>Figure 10.</b> <b>Performance on GUI agent benchmarks.</b>
    </figcaption>
</figure></p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split"></p>
            <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('Qwen2-VL-Foundation Algorithms & Models-details')"><i>Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution</i></p>
            <p class="paper_detail">Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, Junyang Lin</p>
            <p class="paper_detail">Alibaba Group</p>
            <p class="paper_detail"><b><font color=#202020>Sep 18, 2024 &nbsp; Qwen2-VL</font></b> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/QwenLM/Qwen2.5-VL">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2409.12191">arXiv 2024</a> &nbsp; <font color=#D0D0D0>arXiv preprint</font></p>
            
            
            <div id='Qwen2-VL-Foundation Algorithms & Models-details' class="info_detail">
                <p class="summary">It improves Qwen-VL by using a <b>naive dynamic resolution</b> mechanism with <b>multimodal RoPE</b>, and a <b>unified image-video processing</b> paradigm.</p>
                
                <p><ul>
<li> <b>Visual encoder (675M).</b> Use self-developed ViT. Employ Naive Dynamic Resolution with 2D-RoPE to provide a variable number of visual tokens for images or videos with different resolution and frame number. Compress visual tokens by 2x2 using MLP.
<li> <b>Language model (1.5B, 7.6B, 72B).</b> Qwen series.
<li> <b>Unified image and video processing:</b> (1) Sample each video at two frames per second; (2) Compress video inputs by 4x using 3D convs; (3) Each image is treated as two identical frames for consistency. (4) Limit tokens per video are set to 16384 by adjusting the resolution.
<li> <b>Three-stage training (same as Qwen-VL).</b> (1) Pre-training on 600B tokens by optimizing ViT; (2) Multi-task pre-raining on 600B + 800B tokens by optimizing all model parameters; (3) Instruction tuning on instructuion-following data (ChatML format) by optimizing LLM.
<li> <b>Three model sizes:</b> Qwen2-VL-2B (on-device), Qwen2-VL-7B (performance-optimized), Qwen2-VL-72B (most capable).
<li> <b>Capabilities:</b> general chat, multilingual image text understanding, formula recognition, function calling, UI interation, long document understanding, code/math reasoning, video understanding, grounding, live chat, and agent potential.
</ul>
<figure>
    <img src='resource/figs/Qwen2-VL/Qwen2-VL-fig1.png' width=600>
    <figcaption>
    <b>Figure 1.</b> <b>Qwen2-VL Structure.</b> It discards the multimodal connector module used in Qwen-VL.
    </figcaption>
</figure>
<figure>
    <img src='resource/figs/Qwen2-VL/Qwen2-VL-fig2.png' width=600>
    <figcaption>
    <b>Figure 2.</b> <b>Unified Multimodal Rotary Position Embedding (M-RoPE)</b> for text, images, and videos.
    </figcaption>
</figure>
<figure>
    <img src='resource/figs/Qwen2-VL/Qwen2-VL-fig3.png' width=500>
    <img src='resource/figs/Qwen2-VL/Qwen2-VL-fig4.png' width=500>
    <img src='resource/figs/Qwen2-VL/Qwen2-VL-fig5.png' width=500>
    <figcaption>
    <b>Figure 3.</b> <b>Dataset format</b> example.
    </figcaption>
</figure>
<figure>
    <img src='resource/figs/Qwen2-VL/Qwen2-VL-fig6.png' width=700>
    <img src='resource/figs/Qwen2-VL/Qwen2-VL-fig7.png' width=700>
    <img src='resource/figs/Qwen2-VL/Qwen2-VL-fig8.png' width=600>
    <figcaption>
    <b>Figure 4.</b> Comparisons with other state-of-the-art on <b>image-based</b> (top), <b>video-based</b> (middle), and <b>agent-based</b> (bottom) multimodal tasks.
    </figcaption>
</figure>
<figure>
    <img src='resource/figs/Qwen2-VL/Qwen2-VL-fig9.png' width=550>
    <figcaption>
    <b>Figure 5.</b> <b>Dynamic resolution</b> achieves competitive results while consuming fewer tokens.
    </figcaption>
</figure>
<figure>
    <img src='resource/figs/Qwen2-VL/Qwen2-VL-fig10.png' width=550>
    <figcaption>
    <b>Figure 6.</b> Some perceptual tasks enjoy a reasonable range of <b>min resolution size</b> which is used to scale small images.
    </figcaption>
</figure>
<figure>
    <img src='resource/figs/Qwen2-VL/Qwen2-VL-fig11.png' width=550>
    <img src='resource/figs/Qwen2-VL/Qwen2-VL-fig12.png' width=550>
    <figcaption>
    <b>Figure 7.</b> <b>M-RoPE</b> improves performance and shows great extrapolation capability.
    </figcaption>
</figure>
<figure>
    <img src='resource/figs/Qwen2-VL/Qwen2-VL-fig13.png' width=650>
    <figcaption>
    <b>Figure 8.</b> <b>Scaling performance</b> of model size and consuming tokens.
    </figcaption>
</figure></p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split"></p>
            <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('Qwen-VL-Foundation Algorithms & Models-details')"><i>Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities</i></p>
            <p class="paper_detail">Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, Jingren Zhou</p>
            <p class="paper_detail">Alibaba Group</p>
            <p class="paper_detail"><b><font color=#202020>Aug 24, 2023 &nbsp; Qwen-VL</font></b> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/QwenLM/Qwen-VL">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2308.12966">arXiv 2023</a> &nbsp; <font color=#D0D0D0>arXiv preprint</font></p>
            
            
            <div id='Qwen-VL-Foundation Algorithms & Models-details' class="info_detail">
                <p class="summary">Built upon the language model Qwen-7B, it makes Qwen-VL to learn image description, QA, <b>grounding</b>, and <b>text-reading</b> by three-stage training.</p>
                
                <p><ul>
<li> <b>Visual encoder (1.9B):</b> ViT (Openclip's ViT-bigG).
<li> <b>Vision-language adapter (0.08B):</b> Q-Former with 2D absolute positional encodings to produce 256 visual tokens.
<li> <b>LLM (7.7B):</b> Qwen-7B.
<li> <b>Special tokens:</b> `&lt;img&gt; &lt;/img&gt;`: images; `&lt;box&gt; &lt;/box&gt;`: normalized bounding box; `&lt;ref&gt; &lt;/ref&gt;`: the content referred by bounding box.
<li> <b>Stage 1 (pre-training):</b> large-scale, weakly labeled, web-crawled image-text pairs. 5B data, 1.4B cleaned data (77% English and 23% Chinese). Freeze LLM and optimize the vision encoder and VL adapter. Train 50K steps with batchsize of 30720, consume 1.5B samples. Image: 224x224.
<li> <b>Stage 2 (multi-task pre-training).</b> Captioning, VQA, grounding, ref grounding, grounded captioning, OCR, pure-text autoregression. Image: 448x448. Train the whole model.
<li> <b>Stage 3 (instruction tuning).</b> Use 350K instruction tuning data. Freeze visual encoder and optimize the LLM and adapter.
<li> <b>Capabilities:</b> multi-lingual, multi-image, and multi-round conversation.
</ul>
<figure>
    <img src='resource/figs/Qwen-VL/Qwen-VL-fig3.png' width=500>
    <figcaption>
    <b>Figure 1.</b> <b>Three-stage Training.</b>
    </figcaption>
</figure>
<figure>
    <img src='resource/figs/Qwen-VL/Qwen-VL-fig4.png' width=350>
    <figcaption>
    <b>Figure 2.</b> Data for training <b>stage 1</b>.
    </figcaption>
</figure>
<figure>
    <img src='resource/figs/Qwen-VL/Qwen-VL-fig5.png' width=500>
    <figcaption>
    <b>Figure 3.</b> Data for training <b>stage 2</b>.
    </figcaption>
</figure>
<figure>
    <img src='resource/figs/Qwen-VL/Qwen-VL-fig6.png' width=600>
    <figcaption>
    <b>Figure 4.</b> Performance on <b>image captioning & VQA</b>.
    </figcaption>
</figure>
<figure>
    <img src='resource/figs/Qwen-VL/Qwen-VL-fig7.png' width=500>
    <figcaption>
    <b>Figure 5.</b> Performance on <b>text-oriented VQA</b>.
    </figcaption>
</figure>
<figure>
    <img src='resource/figs/Qwen-VL/Qwen-VL-fig8.png' width=500>
    <figcaption>
    <b>Figure 6.</b> Performance on <b>referring expression comprehension</b>.
    </figcaption>
</figure>
<figure>
    <img src='resource/figs/Qwen-VL/Qwen-VL-fig9.png' width=400>
    <figcaption>
    <b>Figure 7.</b> Performance on <b>instruction-following</b> benchmarks.
    </figcaption>
</figure>
<figure>
    <img src='resource/figs/Qwen-VL/Qwen-VL-fig1.png' width=400>
    <figcaption>
    <b>Figure 8.</b> <b>Overall performance.</b>
    </figcaption>
</figure>
<figure>
    <img src='resource/figs/Qwen-VL/Qwen-VL-fig2.png' width=900>
    <figcaption>
    <b>Figure 9.</b> <b>Showcases</b> of Qwen-VL capability.
    </figcaption>
</figure></p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split"></p>
            <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('CLIP-Foundation Algorithms & Models-details')"><i>Learning Transferable Visual Models From Natural Language Supervision</i></p>
            <p class="paper_detail">Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever</p>
            <p class="paper_detail">OpenAI</p>
            <p class="paper_detail"><b><font color=#202020>Feb 26, 2021 &nbsp; CLIP</font></b> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/OpenAI/CLIP">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2103.00020">ICML 2021</a> &nbsp; <font color=#D0D0D0></font></p>
            <p class="paper_detail"><font color=#FF000>It shifts computer vision research from high-quality, crowd-labeled data with pre-defined labels like ImageNet to web-scale data with natural language supervision. It shows great generalization capability on visual benchmarks, and spurs research on multimodal foundation models. It has over 30K citations (as of Jul 2025).</font></p>
            
            <div id='CLIP-Foundation Algorithms & Models-details' class="info_detail">
                <p class="summary">By training on 400M internet text-image pairs through contrastive learning, it shows great generalization on visual benchmarks.</p>
                
                <p><figure>
    <img src='resource/figs/CLIP/CLIP-fig1.png' width=650>
    <figcaption>
    <b>Figure 1.</b> <b>Training and inference pipelines.</b>
    </figcaption>
</figure>
<figure>
    <img src='resource/figs/CLIP/CLIP-fig2.png' width=300>
    <figcaption>
    <b>Figure 2.</b> <b>Pseudocode for training CLIP.</b>
    </figcaption>
</figure>
<figure>
    <img src='resource/figs/CLIP/CLIP-fig3.png' width=400>
    <figcaption>
    <b>Figure 3.</b> <b>Zero-shot CLIP</b> outperforms few-shot probes of SoTA visual models.
    </figcaption>
</figure>
<figure>
    <img src='resource/figs/CLIP/CLIP-fig4.png' width=500>
    <figcaption>
    <b>Figure 4.</b> <b>Linear probe CLIP</b> outperforms SoTA visual models.
    </figcaption>
</figure>
<figure>
    <img src='resource/figs/CLIP/CLIP-fig5.png' width=700>
    <figcaption>
    <b>Figure 5.</b> CLIP is much more robust to <b>distribution shift</b>.
    </figcaption>
</figure></p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
    <script>
        function toggleTable(tableId) {
            const container = document.getElementById(tableId);
            const button = container.previousElementSibling;
            const isVisible = window.getComputedStyle(container).display !== 'none';
            if (!isVisible) {
                const images = container.querySelectorAll('.lazy-load');
                images.forEach(img => {
                    if (!img.src && img.dataset.src) {
                        img.src = img.dataset.src;
                    }
                });
                container.style.display = 'block';
                
            } else {
                container.style.display = 'none';
                
            }
        }
    </script>
    
</body>
</html>
