
<!DOCTYPE html>
<html>
<head>
<link rel="stylesheet" href="resource/html.css" type="text/css">
<link rel="shortcut icon" href="resource/my_photo.jpg">
<title>Paper Reading List</title>
<meta name="description" content="Paper Reading List">
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<div id="layout-content" style="margin-top:25px">
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.24.1/prism.min.js"></script>
            <link href="https://cdn.jsdelivr.net/npm/prismjs@1.24.1/themes/prism.css" rel="stylesheet">
            <script src="https://cdn.jsdelivr.net/npm/prismjs@1.24.1/components/prism-bash.min.js"></script>
            <script src="https://cdn.jsdelivr.net/npm/prismjs@1.24.1/components/prism-python.min.js"></script>
            <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
            <body>

<h1 id="top">Fundamental Components of Deep Learning</h1>
<p class="larger"><b>Fundamental components to build deep learning systems.</b></p>
<p class="larger"><b><font color='#D93053'>5</font> papers</b></p>
<p>Written by <a href="https://junkunyuan.github.io/">Junkun Yuan</a>.</p>
<p>Click <a href="paper_reading_list.html">here</a> to go back to main contents.</p>
<p><font color=#B0B0B0>Last updated on August 25, 2025 at 14:54 (UTC+8).</font></p>
<hr><p id='table' class="huger"><b>Table of contents:</b></p><p>Papers are displayed in reverse chronological order. Some highly-impact or inspiring works are highlighted in <font color="#D04040">red</font>.</p><ul><li><a class="no_dec larger low_margin" id="Normalization" href="#Normalization-table"><b>Normalization</b></a></li><p><table class="center"><tr>
<tr>
<td><a class="no_dec" href="#RMS Normalizationnormalization"><font color=#D04040>RMS Normalization <font size=1;>(NeurIPS 2019)</font></font></a></td>
<td><a class="no_dec" href="#Group Normalizationnormalization"><font color=#D04040>Group Normalization <font size=1;>(ECCV 2018)</font></font></a></td>
<td><a class="no_dec" href="#Instance Normalizationnormalization"><font color=#D04040>Instance Normalization <font size=1;>(arXiv 2016)</font></font></a></td>
<td><a class="no_dec" href="#Layer Normalizationnormalization"><font color=#D04040>Layer Normalization <font size=1;>(arXiv 2016)</font></font></a></td>
<td><a class="no_dec" href="#Batch Normalizationnormalization"><font color=#D04040>Batch Normalization <font size=1;>(ICML 2015)</font></font></a></td>
</tr>
</table></p><br></ul><h2 id="Normalization-table"><a class="no_dec" href="#Normalization">Normalization</a></h2>
            <p class="little_split" id='RMS Normalizationnormalization'></p>
            <div style="border-left: 14px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('RMS Normalization-Normalization-details')"><i>Root Mean Square Layer Normalization</i></p>
            <p class="paper_detail">Biao Zhang, Rico Sennrich</p>
            <p class="paper_detail">University of Edinburgh, University of Zurich</p>
            <p class="paper_detail"><b>Oct 16, 2019 &nbsp; <font color=#D04040>RMS Normalization</font></b> &nbsp;&nbsp;|&nbsp;&nbsp; NeurIPS 2019 &nbsp; <font color=#D0D0D0>Advances in Neural Information Processing Systems</font></p>
            <p class="paper_detail"><a href="https://arxiv.org/pdf/1910.07467">paper</a> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/bzhangGo/rmsnorm">code</a></p>
            
            
            <div id='RMS Normalization-Normalization-details' class="info_detail">
                <hr class="dashed">
                <p><font color=#202020>
It proposes an <b>efficient layer normalization</b> method that maintains the re-scaling invariance property of LayerNorm while eliminating re-centering.
</font></p>
                
                <p>
<ul>
    <li> <b>Why is RMS Norm more efficient than Layer Norm?</b> Layer Norm needs to calculate mean and variance, requiring two passes over the data and extra subtraction operations. RMS Norm only needs one pass, reducing operations and memory accesses and making it GPU-friendly. 
</ul>
<pre>
<code class="language-python" style="font-size: 14px;">
import torch
import torch.nn as nn

## --------------------------------------------------------------------------------
## Build customized RMS Normalization
## --------------------------------------------------------------------------------
class RMSNorm(nn.Module):
    def __init__(self, hidden_size, eps=1e-6):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(hidden_size))

    def forward(self, x):
        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)
        x_normalized = x / rms
        return self.weight * x_normalized

## --------------------------------------------------------------------------------
## Test the customized RMS Normalization
## --------------------------------------------------------------------------------
batch, token_num, hidden_size = 2, 16, 128
x = torch.randn(batch, token_num, hidden_size)

custom_rmsn = RMSNorm(hidden_size)
torch_rmsn = nn.RMSNorm(hidden_size)

custom_rmsn.weight.data = torch_rmsn.weight.data.clone()

print(torch.allclose(custom_rmsn(x), torch_rmsn(x)))
## --------------------------------------------------------------------------------
</code>
</pre>
</p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split" id='Group Normalizationnormalization'></p>
            <div style="border-left: 14px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('Group Normalization-Normalization-details')"><i>Group Normalization</i></p>
            <p class="paper_detail">Yuxin Wu, Kaiming He</p>
            <p class="paper_detail">Facebook AI Research (FAIR)</p>
            <p class="paper_detail"><b>Mar 22, 2018 &nbsp; <font color=#D04040>Group Normalization</font></b> &nbsp;&nbsp;|&nbsp;&nbsp; ECCV 2018 &nbsp; <font color=#D0D0D0>European Conference on Computer Vision</font></p>
            <p class="paper_detail"><a href="https://arxiv.org/pdf/1803.08494">paper</a> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/facebookresearch/Detectron/tree/main/projects/GN">code</a></p>
            
            
            <div id='Group Normalization-Normalization-details' class="info_detail">
                <hr class="dashed">
                <p><font color=#202020>
It normalizes features along <b>channel groups</b>, achieving stable accuracy even for very small batches.
</font></p>
                
                <p>
<pre>
<code class="language-python" style="font-size: 14px;">
## --------------------------------------------------------------------------------
## Build customized Group Normalization
## --------------------------------------------------------------------------------
import torch
from torch import nn 

class CustomGroupNorm(nn.Module):
    def __init__(self, num_features, num_groups, eps=1e-5):
        super().__init__()
        self.num_groups = num_groups
        self.num_features = num_features
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(num_features))
        self.bias = nn.Parameter(torch.zeros(num_features))

    def forward(self, x):
        N, C, H, W = x.shape
        x = x.view(N, self.num_groups, -1, H, W)
        mean = x.mean(dim=(3, 4), keepdim=True)
        var = x.var(dim=(3, 4), unbiased=False, keepdim=True)
        x_norm = (x - mean) / torch.sqrt(var + self.eps)
        x_norm = x_norm.view(N, C, H, W)
        return self.weight.view(1, C, 1, 1) * x_norm + self.bias.view(1, C, 1, 1)
## --------------------------------------------------------------------------------

## --------------------------------------------------------------------------------
## Test the customized Group Normalization
## --------------------------------------------------------------------------------
if __name__ == "__main__":
    N, C, H, W = 4, 6, 224, 224
    num_groups = 1
    x = torch.rand(N, C, H, W) * 10

    torch_gn = torch.nn.GroupNorm(num_channels=C, num_groups=num_groups)
    custom_gn = CustomGroupNorm(num_features=C, num_groups=num_groups)

    custom_gn.weight.data = torch_gn.weight.data.clone()
    custom_gn.bias.data = torch_gn.bias.data.clone()

    print(torch.allclose(torch_gn(x), custom_gn(x), atol=0.01))  # it prints False
    print(torch.allclose(torch_gn(x), custom_gn(x), atol=0.1))  # it prints False
## --------------------------------------------------------------------------------
</code>
</pre>
</p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split" id='Instance Normalizationnormalization'></p>
            <div style="border-left: 14px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('Instance Normalization-Normalization-details')"><i>Instance Normalization: The Missing Ingredient for Fast Stylization</i></p>
            <p class="paper_detail">Dmitry Ulyanov, Andrea Vedaldi, Victor Lempitsky</p>
            <p class="paper_detail">Skoltech & Yandex, University of Oxford</p>
            <p class="paper_detail"><b>Jul 27, 2016 &nbsp; <font color=#D04040>Instance Normalization</font></b> &nbsp;&nbsp;|&nbsp;&nbsp; arXiv 2016 &nbsp; <font color=#D0D0D0>arXiv preprint</font></p>
            <p class="paper_detail"><a href="https://arxiv.org/pdf/1607.08022">paper</a> &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/DmitryUlyanov/texture_nets">code</a></p>
            
            
            <div id='Instance Normalization-Normalization-details' class="info_detail">
                <hr class="dashed">
                <p><font color=#202020>
It normalizes samples along the <b>batch dimension and channel dimension</b> to improve visual generation quality.
</font></p>
                
                <p>
<pre>
<code class="language-python" style="font-size: 14px;">
## --------------------------------------------------------------------------------
## Build customized Instance Normalization
## --------------------------------------------------------------------------------
import torch
from torch import nn 

class CustomInstanceNorm(nn.Module):
    def __init__(self, num_features, eps=1e-5):
        super().__init__()
        self.gamma = nn.Parameter(torch.ones(num_features))
        self.beta = nn.Parameter(torch.zeros(num_features))
        self.eps = eps

    def forward(self, x):
        mean = x.mean(dim=(2, 3), keepdim=True)
        var = x.var(dim=(2, 3), unbiased=False, keepdim=True)
        x_norm = (x - mean) / torch.sqrt(var + self.eps)
        return self.gamma.view(1, -1, 1, 1) * x_norm + self.beta.view(1, -1, 1, 1)
## --------------------------------------------------------------------------------

## --------------------------------------------------------------------------------
## Test the customized Instance Normalization
## --------------------------------------------------------------------------------
C = 3
x = torch.randn(4, C, 224, 224)
torch_in = torch.nn.InstanceNorm2d(C)
custom_in = CustomInstanceNorm(C)
print(torch.allclose(torch_in(x), custom_in(x), atol=1e-5))  # it prints True
## --------------------------------------------------------------------------------
</code>
</pre>
</p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split" id='Layer Normalizationnormalization'></p>
            <div style="border-left: 14px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('Layer Normalization-Normalization-details')"><i>Layer Normalization</i></p>
            <p class="paper_detail">Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton</p>
            <p class="paper_detail">University of Toronto, Google</p>
            <p class="paper_detail"><b>Jul 21, 2016 &nbsp; <font color=#D04040>Layer Normalization</font></b> &nbsp;&nbsp;|&nbsp;&nbsp; arXiv 2016 &nbsp; <font color=#D0D0D0>arXiv preprint</font></p>
            <p class="paper_detail"><a href="https://arxiv.org/pdf/1607.06450">paper</a></p>
            <p class="paper_detail"><font color=#D04040>It normalizes across features of each sample, making it suitable for RNNs and cases with small / variable batch sizes. It has over 16,000 citaions (as of Aug 2025).</font></p>
            
            <div id='Layer Normalization-Normalization-details' class="info_detail">
                <hr class="dashed">
                <p><font color=#202020>
It normalizes samples along the <b>batch dimension</b> to adapt to cases with small / variable batch sizes.
</font></p>
                
                <p>
<pre>
<code class="language-python" style="font-size: 14px;">
## --------------------------------------------------------------------------------
## Build customized Layer Normalization
## --------------------------------------------------------------------------------
import torch
from torch import nn 

class CustomLayerNorm(nn.Module):
    def __init__(self, shape, eps=1e-5):
        super().__init__()
        self.gamma = nn.Parameter(torch.ones(shape))
        self.beta = nn.Parameter(torch.zeros(shape))
        self.eps = eps

    def forward(self, x):
        mean = x.mean(dim=(1, 2, 3), keepdim=True)
        var = x.var(dim=(1, 2, 3), unbiased=False, keepdim=True)
        x_norm = (x - mean) / torch.sqrt(var + self.eps)
        return self.gamma * x_norm + self.beta
## --------------------------------------------------------------------------------

## --------------------------------------------------------------------------------
## Test the customized Layer Normalization
## --------------------------------------------------------------------------------
C, H, W = 3, 224, 224
x = torch.randn(4, C, H, W)
## Note that LayerNorm often applies to the final dim (the feature dim)
torch_ln = torch.nn.LayerNorm(normalized_shape=(C, H, W))
custom_ln = CustomLayerNorm((C, H, W))
print(torch.allclose(custom_ln(x), torch_ln(x), atol=1e-5))  # it prints True
## --------------------------------------------------------------------------------
</code>
</pre>
</p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
            <p class="little_split" id='Batch Normalizationnormalization'></p>
            <div style="border-left: 14px solid #ADDEFF; padding-left: 10px">
            <div style="height: 0.3em;"></div>
            <p class="paper_title" onclick="toggleTable('Batch Normalization-Normalization-details')"><i>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</i></p>
            <p class="paper_detail">Sergey Ioffe, Christian Szegedy</p>
            <p class="paper_detail">Google</p>
            <p class="paper_detail"><b>Feb 11, 2015 &nbsp; <font color=#D04040>Batch Normalization</font></b> &nbsp;&nbsp;|&nbsp;&nbsp; ICML 2015 &nbsp; <font color=#D0D0D0>International Conference on Machine Learning</font></p>
            <p class="paper_detail"><a href="https://arxiv.org/pdf/1502.03167">paper</a></p>
            <p class="paper_detail"><font color=#D04040>It normalizes the activations of each layer within a batch, improving training speed, stability, and generalization. It has over 60,000 citations (as of Aug 2025).</font></p>
            
            <div id='Batch Normalization-Normalization-details' class="info_detail">
                <hr class="dashed">
                <p><font color=#202020>
It <b>normalizes layer inputs along channels</b> such that higher lr and saturating nonlinearities can be applied, careful param intialization is not needed.
</font></p>
                
                <p><ul>
<li> <b>Reason of instability.</b> The inputs to each layer are affected by the parameters of all preceding layers, so that small changes to the network amplify as the network becomes deeper. Besides, the input and output distributions of each layer changes hinder the training of the layer.
<li> <b>Previous solutions to instability.</b> non-saturating nonlinearities like ReLU, careful parameter intialization, small learning rate, dropout.
<li> <b>Batch normalization.</b> It normalizes each channel by the mean and standard error to stablize the input distribution of each layer.
<li> <b>Performance.</b> It applies to the best performing ImageNet classification network and matches its performance using only 7% of the training steps.
</ul>
<figure>
<img data-src='resource\figs\Batch Normalization\Batch Normalization-fig1.png' width=300>
<img data-src='resource\figs\Batch Normalization\Batch Normalization-fig2.png' width=300>
<figcaption>
<b>Figure 1.</b> <b>(left) Batch Normalization.</b> The \(\gamma\) and \( \beta \) are employed to make it can represent identity transformation. <b> (right) Training and inference.</b>
</figcaption>
</figure><pre>
<code class="language-python" style="font-size: 14px;">
import torch
from torch import nn

## --------------------------------------------------------------------------------
## Build customized Batch Normalization (2D)
## --------------------------------------------------------------------------------
class MyBatchNorm2d(nn.Module):
    def __init__(self, num_features, eps=1e-5, momentum=0.1):
        super().__init__()
        self.eps = eps
        self.momentum = momentum
        self.weight = torch.nn.Parameter(torch.ones(num_features))
        self.bias = torch.nn.Parameter(torch.zeros(num_features))
        self.register_buffer('running_mean', torch.zeros(num_features))
        self.register_buffer('running_var', torch.ones(num_features))

    def forward(self, x):
        if self.training:
            mean = x.mean(dim=(0, 2, 3), keepdim=True)
            var = x.var(dim=(0, 2, 3), keepdim=True, unbiased=False)
            x_hat = (x - mean) / torch.sqrt(var + self.eps)

            # Update running stats
            self.running_mean = (1 - self.momentum) * self.running_mean + \
                self.momentum * mean.view(-1)
            self.running_var = (1 - self.momentum) * self.running_var + \
                self.momentum * var.view(-1)
        else:
            mean = self.running_mean.view(1, -1, 1, 1)
            var = self.running_var.view(1, -1, 1, 1)
            x_hat = (x - mean) / torch.sqrt(var + self.eps)

        return self.weight.view(1, -1, 1, 1) * x_hat + self.bias.view(1, -1, 1, 1)
## --------------------------------------------------------------------------------

## --------------------------------------------------------------------------------
## Test the customized Batch Normalization
## --------------------------------------------------------------------------------
## Input
x = torch.randn(8, 3, 32, 32)  # BCHW

## Instantiate both modules
torch_bn = torch.nn.BatchNorm2d(3)
custom_bn = MyBatchNorm2d(3)

## Sync initial parameters
custom_bn.weight.data.copy_(torch_bn.weight.data)
custom_bn.bias.data.copy_(torch_bn.bias.data)
custom_bn.running_mean.copy_(torch_bn.running_mean)
custom_bn.running_var.copy_(torch_bn.running_var)

## Training
torch_bn.train()
custom_bn.train()
print(torch.allclose(torch_bn(x), custom_bn(x)))  # it prints True

## Inference
torch_bn.eval()
custom_bn.eval()
print(torch.allclose(torch_bn(x), custom_bn(x)))  # it prints True
## --------------------------------------------------------------------------------
</code>
</pre>
</p>
            </div>
            <div style="height: 0.05em;"></div>
            </div>
            <p class="little_split"></p>
            
    <script>
        function toggleTable(tableId) {
            const container = document.getElementById(tableId);
            const button = container.previousElementSibling;
            const isVisible = window.getComputedStyle(container).display !== 'none';
            if (!isVisible) {
                const images = container.querySelectorAll('img');
                images.forEach(img => {
                    if (img.dataset.src !== '') {img.src = img.dataset.src;}
                });
                container.style.display = 'block';
            } else {container.style.display = 'none';}
        }
    </script>
    
<button id="backToTop" title="back to top">↑</button>
<script>
    const button = document.getElementById("backToTop");
    window.addEventListener("scroll", () => {
    if (document.documentElement.scrollTop > 300) {
        button.style.display = "block";
    } else {
        button.style.display = "none";
    }
    });

    function updateButtonPosition() {
    const bodyRect = document.body.getBoundingClientRect();
    const windowWidth = window.innerWidth;
    const rightOffset = Math.max((windowWidth - bodyRect.width) / 2, 10);
    button.style.right = rightOffset + "px";
    }

    window.addEventListener("resize", updateButtonPosition);
    window.addEventListener("load", updateButtonPosition);

    button.addEventListener("click", () => {
    window.scrollTo({ top: 0, behavior: 'smooth' });
    });
</script>

</body>
</html>
