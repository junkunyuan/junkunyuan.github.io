
<!DOCTYPE html>
<html>
<head>
    <link rel="stylesheet" href="resource/index.css" type="text/css">
    <link rel="shortcut icon" href="resource/my_photo.jpg">
    <title>Junkun Yuan</title>
    <meta name="description" content="Junkun Yuan">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <div id="layout-content" style="margin-top:25px">
    <table>
        <tbody>
            <tr>
                <td width="870">
                    <h1>Junkun Yuan &nbsp; 袁俊坤</h1>
                    <p>Research Scientist, &nbsp;Hunyuan Multimodal Model Group&nbsp;&nbsp;@&nbsp;&nbsp;Tencent</p>
                    <p>yuanjk0921@outlook.com</p>
                    <p>work and live in Shenzhen, China</p>
                    <p><font color=#D0D0D0>Last updated on August 30, 2025 at 14:51 (UTC+8)</font></p>
                    <p><font color="D04040">I am currently on the job market and welcome potential opportunities. Please feel free to reach out to me.</p>
                </td>
                <td style="padding-right: 120px; padding-top: 10px;">
                    <img src="resource/my_photo.jpg" width="160">
                </td>
            </tr>
        </tbody>
    </table>
</head>
<body>

<h2>Biography</h2>
<p>
    I have been working as a research scientist in the Foundation Model Team of the Hunyuan Multimodal Model Group at Tencent since Jul 2024, working with <a href="https://scholar.google.com.hk/citations?user=igtXP_kAAAAJ&hl=zh-CN&oi=ao">Zhao Zhong</a> and <a href="https://scholar.google.com.hk/citations?user=FJwtMf0AAAAJ&hl=zh-CN&oi=ao">Liefeng Bo</a>. I am focusing on multimodal generative foundation models and their various downstream applications.
    <br><br>

    During Sep 2023 — Jul 2024, I interned in the Hunyuan Multimodal Model Group at Tencent, working with <a href="https://scholar.google.com/citations?user=AjxoEpIAAAAJ">Wei Liu</a>.
    
    During Jul 2022 — Aug 2023, I interned in the Computer Vision Group at Baidu, working with <a href="https://scholar.google.com/citations?user=PSzJxD8AAAAJ">Xinyu Zhang</a> and <a href="https://scholar.google.com/citations?user=z5SPCmgAAAAJ">Jingdong Wang</a>.<br><br>

    I received my Ph.D. degree in Computer Science from Zhejiang University (2019 — 2024), co-supervised by professors of <a href="https://scholar.google.com/citations?user=FOsNiMQAAAAJ">Kun Kuang</a>, <a href="https://person.zju.edu.cn/0096005">Lanfen Lin</a>, and 
  <a href="https://scholar.google.com/citations?user=XJLn4MYAAAAJ">Fei Wu</a>. I received my B.S. degree in Automation from Zhejiang University of Technology (2015 — 2019), co-supervised by professors of <a href="https://scholar.google.com.hk/citations?user=smi7bpoAAAAJ&hl=zh-CN&oi=ao">Qi Xuan</a> and <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=CnBn6FwAAAAJ">Li Yu</a>.<br><br>

  I have been fortunate to work closely with some friends such as <a href="https://scholar.google.com.hk/citations?user=F5P_8NkAAAAJ&hl=zh-CN&oi=ao">Defang Chen</a>, <a href="https://scholar.google.com.hk/citations?user=kwBR1ygAAAAJ&hl=zh-CN&oi=ao">Yue Ma</a>, their insights also profoundly shape my approach to research.
</p>

    <h2>Publications</h2>
    <p class="larger"><a href="https://scholar.google.com/citations?user=j3iFVPsAAAAJ">Google Scholar Profile</a></p>
    
        <p class="little_split"></p>
        <div style="border-left: 14px solid #ADDEFF; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Hunyuan-Game: Industrial-grade Intelligent Game Creation Model</i></p>
        <p class="paper_detail">Hunyuan Multimodal Model Group at Tencent (<b><font color=#404040>as a group member</font></b>)</p>
        <p class="paper_detail">May 20, 2025 &nbsp;&nbsp;|&nbsp;&nbsp; <b><font color=#404040>arXiv 2025</font></b> &nbsp; <font color=#D0D0D0>arXiv preprint</font></p>
        <p class="paper_detail"><a href="https://arxiv.org/pdf/2505.14135">paper</a>&nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/Tencent-Hunyuan/Hunyuan-GameCraft-1.0">code</a></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div style="border-left: 14px solid #FFD6AD; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>HunyuanVideo: A Systematic Framework For Large Video Generative Models</i></p>
        <p class="paper_detail">Hunyuan Multimodal Model Group at Tencent (<b><font color=#404040>as a group member</font></b>)</p>
        <p class="paper_detail">Dec 03, 2024 &nbsp;&nbsp;|&nbsp;&nbsp; <b><font color=#404040>arXiv 2024</font></b> &nbsp; <font color=#D0D0D0>arXiv preprint</font></p>
        <p class="paper_detail"><a href="https://arxiv.org/pdf/2412.03603">paper</a>&nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/Tencent-Hunyuan/HunyuanVideo">code</a></p>
        <p class="paper_detail"><font color=#D04040>It is an open-source large-scale video generation model with 13B parameters. It has over 300 citations and over 10,000 GitHub stars (as of Aug 2025).</font></p>
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div style="border-left: 14px solid #FFD6AD; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Follow-Your-Canvas: Higher-Resolution Video Outpainting with Extensive Content Generation</i></p>
        <p class="paper_detail">Qihua Chen<sup>&#10035</sup>, Yue Ma<sup>&#10035</sup>, Hongfa Wang<sup>&#10035</sup>, <b><font color=#404040>Junkun Yuan</font></b><sup>&#10035</sup><sup>&#9993</sup>, Wenzhe Zhao, Qi Tian, Hongmei Wang, Shaobo Min, Qifeng Chen<sup>&#9993</sup>, and Wei Liu</p>
        <p class="paper_detail">Sep 02, 2024 &nbsp;&nbsp;|&nbsp;&nbsp; <b><font color=#404040>AAAI 2025</font></b> &nbsp; <font color=#D0D0D0>AAAI Conference on Artificial Intelligence</font></p>
        <p class="paper_detail"><a href="https://arxiv.org/pdf/2409.01055">paper</a>&nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/mayuelala/FollowYourCanvas">code</a></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div style="border-left: 14px solid #FFD6AD; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Follow-Your-Emoji: Fine-Controllable and Expressive Freestyle Portrait Animation</i></p>
        <p class="paper_detail">Yue Ma<sup>&#10035</sup>, Hongyu Liu<sup>&#10035</sup>, Hongfa Wang<sup>&#10035</sup>, Heng Pan<sup>&#10035</sup>, Yingqing He, <b><font color=#404040>Junkun Yuan</font></b>, Ailing Zeng, Chengfei Cai, Heung-Yeung Shum, Wei Liu<sup>&#9993</sup>, and Qifeng Chen<sup>&#9993</sup></p>
        <p class="paper_detail">Jun 04, 2024 &nbsp;&nbsp;|&nbsp;&nbsp; <b><font color=#404040>SIGGRAPH-Asia 2024</font></b> &nbsp; <font color=#D0D0D0>ACM SIGGRAPH Annual Conference in Asia</font></p>
        <p class="paper_detail"><a href="https://arxiv.org/pdf/2406.01900">paper</a>&nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/mayuelala/FollowYourEmoji">code</a></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div style="border-left: 14px solid #FFD6AD; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Domaindiff: Boost Out-of-Distribution Generalization with Synthetic Data</i></p>
        <p class="paper_detail">Qiaowei Miao, <b><font color=#404040>Junkun Yuan</font></b>, Shengyu Zhang, Fei Wu, and Kun Kuang<sup>&#9993</sup></p>
        <p class="paper_detail">Apr 14, 2024 &nbsp;&nbsp;|&nbsp;&nbsp; <b><font color=#404040>ICASSP 2024</font></b> &nbsp; <font color=#D0D0D0>International Conference on Acoustics, Speech and Signal Processing</font></p>
        <p class="paper_detail"><a href="https://ieeexplore.ieee.org/iel7/10445798/10445803/10446788.pdf">paper</a></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div style="border-left: 14px solid #FFD6AD; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Knowledge Distillation-Based Domain-Invariant Representation Learning for Domain Generalization</i></p>
        <p class="paper_detail">Ziwei Niu, <b><font color=#404040>Junkun Yuan</font></b>, Xu Ma, Yingying Xu, Jing Liu, Yen-Wei Chen, Ruofeng Tong, and Lanfen Lin<sup>&#9993</sup></p>
        <p class="paper_detail">Jan 01, 2024 &nbsp;&nbsp;|&nbsp;&nbsp; <b><font color=#404040>TMM 2023</font></b> &nbsp; <font color=#D0D0D0>IEEE Transactions on Multimedia</font></p>
        <p class="paper_detail"><a href="https://dl.acm.org/doi/10.1109/TMM.2023.3263549">paper</a></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div style="border-left: 14px solid #B2EEC8; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>HAP: Structure-Aware Masked Image Modeling for Human-Centric Perception</i></p>
        <p class="paper_detail"><b><font color=#404040>Junkun Yuan</font></b><sup>&#10035</sup>, Xinyu Zhang<sup>&#10035</sup><sup>&#9993</sup>, Hao Zhou, Jian Wang, Zhongwei Qiu, Zhiyin Shao, Shaofeng Zhang, Sifan Long, and Kun Kuang<sup>&#9993</sup>, Kun Yao, Junyu Han, Errui Ding, Lanfen Lin, Fei Wu, and Jingdong Wang<sup>&#9993</sup></p>
        <p class="paper_detail">Oct 31, 2023 &nbsp;&nbsp;|&nbsp;&nbsp; <b><font color=#404040>NeurIPS 2023</font></b> &nbsp; <font color=#D0D0D0>Advances in Neural Information Processing Systems</font></p>
        <p class="paper_detail"><a href="https://arxiv.org/pdf/2310.20695">paper</a>&nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/junkunyuan/HAP">code</a></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div style="border-left: 14px solid #B2EEC8; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>MAP: Towards Balanced Generalization of IID and OOD through Model-Agnostic Adapters</i></p>
        <p class="paper_detail">Min Zhang, <b><font color=#404040>Junkun Yuan</font></b>, Yue He, Wenbin Li, Zhengyu Chen, and Kun Kuang<sup>&#9993</sup></p>
        <p class="paper_detail">Oct 02, 2023 &nbsp;&nbsp;|&nbsp;&nbsp; <b><font color=#404040>ICCV 2023</font></b> &nbsp; <font color=#D0D0D0>International Conference on Computer Vision</font></p>
        <p class="paper_detail"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_MAP_Towards_Balanced_Generalization_of_IID_and_OOD_through_Model-Agnostic_ICCV_2023_paper.pdf">paper</a>&nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/remiMZ/MAP-ICCV23">code</a></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div style="border-left: 14px solid #B2EEC8; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Neural Collapse Anchored Prompt Tuning for Generalizable Vision-Language Models</i></p>
        <p class="paper_detail">Didi Zhu, Zexi Li, Min Zhang, <b><font color=#404040>Junkun Yuan</font></b>, Yunfeng Shao, Jiashuo Liu, Kun Kuang<sup>&#9993</sup>, Yinchuan Li, and Chao Wu</p>
        <p class="paper_detail">Jun 28, 2023 &nbsp;&nbsp;|&nbsp;&nbsp; <b><font color=#404040>KDD 2024</font></b> &nbsp; <font color=#D0D0D0>ACM SIGKDD Conference on Knowledge Discovery and Data Mining</font></p>
        <p class="paper_detail"><a href="https://arxiv.org/pdf/2306.15955">paper</a></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div style="border-left: 14px solid #B2EEC8; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Quantitatively Measuring and Contrastively Exploring Heterogeneity for Domain Generalization</i></p>
        <p class="paper_detail">Yunze Tong, <b><font color=#404040>Junkun Yuan</font></b>, Min Zhang, Didi Zhu, Keli Zhang, Fei Wu, and Kun Kuang<sup>&#9993</sup></p>
        <p class="paper_detail">May 25, 2023 &nbsp;&nbsp;|&nbsp;&nbsp; <b><font color=#404040>KDD 2023</font></b> &nbsp; <font color=#D0D0D0>ACM SIGKDD Conference on Knowledge Discovery and Data Mining</font></p>
        <p class="paper_detail"><a href="https://arxiv.org/pdf/2305.15889">paper</a>&nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/YunzeTong/HTCL">code</a></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div style="border-left: 14px solid #B2EEC8; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Universal Domain Adaptation via Compressive Attention Matching</i></p>
        <p class="paper_detail">Didi Zhu<sup>&#10035</sup>, Yincuan Li<sup>&#10035</sup>, <b><font color=#404040>Junkun Yuan</font></b>, Zexi Li, Kun Kuang, and Chao Wu<sup>&#9993</sup></p>
        <p class="paper_detail">Apr 24, 2023 &nbsp;&nbsp;|&nbsp;&nbsp; <b><font color=#404040>ICCV 2023</font></b> &nbsp; <font color=#D0D0D0>International Conference on Computer Vision</font></p>
        <p class="paper_detail"><a href="https://arxiv.org/pdf/2304.11862">paper</a></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div style="border-left: 14px solid #B2EEC8; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Mutual Prompt Learning for Vision Language Models</i></p>
        <p class="paper_detail">Sifan Long<sup>&#10035</sup>, Zhen Zhao<sup>&#10035</sup>, <b><font color=#404040>Junkun Yuan</font></b><sup>&#10035</sup>, Zichang Tan<sup>&#10035</sup>, Jiangjiang Liu, Jingyuan Feng, Shengsheng Wang<sup>&#9993</sup>, and Jingdong Wang</p>
        <p class="paper_detail">Mar 30, 2023 &nbsp;&nbsp;|&nbsp;&nbsp; <b><font color=#404040>IJCV 2024</font></b> &nbsp; <font color=#D0D0D0>International Journal of Computer Vision</font></p>
        <p class="paper_detail"><a href="https://arxiv.org/pdf/2303.17169">paper</a></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div style="border-left: 14px solid #B2EEC8; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Task-Oriented Multi-Modal Mutual Leaning for Vision-Language Models</i></p>
        <p class="paper_detail">Sifan Long<sup>&#10035</sup>, Zhen Zhao<sup>&#10035</sup>, <b><font color=#404040>Junkun Yuan</font></b><sup>&#10035</sup>, Zichang Tan, Jiangjiang Liu, Luping Zhou, Shengsheng Wang<sup>&#9993</sup>, and Jingdong Wang<sup>&#9993</sup></p>
        <p class="paper_detail">Mar 30, 2023 &nbsp;&nbsp;|&nbsp;&nbsp; <b><font color=#404040>ICCV 2023</font></b> &nbsp; <font color=#D0D0D0>International Conference on Computer Vision</font></p>
        <p class="paper_detail"><a href="https://arxiv.org/pdf/2303.17169">paper</a></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div style="border-left: 14px solid #FFBBCC; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>CAE v2: Context Autoencoder with CLIP Target</i></p>
        <p class="paper_detail">Xinyu Zhang<sup>&#10035</sup>, Jiahui Chen<sup>&#10035</sup>, <b><font color=#404040>Junkun Yuan</font></b>, Qiang Chen, Jian Wang, Xiaodi Wang, Shumin Han, Xiaokang Chen, Jimin Pi, Kun Yao, Junyu Han, Errui Ding, and Jingdong Wang<sup>&#9993</sup></p>
        <p class="paper_detail">Nov 17, 2022 &nbsp;&nbsp;|&nbsp;&nbsp; <b><font color=#404040>TMLR 2023</font></b> &nbsp; <font color=#D0D0D0>Transactions on Machine Learning Research</font></p>
        <p class="paper_detail"><a href="https://arxiv.org/pdf/2211.09799">paper</a>&nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/Atten4Vis/CAE">code</a></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div style="border-left: 14px solid #FFBBCC; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Label-Efficient Domain Generalization via Collaborative Exploration and Generalization</i></p>
        <p class="paper_detail"><b><font color=#404040>Junkun Yuan</font></b><sup>&#10035</sup>, Xu Ma<sup>&#10035</sup>, Defang Chen, Kun Kuang<sup>&#9993</sup>, Fei Wu, and Lanfen Lin</p>
        <p class="paper_detail">Aug 07, 2022 &nbsp;&nbsp;|&nbsp;&nbsp; <b><font color=#404040>MM 2022</font></b> &nbsp; <font color=#D0D0D0>International Conference on Multimedia</font></p>
        <p class="paper_detail"><a href="https://arxiv.org/pdf/2208.03644">paper</a>&nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/junkunyuan/CEG">code</a></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div style="border-left: 14px solid #FFBBCC; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Attention-based Cross-Layer Domain Alignment for Unsupervised Domain Adaptation</i></p>
        <p class="paper_detail">Xu Ma, <b><font color=#404040>Junkun Yuan</font></b>, Yen-Wei Chen, Ruofeng Tong, and Lanfen Lin<sup>&#9993</sup></p>
        <p class="paper_detail">Feb 27, 2022 &nbsp;&nbsp;|&nbsp;&nbsp; <b><font color=#404040>Neurocomputing 2022</font></b> &nbsp; <font color=#D0D0D0></font></p>
        <p class="paper_detail"><a href="https://arxiv.org/pdf/2202.13310">paper</a></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div style="border-left: 14px solid #ADDEFF; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Domain-Specific Bias Filtering for Single Labeled Domain Generalization</i></p>
        <p class="paper_detail"><b><font color=#404040>Junkun Yuan</font></b><sup>&#10035</sup>, Xu Ma<sup>&#10035</sup>, Defang Chen, Kun Kuang<sup>&#9993</sup>, Fei Wu, and Lanfen Lin</p>
        <p class="paper_detail">Oct 02, 2021 &nbsp;&nbsp;|&nbsp;&nbsp; <b><font color=#404040>IJCV 2022</font></b> &nbsp; <font color=#D0D0D0>International Journal of Computer Vision</font></p>
        <p class="paper_detail"><a href="https://arxiv.org/pdf/2110.00726">paper</a>&nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/junkunyuan/DSBF">code</a></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div style="border-left: 14px solid #ADDEFF; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Collaborative Semantic Aggregation and Calibration for Federated Domain Generalization</i></p>
        <p class="paper_detail"><b><font color=#404040>Junkun Yuan</font></b><sup>&#10035</sup>, Xu Ma<sup>&#10035</sup>, Defang Chen, Fei Wu, Lanfen Lin, and Kun Kuang<sup>&#9993</sup></p>
        <p class="paper_detail">Oct 13, 2021 &nbsp;&nbsp;|&nbsp;&nbsp; <b><font color=#404040>TKDE 2023</font></b> &nbsp; <font color=#D0D0D0>IEEE Transactions on Knowledge and Data Engineering</font></p>
        <p class="paper_detail"><a href="https://arxiv.org/pdf/2110.06736">paper</a>&nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/junkunyuan/CSAC">code</a></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div style="border-left: 14px solid #ADDEFF; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Instrumental Variable-Driven Domain Generalization with Unobserved Confounders</i></p>
        <p class="paper_detail"><b><font color=#404040>Junkun Yuan</font></b>, Xu Ma, Kun Kuang<sup>&#9993</sup>, Ruoxuan Xiong, Mingming Gong, and Lanfen Lin</p>
        <p class="paper_detail">Oct 04, 2021 &nbsp;&nbsp;|&nbsp;&nbsp; <b><font color=#404040>TKDD 2023</font></b> &nbsp; <font color=#D0D0D0>ACM Transactions on Knowledge Discovery from Data</font></p>
        <p class="paper_detail"><a href="https://arxiv.org/pdf/2110.01438">paper</a>&nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/junkunyuan/DSBF">code</a></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div style="border-left: 14px solid #ADDEFF; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Auto IV: Counterfactual Prediction via Automatic Instrumental Variable Decomposition</i></p>
        <p class="paper_detail"><b><font color=#404040>Junkun Yuan</font></b><sup>&#10035</sup>, Anpeng Wu<sup>&#10035</sup>, Kun Kuang<sup>&#9993</sup>, Bo Li, Runze Wu, Fei Wu, and Lanfen Lin</p>
        <p class="paper_detail">Jul 13, 2021 &nbsp;&nbsp;|&nbsp;&nbsp; <b><font color=#404040>TKDD 2022</font></b> &nbsp; <font color=#D0D0D0>ACM Transactions on Knowledge Discovery from Data</font></p>
        <p class="paper_detail"><a href="https://arxiv.org/pdf/2107.05884">paper</a>&nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/junkunyuan/AutoIV">code</a></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div style="border-left: 14px solid #FFD6AD; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Black-Box Adversarial Attacks Against Deep Learning Based Malware Binaries Detection with GAN</i></p>
        <p class="paper_detail"><b><font color=#404040>Junkun Yuan</font></b>, Shaofang Zhou, Lanfen Lin<sup>&#9993</sup>, Feng Wang, and Jia Cui</p>
        <p class="paper_detail">Aug 29, 2020 &nbsp;&nbsp;|&nbsp;&nbsp; <b><font color=#404040>ECAI 2020</font></b> &nbsp; <font color=#D0D0D0>European Conference on Artificial Intelligence</font></p>
        <p class="paper_detail"><a href="https://ebooks.iospress.nl/pdf/doi/10.3233/FAIA200388">paper</a></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div style="border-left: 14px solid #FFD6AD; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Learning Decomposed Representation for Counterfactual Inference</i></p>
        <p class="paper_detail">Anpeng Wu<sup>&#10035</sup>, <b><font color=#404040>Junkun Yuan</font></b><sup>&#10035</sup>, Kun Kuang<sup>&#9993</sup>, Bo Li<sup>&#9993</sup>, Runze Wu, Qiang Zhu, Yueting Zhuang, and Fei Wu</p>
        <p class="paper_detail">Jun 12, 2020 &nbsp;&nbsp;|&nbsp;&nbsp; <b><font color=#404040>TKDE 2023</font></b> &nbsp; <font color=#D0D0D0>IEEE Transactions on Knowledge and Data Engineering</font></p>
        <p class="paper_detail"><a href="https://arxiv.org/pdf/2006.07040">paper</a>&nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/anpwu/DeR-CFR">code</a></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div style="border-left: 14px solid #B2EEC8; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Subgraph Networks with Application to Structural Feature Space Expansion</i></p>
        <p class="paper_detail">Qi Xuan<sup>&#9993</sup>, Jinhuan Wang, Minghao Zhao, <b><font color=#404040>Junkun Yuan</font></b>, Chenbo Fu, Zhongyuan Ruan<sup>&#9993</sup>, Guanrong Chen</p>
        <p class="paper_detail">Mar 21, 2019 &nbsp;&nbsp;|&nbsp;&nbsp; <b><font color=#404040>TKDE 2021</font></b> &nbsp; <font color=#D0D0D0>IEEE Transactions on Knowledge and Data Engineering</font></p>
        <p class="paper_detail"><a href="https://arxiv.org/pdf/1903.09022">paper</a>&nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/GalateaWang/SGNs-master">code</a></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
<br>
<h2>Professional Service</h2>
<ul>
<li> <b>Conference Reviewer.</b>&nbsp;&nbsp; 
CVPR 2021 &nbsp;&nbsp;<font color=#A0A0A0>|</font>&nbsp;&nbsp;
ICCV 2023 &nbsp;&nbsp;<font color=#A0A0A0>|</font>&nbsp;&nbsp;
AAAI 2023, 2026 &nbsp;&nbsp;<font color=#A0A0A0>|</font>&nbsp;&nbsp;
MM 2023

<li> <b>Journal Reviewer.</b>&nbsp;&nbsp; 
TNNLS 2022 &nbsp;&nbsp;<font color=#A0A0A0>|</font>&nbsp;&nbsp;
TCSVT 2023 &nbsp;&nbsp;<font color=#A0A0A0>|</font>&nbsp;&nbsp;
PR 2026 &nbsp;&nbsp;<font color=#A0A0A0>|</font>&nbsp;&nbsp;
NN 2023 &nbsp;&nbsp;<font color=#A0A0A0>|</font>&nbsp;&nbsp;
TKDD 2023
</ul>

<button id="backToTop" title="back to top">↑</button>
<script>
    const button = document.getElementById("backToTop");
    window.addEventListener("scroll", () => {
        if (document.documentElement.scrollTop > 300) {
            button.style.display = "block";
        } else {
            button.style.display = "none";
        }
    });

    function updateButtonPosition() {
        const bodyRect = document.body.getBoundingClientRect();
        const windowWidth = window.innerWidth;
        const rightOffset = Math.max((windowWidth - bodyRect.width) / 2, 10);
        button.style.right = rightOffset + "px";
    }

    window.addEventListener("resize", updateButtonPosition);
    window.addEventListener("load", updateButtonPosition);

    button.addEventListener("click", () => {
        window.scrollTo({ top: 0, behavior: 'smooth' });
    });
</script>

</body>
</html>
