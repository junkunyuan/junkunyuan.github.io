
<!DOCTYPE html>
<html>
<head>
    <link rel="stylesheet" href="resource/index.css" type="text/css">
    <link rel="shortcut icon" href="resource/my_photo.jpg">
    <title>Junkun Yuan</title>
    <meta name="description" content="Junkun Yuan">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <div id="layout-content" style="margin-top:25px">
    <table>
        <tbody>
            <tr>
                <td width="870">
                    <h1>Junkun Yuan &nbsp; 袁俊坤</h1>
                    <p>Research Scientist, &nbsp;Hunyuan Multimodal Model Group&nbsp;&nbsp;@&nbsp;&nbsp;Tencent</p>
                    <p>yuanjk0921@outlook.com</p>
                    <p>work and live in Shenzhen, China</p>
                    <p><font color=#D0D0D0>Last updated on September 29, 2025 at 14:10 (UTC+8)</font></p>
                </td>
                <td style="padding-right: 120px; padding-top: 10px;">
                    <img src="resource/my_photo.jpg" width="160">
                </td>
            </tr>
        </tbody>
    </table>
</head>
<body>

<h2>Biography</h2>
<p>
    I have been working as a research scientist in the Foundation Model Team of the Hunyuan Multimodal Model Group at Tencent since Jul 2024, working with <a href="https://scholar.google.com.hk/citations?user=igtXP_kAAAAJ&hl=zh-CN&oi=ao">Zhao Zhong</a> and <a href="https://scholar.google.com.hk/citations?user=FJwtMf0AAAAJ&hl=zh-CN&oi=ao">Liefeng Bo</a>. I am focusing on multimodal generative foundation models and their various downstream applications.
    <br><br>

    During Sep 2023 — Jul 2024, I interned in the Hunyuan Multimodal Model Group at Tencent, working with <a href="https://scholar.google.com/citations?user=AjxoEpIAAAAJ">Wei Liu</a>.
    
    During Jul 2022 — Aug 2023, I interned in the Computer Vision Group at Baidu, working with <a href="https://scholar.google.com/citations?user=PSzJxD8AAAAJ">Xinyu Zhang</a> and <a href="https://scholar.google.com/citations?user=z5SPCmgAAAAJ">Jingdong Wang</a>.<br><br>

    I received my Ph.D. degree in Computer Science from Zhejiang University (2019 — 2024), co-supervised by professors of <a href="https://scholar.google.com/citations?user=FOsNiMQAAAAJ">Kun Kuang</a>, <a href="https://person.zju.edu.cn/0096005">Lanfen Lin</a>, and 
  <a href="https://scholar.google.com/citations?user=XJLn4MYAAAAJ">Fei Wu</a>. I received my B.S. degree in Automation from Zhejiang University of Technology (2015 — 2019), co-supervised by professors of <a href="https://scholar.google.com.hk/citations?user=smi7bpoAAAAJ&hl=zh-CN&oi=ao">Qi Xuan</a> and <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=CnBn6FwAAAAJ">Li Yu</a>.<br><br>

  I have been fortunate to work closely with some friends such as <a href="https://scholar.google.com.hk/citations?user=F5P_8NkAAAAJ&hl=zh-CN&oi=ao">Defang Chen</a> and <a href="https://scholar.google.com.hk/citations?user=kwBR1ygAAAAJ&hl=zh-CN&oi=ao">Yue Ma</a>, their insights also profoundly shape my approach to research.
</p>

    <h2>Publications</h2>
    <p class="larger"><a href="https://scholar.google.com/citations?user=j3iFVPsAAAAJ">Google Scholar Profile</a></p>
    
    <p style="display: flex; flex-wrap: wrap; font-size: 13px;">
        <a href="#Follow-Your-Emoji-Fasterpublications" class="no_dec"><font color=#777777><b>Follow-Your-Emoji-Faster</b> <font style="color:#AAAAAA;font-size:11px;">(IJCV 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#Hunyuan-Gamepublications" class="no_dec"><font color=#777777><b>Hunyuan-Game</b> <font style="color:#AAAAAA;font-size:11px;">(arXiv 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#HunyuanVideopublications" class="no_dec"><font color=#C55253><b>HunyuanVideo</b> <font style="color:#AAAAAA;font-size:11px;">(arXiv 2024)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#MPLpublications" class="no_dec"><font color=#777777><b>MPL</b> <font style="color:#AAAAAA;font-size:11px;">(IJCV 2024)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#Follow-Your-Canvaspublications" class="no_dec"><font color=#777777><b>Follow-Your-Canvas</b> <font style="color:#AAAAAA;font-size:11px;">(AAAI 2025)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#Follow-Your-Emojipublications" class="no_dec"><font color=#777777><b>Follow-Your-Emoji</b> <font style="color:#AAAAAA;font-size:11px;">(SIGGRAPH-Asia 2024)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#Domaindiffpublications" class="no_dec"><font color=#777777><b>Domaindiff</b> <font style="color:#AAAAAA;font-size:11px;">(ICASSP 2024)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#KDDRLpublications" class="no_dec"><font color=#777777><b>KDDRL</b> <font style="color:#AAAAAA;font-size:11px;">(TMM 2023)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#HAPpublications" class="no_dec"><font color=#777777><b>HAP</b> <font style="color:#AAAAAA;font-size:11px;">(NeurIPS 2023)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#MAPpublications" class="no_dec"><font color=#777777><b>MAP</b> <font style="color:#AAAAAA;font-size:11px;">(ICCV 2023)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#NPTpublications" class="no_dec"><font color=#777777><b>NPT</b> <font style="color:#AAAAAA;font-size:11px;">(KDD 2024)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#HTCLpublications" class="no_dec"><font color=#777777><b>HTCL</b> <font style="color:#AAAAAA;font-size:11px;">(KDD 2023)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#CAMpublications" class="no_dec"><font color=#777777><b>CAM</b> <font style="color:#AAAAAA;font-size:11px;">(ICCV 2023)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#MPLpublications" class="no_dec"><font color=#777777><b>MPL</b> <font style="color:#AAAAAA;font-size:11px;">(ICCV 2023)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#CAE v2publications" class="no_dec"><font color=#777777><b>CAE v2</b> <font style="color:#AAAAAA;font-size:11px;">(TMLR 2023)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#CEGpublications" class="no_dec"><font color=#777777><b>CEG</b> <font style="color:#AAAAAA;font-size:11px;">(MM 2022)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#ACDApublications" class="no_dec"><font color=#777777><b>ACDA</b> <font style="color:#AAAAAA;font-size:11px;">(Neurocomputing 2022)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#DSBFpublications" class="no_dec"><font color=#777777><b>DSBF</b> <font style="color:#AAAAAA;font-size:11px;">(IJCV 2022)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#CSACpublications" class="no_dec"><font color=#777777><b>CSAC</b> <font style="color:#AAAAAA;font-size:11px;">(TKDE 2023)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#DSBFpublications" class="no_dec"><font color=#777777><b>DSBF</b> <font style="color:#AAAAAA;font-size:11px;">(TKDD 2023)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#AutoIVpublications" class="no_dec"><font color=#777777><b>AutoIV</b> <font style="color:#AAAAAA;font-size:11px;">(TKDD 2022)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#GAPGANpublications" class="no_dec"><font color=#777777><b>GAPGAN</b> <font style="color:#AAAAAA;font-size:11px;">(ECAI 2020)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#DeR-CFRpublications" class="no_dec"><font color=#777777><b>DeR-CFR</b> <font style="color:#AAAAAA;font-size:11px;">(TKDE 2023)</font></font></a> &nbsp; &nbsp; &nbsp; <a href="#SGNspublications" class="no_dec"><font color=#777777><b>SGNs</b> <font style="color:#AAAAAA;font-size:11px;">(TKDE 2021)</font></font></a>
    </p>
    
        <p class="little_split"></p>
        <div id="Follow-Your-Emoji-Fasterpublications" style="border-left: 16px solid #9ACAF0; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation</i></p>
        <p class="paper_detail">Yue Ma<sup>&#10035</sup>, Zexuan Yan<sup>&#10035</sup>, Hongyu Liu<sup>&#10035</sup>, Hongfa Wang, Heng Pan, Yingqing He, <b><font color=#404040>Junkun Yuan</font></b>, Ailing Zeng, Chengfei Cai, Heung-Yeung Shum, Zhifeng Li, Wei Liu, Linfeng Zhang<sup>&#9993</sup>, Qifeng Chen<sup>&#9993</sup></p>
        <p class="paper_detail">International Journal of Computer Vision (<b><font color=#404040>IJCV</font></b>), <b>2025</b></p>
        <p class="paper_detail"><b><font color=#404040>Sep 20, 2025</font></b> &nbsp;&nbsp;<font color=#CCCCCC>|</font>&nbsp;&nbsp; <b><font color=#404040>Follow-Your-Emoji-Faster</font></b></p>
        <p class="paper_detail"><a href="https://arxiv.org/pdf/2509.16630">paper</a> &nbsp;&nbsp;<font color=#CCCCCC>|</font>&nbsp;&nbsp; <a href="https://github.com/mayuelala/FollowYourEmoji">code</a></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div id="Hunyuan-Gamepublications" style="border-left: 16px solid #9ACAF0; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Hunyuan-Game: Industrial-grade Intelligent Game Creation Model</i></p>
        <p class="paper_detail">Hunyuan Multimodal Model Group at Tencent (<b><font color=#404040>as a group member</font></b>)</p>
        <p class="paper_detail"> <b><font color=#404040>arXiv</font></b> <b>2025</b></p>
        <p class="paper_detail"><b><font color=#404040>May 20, 2025</font></b> &nbsp;&nbsp;<font color=#CCCCCC>|</font>&nbsp;&nbsp; <b><font color=#404040>Hunyuan-Game</font></b></p>
        <p class="paper_detail"><a href="https://arxiv.org/pdf/2505.14135">paper</a> &nbsp;&nbsp;<font color=#CCCCCC>|</font>&nbsp;&nbsp; <a href="https://github.com/Tencent-Hunyuan/Hunyuan-GameCraft-1.0">code</a></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div id="HunyuanVideopublications" style="border-left: 16px solid #F9D0A5; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>HunyuanVideo: A Systematic Framework For Large Video Generative Models</i></p>
        <p class="paper_detail">Hunyuan Multimodal Model Group at Tencent (<b><font color=#404040>as a group member</font></b>)</p>
        <p class="paper_detail"> <b><font color=#404040>arXiv</font></b> <b>2024</b></p>
        <p class="paper_detail"><b><font color=#404040>Dec 03, 2024</font></b> &nbsp;&nbsp;<font color=#CCCCCC>|</font>&nbsp;&nbsp; <b><font color=#C55253>HunyuanVideo</font></b></p>
        <p class="paper_detail"><a href="https://arxiv.org/pdf/2412.03603">paper</a> &nbsp;&nbsp;<font color=#CCCCCC>|</font>&nbsp;&nbsp; <a href="https://github.com/Tencent-Hunyuan/HunyuanVideo">code</a></p>
        <p class="paper_detail"><font color=#C55253>It is an open-source large-scale video generation model with 13B parameters. It has over 300 citations and over 11,000 GitHub stars (as of Sep 2025).</font></p>
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div id="MPLpublications" style="border-left: 16px solid #F9D0A5; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Mutual Prompt Learning for Vision Language Models</i></p>
        <p class="paper_detail">Sifan Long<sup>&#10035</sup>, Zhen Zhao<sup>&#10035</sup>, <b><font color=#404040>Junkun Yuan</font></b><sup>&#10035</sup>, Zichang Tan<sup>&#10035</sup>, Jiangjiang Liu, Jingyuan Feng, Shengsheng Wang<sup>&#9993</sup>, and Jingdong Wang</p>
        <p class="paper_detail">International Journal of Computer Vision (<b><font color=#404040>IJCV</font></b>), <b>2024</b></p>
        <p class="paper_detail"><b><font color=#404040>Sep 26, 2024</font></b> &nbsp;&nbsp;<font color=#CCCCCC>|</font>&nbsp;&nbsp; <b><font color=#404040>MPL</font></b></p>
        <p class="paper_detail"><a href="https://arxiv.org/pdf/2303.17169">paper</a></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div id="Follow-Your-Canvaspublications" style="border-left: 16px solid #F9D0A5; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Follow-Your-Canvas: Higher-Resolution Video Outpainting with Extensive Content Generation</i></p>
        <p class="paper_detail">Qihua Chen<sup>&#10035</sup>, Yue Ma<sup>&#10035</sup>, Hongfa Wang<sup>&#10035</sup>, <b><font color=#404040>Junkun Yuan</font></b><sup>&#10035</sup><sup>&#9993</sup>, Wenzhe Zhao, Qi Tian, Hongmei Wang, Shaobo Min, Qifeng Chen<sup>&#9993</sup>, and Wei Liu</p>
        <p class="paper_detail">AAAI Conference on Artificial Intelligence (<b><font color=#404040>AAAI</font></b>), <b>2025</b></p>
        <p class="paper_detail"><b><font color=#404040>Sep 02, 2024</font></b> &nbsp;&nbsp;<font color=#CCCCCC>|</font>&nbsp;&nbsp; <b><font color=#404040>Follow-Your-Canvas</font></b></p>
        <p class="paper_detail"><a href="https://arxiv.org/pdf/2409.01055">paper</a> &nbsp;&nbsp;<font color=#CCCCCC>|</font>&nbsp;&nbsp; <a href="https://github.com/mayuelala/FollowYourCanvas">code</a></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div id="Follow-Your-Emojipublications" style="border-left: 16px solid #F9D0A5; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Follow-Your-Emoji: Fine-Controllable and Expressive Freestyle Portrait Animation</i></p>
        <p class="paper_detail">Yue Ma<sup>&#10035</sup>, Hongyu Liu<sup>&#10035</sup>, Hongfa Wang<sup>&#10035</sup>, Heng Pan<sup>&#10035</sup>, Yingqing He, <b><font color=#404040>Junkun Yuan</font></b>, Ailing Zeng, Chengfei Cai, Heung-Yeung Shum, Wei Liu<sup>&#9993</sup>, and Qifeng Chen<sup>&#9993</sup></p>
        <p class="paper_detail">ACM SIGGRAPH Annual Conference in Asia (<b><font color=#404040>SIGGRAPH-Asia</font></b>), <b>2024</b></p>
        <p class="paper_detail"><b><font color=#404040>Jun 04, 2024</font></b> &nbsp;&nbsp;<font color=#CCCCCC>|</font>&nbsp;&nbsp; <b><font color=#404040>Follow-Your-Emoji</font></b></p>
        <p class="paper_detail"><a href="https://arxiv.org/pdf/2406.01900">paper</a> &nbsp;&nbsp;<font color=#CCCCCC>|</font>&nbsp;&nbsp; <a href="https://github.com/mayuelala/FollowYourEmoji">code</a></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div id="Domaindiffpublications" style="border-left: 16px solid #F9D0A5; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Domaindiff: Boost Out-of-Distribution Generalization with Synthetic Data</i></p>
        <p class="paper_detail">Qiaowei Miao, <b><font color=#404040>Junkun Yuan</font></b>, Shengyu Zhang, Fei Wu, and Kun Kuang<sup>&#9993</sup></p>
        <p class="paper_detail">International Conference on Acoustics, Speech and Signal Processing (<b><font color=#404040>ICASSP</font></b>), <b>2024</b></p>
        <p class="paper_detail"><b><font color=#404040>Apr 14, 2024</font></b> &nbsp;&nbsp;<font color=#CCCCCC>|</font>&nbsp;&nbsp; <b><font color=#404040>Domaindiff</font></b></p>
        <p class="paper_detail"><a href="https://ieeexplore.ieee.org/iel7/10445798/10445803/10446788.pdf">paper</a></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div id="KDDRLpublications" style="border-left: 16px solid #F9D0A5; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Knowledge Distillation-Based Domain-Invariant Representation Learning for Domain Generalization</i></p>
        <p class="paper_detail">Ziwei Niu, <b><font color=#404040>Junkun Yuan</font></b>, Xu Ma, Yingying Xu, Jing Liu, Yen-Wei Chen, Ruofeng Tong, and Lanfen Lin<sup>&#9993</sup></p>
        <p class="paper_detail">IEEE Transactions on Multimedia (<b><font color=#404040>TMM</font></b>), <b>2023</b></p>
        <p class="paper_detail"><b><font color=#404040>Jan 01, 2024</font></b> &nbsp;&nbsp;<font color=#CCCCCC>|</font>&nbsp;&nbsp; <b><font color=#404040>KDDRL</font></b></p>
        <p class="paper_detail"><a href="https://dl.acm.org/doi/10.1109/TMM.2023.3263549">paper</a></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div id="HAPpublications" style="border-left: 16px solid #A0D0A0; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>HAP: Structure-Aware Masked Image Modeling for Human-Centric Perception</i></p>
        <p class="paper_detail"><b><font color=#404040>Junkun Yuan</font></b><sup>&#10035</sup>, Xinyu Zhang<sup>&#10035</sup><sup>&#9993</sup>, Hao Zhou, Jian Wang, Zhongwei Qiu, Zhiyin Shao, Shaofeng Zhang, Sifan Long, and Kun Kuang<sup>&#9993</sup>, Kun Yao, Junyu Han, Errui Ding, Lanfen Lin, Fei Wu, and Jingdong Wang<sup>&#9993</sup></p>
        <p class="paper_detail">Advances in Neural Information Processing Systems (<b><font color=#404040>NeurIPS</font></b>), <b>2023</b></p>
        <p class="paper_detail"><b><font color=#404040>Oct 31, 2023</font></b> &nbsp;&nbsp;<font color=#CCCCCC>|</font>&nbsp;&nbsp; <b><font color=#404040>HAP</font></b></p>
        <p class="paper_detail"><a href="https://arxiv.org/pdf/2310.20695">paper</a> &nbsp;&nbsp;<font color=#CCCCCC>|</font>&nbsp;&nbsp; <a href="https://github.com/junkunyuan/HAP">code</a></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div id="MAPpublications" style="border-left: 16px solid #A0D0A0; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>MAP: Towards Balanced Generalization of IID and OOD through Model-Agnostic Adapters</i></p>
        <p class="paper_detail">Min Zhang, <b><font color=#404040>Junkun Yuan</font></b>, Yue He, Wenbin Li, Zhengyu Chen, and Kun Kuang<sup>&#9993</sup></p>
        <p class="paper_detail">International Conference on Computer Vision (<b><font color=#404040>ICCV</font></b>), <b>2023</b></p>
        <p class="paper_detail"><b><font color=#404040>Oct 02, 2023</font></b> &nbsp;&nbsp;<font color=#CCCCCC>|</font>&nbsp;&nbsp; <b><font color=#404040>MAP</font></b></p>
        <p class="paper_detail"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_MAP_Towards_Balanced_Generalization_of_IID_and_OOD_through_Model-Agnostic_ICCV_2023_paper.pdf">paper</a> &nbsp;&nbsp;<font color=#CCCCCC>|</font>&nbsp;&nbsp; <a href="https://github.com/remiMZ/MAP-ICCV23">code</a></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div id="NPTpublications" style="border-left: 16px solid #A0D0A0; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Neural Collapse Anchored Prompt Tuning for Generalizable Vision-Language Models</i></p>
        <p class="paper_detail">Didi Zhu, Zexi Li, Min Zhang, <b><font color=#404040>Junkun Yuan</font></b>, Yunfeng Shao, Jiashuo Liu, Kun Kuang<sup>&#9993</sup>, Yinchuan Li, and Chao Wu</p>
        <p class="paper_detail">ACM SIGKDD Conference on Knowledge Discovery and Data Mining (<b><font color=#404040>KDD</font></b>), <b>2024</b></p>
        <p class="paper_detail"><b><font color=#404040>Jun 28, 2023</font></b> &nbsp;&nbsp;<font color=#CCCCCC>|</font>&nbsp;&nbsp; <b><font color=#404040>NPT</font></b></p>
        <p class="paper_detail"><a href="https://arxiv.org/pdf/2306.15955">paper</a></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div id="HTCLpublications" style="border-left: 16px solid #A0D0A0; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Quantitatively Measuring and Contrastively Exploring Heterogeneity for Domain Generalization</i></p>
        <p class="paper_detail">Yunze Tong, <b><font color=#404040>Junkun Yuan</font></b>, Min Zhang, Didi Zhu, Keli Zhang, Fei Wu, and Kun Kuang<sup>&#9993</sup></p>
        <p class="paper_detail">ACM SIGKDD Conference on Knowledge Discovery and Data Mining (<b><font color=#404040>KDD</font></b>), <b>2023</b></p>
        <p class="paper_detail"><b><font color=#404040>May 25, 2023</font></b> &nbsp;&nbsp;<font color=#CCCCCC>|</font>&nbsp;&nbsp; <b><font color=#404040>HTCL</font></b></p>
        <p class="paper_detail"><a href="https://arxiv.org/pdf/2305.15889">paper</a> &nbsp;&nbsp;<font color=#CCCCCC>|</font>&nbsp;&nbsp; <a href="https://github.com/YunzeTong/HTCL">code</a></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div id="CAMpublications" style="border-left: 16px solid #A0D0A0; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Universal Domain Adaptation via Compressive Attention Matching</i></p>
        <p class="paper_detail">Didi Zhu<sup>&#10035</sup>, Yincuan Li<sup>&#10035</sup>, <b><font color=#404040>Junkun Yuan</font></b>, Zexi Li, Kun Kuang, and Chao Wu<sup>&#9993</sup></p>
        <p class="paper_detail">International Conference on Computer Vision (<b><font color=#404040>ICCV</font></b>), <b>2023</b></p>
        <p class="paper_detail"><b><font color=#404040>Apr 24, 2023</font></b> &nbsp;&nbsp;<font color=#CCCCCC>|</font>&nbsp;&nbsp; <b><font color=#404040>CAM</font></b></p>
        <p class="paper_detail"><a href="https://arxiv.org/pdf/2304.11862">paper</a></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div id="MPLpublications" style="border-left: 16px solid #A0D0A0; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Task-Oriented Multi-Modal Mutual Leaning for Vision-Language Models</i></p>
        <p class="paper_detail">Sifan Long<sup>&#10035</sup>, Zhen Zhao<sup>&#10035</sup>, <b><font color=#404040>Junkun Yuan</font></b><sup>&#10035</sup>, Zichang Tan, Jiangjiang Liu, Luping Zhou, Shengsheng Wang<sup>&#9993</sup>, and Jingdong Wang<sup>&#9993</sup></p>
        <p class="paper_detail">International Conference on Computer Vision (<b><font color=#404040>ICCV</font></b>), <b>2023</b></p>
        <p class="paper_detail"><b><font color=#404040>Mar 30, 2023</font></b> &nbsp;&nbsp;<font color=#CCCCCC>|</font>&nbsp;&nbsp; <b><font color=#404040>MPL</font></b></p>
        <p class="paper_detail"><a href="https://arxiv.org/pdf/2303.17169">paper</a></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div id="CAE v2publications" style="border-left: 16px solid #F0BBCC; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>CAE v2: Context Autoencoder with CLIP Target</i></p>
        <p class="paper_detail">Xinyu Zhang<sup>&#10035</sup>, Jiahui Chen<sup>&#10035</sup>, <b><font color=#404040>Junkun Yuan</font></b>, Qiang Chen, Jian Wang, Xiaodi Wang, Shumin Han, Xiaokang Chen, Jimin Pi, Kun Yao, Junyu Han, Errui Ding, and Jingdong Wang<sup>&#9993</sup></p>
        <p class="paper_detail">Transactions on Machine Learning Research (<b><font color=#404040>TMLR</font></b>), <b>2023</b></p>
        <p class="paper_detail"><b><font color=#404040>Nov 17, 2022</font></b> &nbsp;&nbsp;<font color=#CCCCCC>|</font>&nbsp;&nbsp; <b><font color=#404040>CAE v2</font></b></p>
        <p class="paper_detail"><a href="https://arxiv.org/pdf/2211.09799">paper</a> &nbsp;&nbsp;<font color=#CCCCCC>|</font>&nbsp;&nbsp; <a href="https://github.com/Atten4Vis/CAE">code</a></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div id="CEGpublications" style="border-left: 16px solid #F0BBCC; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Label-Efficient Domain Generalization via Collaborative Exploration and Generalization</i></p>
        <p class="paper_detail"><b><font color=#404040>Junkun Yuan</font></b><sup>&#10035</sup>, Xu Ma<sup>&#10035</sup>, Defang Chen, Kun Kuang<sup>&#9993</sup>, Fei Wu, and Lanfen Lin</p>
        <p class="paper_detail">International Conference on Multimedia (<b><font color=#404040>MM</font></b>), <b>2022</b></p>
        <p class="paper_detail"><b><font color=#404040>Aug 07, 2022</font></b> &nbsp;&nbsp;<font color=#CCCCCC>|</font>&nbsp;&nbsp; <b><font color=#404040>CEG</font></b></p>
        <p class="paper_detail"><a href="https://arxiv.org/pdf/2208.03644">paper</a> &nbsp;&nbsp;<font color=#CCCCCC>|</font>&nbsp;&nbsp; <a href="https://github.com/junkunyuan/CEG">code</a></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div id="ACDApublications" style="border-left: 16px solid #F0BBCC; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Attention-based Cross-Layer Domain Alignment for Unsupervised Domain Adaptation</i></p>
        <p class="paper_detail">Xu Ma, <b><font color=#404040>Junkun Yuan</font></b>, Yen-Wei Chen, Ruofeng Tong, and Lanfen Lin<sup>&#9993</sup></p>
        <p class="paper_detail"> <b><font color=#404040>Neurocomputing</font></b> <b>2022</b></p>
        <p class="paper_detail"><b><font color=#404040>Feb 27, 2022</font></b> &nbsp;&nbsp;<font color=#CCCCCC>|</font>&nbsp;&nbsp; <b><font color=#404040>ACDA</font></b></p>
        <p class="paper_detail"><a href="https://arxiv.org/pdf/2202.13310">paper</a></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div id="DSBFpublications" style="border-left: 16px solid #9ACAF0; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Domain-Specific Bias Filtering for Single Labeled Domain Generalization</i></p>
        <p class="paper_detail"><b><font color=#404040>Junkun Yuan</font></b><sup>&#10035</sup>, Xu Ma<sup>&#10035</sup>, Defang Chen, Kun Kuang<sup>&#9993</sup>, Fei Wu, and Lanfen Lin</p>
        <p class="paper_detail">International Journal of Computer Vision (<b><font color=#404040>IJCV</font></b>), <b>2022</b></p>
        <p class="paper_detail"><b><font color=#404040>Oct 02, 2021</font></b> &nbsp;&nbsp;<font color=#CCCCCC>|</font>&nbsp;&nbsp; <b><font color=#404040>DSBF</font></b></p>
        <p class="paper_detail"><a href="https://arxiv.org/pdf/2110.00726">paper</a> &nbsp;&nbsp;<font color=#CCCCCC>|</font>&nbsp;&nbsp; <a href="https://github.com/junkunyuan/DSBF">code</a></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div id="CSACpublications" style="border-left: 16px solid #9ACAF0; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Collaborative Semantic Aggregation and Calibration for Federated Domain Generalization</i></p>
        <p class="paper_detail"><b><font color=#404040>Junkun Yuan</font></b><sup>&#10035</sup>, Xu Ma<sup>&#10035</sup>, Defang Chen, Fei Wu, Lanfen Lin, and Kun Kuang<sup>&#9993</sup></p>
        <p class="paper_detail">IEEE Transactions on Knowledge and Data Engineering (<b><font color=#404040>TKDE</font></b>), <b>2023</b></p>
        <p class="paper_detail"><b><font color=#404040>Oct 13, 2021</font></b> &nbsp;&nbsp;<font color=#CCCCCC>|</font>&nbsp;&nbsp; <b><font color=#404040>CSAC</font></b></p>
        <p class="paper_detail"><a href="https://arxiv.org/pdf/2110.06736">paper</a> &nbsp;&nbsp;<font color=#CCCCCC>|</font>&nbsp;&nbsp; <a href="https://github.com/junkunyuan/CSAC">code</a></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div id="DSBFpublications" style="border-left: 16px solid #9ACAF0; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Instrumental Variable-Driven Domain Generalization with Unobserved Confounders</i></p>
        <p class="paper_detail"><b><font color=#404040>Junkun Yuan</font></b>, Xu Ma, Kun Kuang<sup>&#9993</sup>, Ruoxuan Xiong, Mingming Gong, and Lanfen Lin</p>
        <p class="paper_detail">ACM Transactions on Knowledge Discovery from Data (<b><font color=#404040>TKDD</font></b>), <b>2023</b></p>
        <p class="paper_detail"><b><font color=#404040>Oct 04, 2021</font></b> &nbsp;&nbsp;<font color=#CCCCCC>|</font>&nbsp;&nbsp; <b><font color=#404040>DSBF</font></b></p>
        <p class="paper_detail"><a href="https://arxiv.org/pdf/2110.01438">paper</a> &nbsp;&nbsp;<font color=#CCCCCC>|</font>&nbsp;&nbsp; <a href="https://github.com/junkunyuan/DSBF">code</a></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div id="AutoIVpublications" style="border-left: 16px solid #9ACAF0; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Auto IV: Counterfactual Prediction via Automatic Instrumental Variable Decomposition</i></p>
        <p class="paper_detail"><b><font color=#404040>Junkun Yuan</font></b><sup>&#10035</sup>, Anpeng Wu<sup>&#10035</sup>, Kun Kuang<sup>&#9993</sup>, Bo Li, Runze Wu, Fei Wu, and Lanfen Lin</p>
        <p class="paper_detail">ACM Transactions on Knowledge Discovery from Data (<b><font color=#404040>TKDD</font></b>), <b>2022</b></p>
        <p class="paper_detail"><b><font color=#404040>Jul 13, 2021</font></b> &nbsp;&nbsp;<font color=#CCCCCC>|</font>&nbsp;&nbsp; <b><font color=#404040>AutoIV</font></b></p>
        <p class="paper_detail"><a href="https://arxiv.org/pdf/2107.05884">paper</a> &nbsp;&nbsp;<font color=#CCCCCC>|</font>&nbsp;&nbsp; <a href="https://github.com/junkunyuan/AutoIV">code</a></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div id="GAPGANpublications" style="border-left: 16px solid #F9D0A5; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Black-Box Adversarial Attacks Against Deep Learning Based Malware Binaries Detection with GAN</i></p>
        <p class="paper_detail"><b><font color=#404040>Junkun Yuan</font></b>, Shaofang Zhou, Lanfen Lin<sup>&#9993</sup>, Feng Wang, and Jia Cui</p>
        <p class="paper_detail">European Conference on Artificial Intelligence (<b><font color=#404040>ECAI</font></b>), <b>2020</b></p>
        <p class="paper_detail"><b><font color=#404040>Aug 29, 2020</font></b> &nbsp;&nbsp;<font color=#CCCCCC>|</font>&nbsp;&nbsp; <b><font color=#404040>GAPGAN</font></b></p>
        <p class="paper_detail"><a href="https://ebooks.iospress.nl/pdf/doi/10.3233/FAIA200388">paper</a></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div id="DeR-CFRpublications" style="border-left: 16px solid #F9D0A5; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Learning Decomposed Representation for Counterfactual Inference</i></p>
        <p class="paper_detail">Anpeng Wu<sup>&#10035</sup>, <b><font color=#404040>Junkun Yuan</font></b><sup>&#10035</sup>, Kun Kuang<sup>&#9993</sup>, Bo Li<sup>&#9993</sup>, Runze Wu, Qiang Zhu, Yueting Zhuang, and Fei Wu</p>
        <p class="paper_detail">IEEE Transactions on Knowledge and Data Engineering (<b><font color=#404040>TKDE</font></b>), <b>2023</b></p>
        <p class="paper_detail"><b><font color=#404040>Jun 12, 2020</font></b> &nbsp;&nbsp;<font color=#CCCCCC>|</font>&nbsp;&nbsp; <b><font color=#404040>DeR-CFR</font></b></p>
        <p class="paper_detail"><a href="https://arxiv.org/pdf/2006.07040">paper</a> &nbsp;&nbsp;<font color=#CCCCCC>|</font>&nbsp;&nbsp; <a href="https://github.com/anpwu/DeR-CFR">code</a></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div id="SGNspublications" style="border-left: 16px solid #A0D0A0; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Subgraph Networks with Application to Structural Feature Space Expansion</i></p>
        <p class="paper_detail">Qi Xuan<sup>&#9993</sup>, Jinhuan Wang, Minghao Zhao, <b><font color=#404040>Junkun Yuan</font></b>, Chenbo Fu, Zhongyuan Ruan<sup>&#9993</sup>, Guanrong Chen</p>
        <p class="paper_detail">IEEE Transactions on Knowledge and Data Engineering (<b><font color=#404040>TKDE</font></b>), <b>2021</b></p>
        <p class="paper_detail"><b><font color=#404040>Mar 21, 2019</font></b> &nbsp;&nbsp;<font color=#CCCCCC>|</font>&nbsp;&nbsp; <b><font color=#404040>SGNs</font></b></p>
        <p class="paper_detail"><a href="https://arxiv.org/pdf/1903.09022">paper</a> &nbsp;&nbsp;<font color=#CCCCCC>|</font>&nbsp;&nbsp; <a href="https://github.com/GalateaWang/SGNs-master">code</a></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
<br>
<h2>Professional Service</h2>
<ul>
<li> <b>Conference Reviewer.</b>&nbsp;&nbsp; 
ICLR 2026 &nbsp;&nbsp;<font color=#A0A0A0>|</font>&nbsp;&nbsp;
CVPR 2021 &nbsp;&nbsp;<font color=#A0A0A0>|</font>&nbsp;&nbsp;
ICCV 2023 &nbsp;&nbsp;<font color=#A0A0A0>|</font>&nbsp;&nbsp;
AAAI 2023, 2026 &nbsp;&nbsp;<font color=#A0A0A0>|</font>&nbsp;&nbsp;
MM 2023

<li> <b>Journal Reviewer.</b>&nbsp;&nbsp; 
TNNLS 2022 &nbsp;&nbsp;<font color=#A0A0A0>|</font>&nbsp;&nbsp;
TCSVT 2023 &nbsp;&nbsp;<font color=#A0A0A0>|</font>&nbsp;&nbsp;
PR 2026 &nbsp;&nbsp;<font color=#A0A0A0>|</font>&nbsp;&nbsp;
NN 2023 &nbsp;&nbsp;<font color=#A0A0A0>|</font>&nbsp;&nbsp;
TKDD 2023
</ul>

<button id="backToTop" title="back to top">↑</button>
<script>
    const button = document.getElementById("backToTop");
    window.addEventListener("scroll", () => {
        if (document.documentElement.scrollTop > 300) {
            button.style.display = "block";
        } else {
            button.style.display = "none";
        }
    });

    function updateButtonPosition() {
        const bodyRect = document.body.getBoundingClientRect();
        const windowWidth = window.innerWidth;
        const rightOffset = Math.max((windowWidth - bodyRect.width) / 2, 10);
        button.style.right = rightOffset + "px";
    }

    window.addEventListener("resize", updateButtonPosition);
    window.addEventListener("load", updateButtonPosition);

    button.addEventListener("click", () => {
        window.scrollTo({ top: 0, behavior: 'smooth' });
    });
</script>

</body>
</html>
