
<!DOCTYPE html>
<html>
<head>
    <link rel="stylesheet" href="resource/index.css" type="text/css">
    <link rel="shortcut icon" href="resource/my_photo.jpg">
    <title>Junkun Yuan</title>
    <meta name="description" content="Junkun Yuan">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <div id="layout-content" style="margin-top:25px">
    <table>
        <tbody>
            <tr>
                <td width="870">
                    <h1>Junkun Yuan &nbsp; 袁俊坤</h1>
                    <p>Research Scientist, &nbsp;<a href="https://hunyuan.tencent.com/">Hunyuan Multimodal Generation Team</a>&nbsp;&nbsp;@&nbsp;&nbsp;<a href="https://www.tencent.com/">Tencent</a></p>
                    <p>yuanjk0921@outlook.com</p>
                    <p>work and live in Shenzhen, China</p>
                    <p><FONT color=#B0B0B0>Last updated on June 26, 2025 at 20:23 (UTC+8)</p>
                </td>
                <td style="padding-right: 120px;">
                    <img src="resource/my_photo.jpg" width="150">
                </td>
            </tr>
        </tbody>
    </table>
<body>

<h2>Biography</h2>
<p>
  I am a research scientist in <a href="https://hunyuan.tencent.com/">Hunyuan Multimodal Generation Team</a> at <a href="https://www.tencent.com/">Tencent</a>, working on multimodal generative foundation models.
  <br><br>

  I previously worked/interned in <a href="https://hunyuan.tencent.com/">Hunyuan Multimodal Generation Team</a> at <a href="https://www.tencent.com/">Tencent</a> from 2023 to 2025 (working with <a href="https://scholar.google.com/citations?user=AjxoEpIAAAAJ">Wei Liu</a>), and in <a href="http://vis.baidu.com/">Computer Vision Group</a> at <a href="https://home.baidu.com/">Baidu</a> from 2022 to 2023 (working with <a href="https://scholar.google.com/citations?user=PSzJxD8AAAAJ">Xinyu Zhang</a> and <a href="https://scholar.google.com/citations?user=z5SPCmgAAAAJ">Jingdong Wang</a>).<br><br>

  I received my PhD degree from <a href="http://www.zju.edu.cn/">Zhejiang University</a> in 2024, co-supervised by professors of <a href="https://scholar.google.com/citations?user=FOsNiMQAAAAJ">Kun Kuang</a>, <a href="https://person.zju.edu.cn/0096005">Lanfen Lin</a>, and 
  <a href="https://scholar.google.com/citations?user=XJLn4MYAAAAJ">Fei Wu</a>.
</p>

    <p class="little_split"></p>
    <h2>Publications</h2>
    <p><a href="https://scholar.google.com/citations?user=j3iFVPsAAAAJ">Google Scholar Profile</a></p>
    
        <p class="little_split"></p>
        <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
        <p class="pub_title"><i>HunyuanVideo: A Systematic Framework For Large Video Generative Models</i></p>
        <p class="pub_detail">Hunyuan Multimodal Generation Team at Tencent</p>
        <p class="pub_detail">Dec. 2024 &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/Tencent-Hunyuan/HunyuanVideo">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <b><a href="https://arxiv.org/pdf/2412.03603"><FONT color=#202020>arXiv 2024</FONT></a></b> &nbsp; <FONT color=#B0B0B0></FONT></p>
        <p class="pub_detail"><font color=#FF000>The first open-sourced large-scale video generation model with 13B parameters.</p>
        </div>
        
        <p class="little_split"></p>
        <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
        <p class="pub_title"><i>Follow-Your-Canvas: Higher-Resolution Video Outpainting with Extensive Content Generation</i></p>
        <p class="pub_detail">Qihua Chen<sup>&#10035</sup>, Yue Ma<sup>&#10035</sup>, Hongfa Wang<sup>&#10035</sup>, <b><FONT color=#202020>Junkun Yuan</FONT></b><sup>&#10035</sup><sup>&#9993</sup>, Wenzhe Zhao, Qi Tian, Hongmei Wang, Shaobo Min, Qifeng Chen<sup>&#9993</sup>, and Wei Liu</p>
        <p class="pub_detail">Sep. 2024 &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/mayuelala/FollowYourCanvas">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <b><a href="https://arxiv.org/pdf/2409.01055"><FONT color=#202020>AAAI 2025</FONT></a></b> &nbsp; <FONT color=#B0B0B0>AAAI Conference on Artificial Intelligence</FONT></p>
        
        </div>
        
        <p class="little_split"></p>
        <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
        <p class="pub_title"><i>Mutual Prompt Leaning for Vision Language Models</i></p>
        <p class="pub_detail">Sifan Long<sup>&#10035</sup>, Zhen Zhao<sup>&#10035</sup>, <b><FONT color=#202020>Junkun Yuan</FONT></b><sup>&#10035</sup>, Zichang Tan<sup>&#10035</sup>, Jiangjiang Liu, Jingyuan Feng, Shengsheng Wang<sup>&#9993</sup>, and Jingdong Wang</p>
        <p class="pub_detail">Sep. 2024  &nbsp;&nbsp;|&nbsp;&nbsp; <b><a href="https://arxiv.org/pdf/2303.17169"><FONT color=#202020>IJCV 2024</FONT></a></b> &nbsp; <FONT color=#B0B0B0>International Journal of Computer Vision</FONT></p>
        
        </div>
        
        <p class="little_split"></p>
        <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
        <p class="pub_title"><i>Follow-Your-Emoji: Fine-Controllable and Expressive Freestyle Portrait Animation</i></p>
        <p class="pub_detail">Yue Ma<sup>&#10035</sup>, Hongyu Liu<sup>&#10035</sup>, Hongfa Wang<sup>&#10035</sup>, Heng Pan<sup>&#10035</sup>, Yingqing He, <b><FONT color=#202020>Junkun Yuan</FONT></b>, Ailing Zeng, Chengfei Cai, Heung-Yeung Shum, Wei Liu<sup>&#9993</sup>, and Qifeng Chen<sup>&#9993</sup></p>
        <p class="pub_detail">June 2024 &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/mayuelala/FollowYourEmoji">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <b><a href="https://arxiv.org/pdf/2406.01900"><FONT color=#202020>SIGGRAPH-Asia 2024</FONT></a></b> &nbsp; <FONT color=#B0B0B0>ACM SIGGRAPH Annual Conference in Asia</FONT></p>
        
        </div>
        
        <p class="little_split"></p>
        <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
        <p class="pub_title"><i>Hunyuan-DiT: A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding</i></p>
        <p class="pub_detail">Hunyuan Multimodal Generation Team at Tencent (as an intern)</p>
        <p class="pub_detail">May 2024 &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/Tencent/HunyuanDiT">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <b><a href="https://arxiv.org/pdf/2405.08748"><FONT color=#202020>arXiv 2024</FONT></a></b> &nbsp; <FONT color=#B0B0B0></FONT></p>
        
        </div>
        
        <p class="little_split"></p>
        <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
        <p class="pub_title"><i>HAP: Structure-Aware Masked Image Modeling for Human-Centric Perception</i></p>
        <p class="pub_detail"><b><FONT color=#202020>Junkun Yuan</FONT></b><sup>&#10035</sup>, Xinyu Zhang<sup>&#10035</sup><sup>&#9993</sup>, Hao Zhou, Jian Wang, Zhongwei Qiu, Zhiyin Shao, Shaofeng Zhang, Sifan Long, Kun Kuang<sup>&#9993</sup>, Kun Yao, Junyu Han, Errui Ding, Lanfen Lin, Fei Wu, and Jingdong Wang<sup>&#9993</sup></p>
        <p class="pub_detail">Oct. 2023 &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/junkunyuan/HAP">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <b><a href="https://arxiv.org/pdf/2310.20695"><FONT color=#202020>NeurIPS 2023</FONT></a></b> &nbsp; <FONT color=#B0B0B0>Advances in Neural Information Processing Systems</FONT></p>
        
        </div>
        
        <p class="little_split"></p>
        <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
        <p class="pub_title"><i>MAP: Towards Balanced Generalization of IID and OOD through Model-Agnostic Adapters</i></p>
        <p class="pub_detail">Min Zhang, <b><FONT color=#202020>Junkun Yuan</FONT></b>, Yue He, Wenbin Li, Zhengyu Chen, and Kun Kuang<sup>&#9993</sup></p>
        <p class="pub_detail">Oct. 2023  &nbsp;&nbsp;|&nbsp;&nbsp; <b><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_MAP_Towards_Balanced_Generalization_of_IID_and_OOD_through_Model-Agnostic_ICCV_2023_paper.pdf"><FONT color=#202020>ICCV 2023</FONT></a></b> &nbsp; <FONT color=#B0B0B0>International Conference on Computer Vision</FONT></p>
        
        </div>
        
        <p class="little_split"></p>
        <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
        <p class="pub_title"><i>Neural Collapse Anchored Prompt Tuning for Generalizable Vision-Language Models</i></p>
        <p class="pub_detail">Didi Zhu, Zexi Li, Min Zhang, <b><FONT color=#202020>Junkun Yuan</FONT></b>, Yunfeng Shao, Jiashuo Liu, Kun Kuang<sup>&#9993</sup>, Yinchuan Li, and Chao Wu</p>
        <p class="pub_detail">June. 2023  &nbsp;&nbsp;|&nbsp;&nbsp; <b><a href="https://arxiv.org/pdf/2306.15955"><FONT color=#202020>KDD 2024</FONT></a></b> &nbsp; <FONT color=#B0B0B0>ACM SIGKDD Conference on Knowledge Discovery and Data Mining</FONT></p>
        
        </div>
        
        <p class="little_split"></p>
        <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
        <p class="pub_title"><i>Quantitatively Measuring and Contrastively Exploring Heterogeneity for Domain Generalization</i></p>
        <p class="pub_detail">Yunze Tong, <b><FONT color=#202020>Junkun Yuan</FONT></b>, Min Zhang, Didi Zhu, Keli Zhang, Fei Wu, and Kun Kuang<sup>&#9993</sup></p>
        <p class="pub_detail">May 2023 &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/YunzeTong/HTCL">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <b><a href="https://arxiv.org/pdf/2305.15889"><FONT color=#202020>KDD 2023</FONT></a></b> &nbsp; <FONT color=#B0B0B0>ACM SIGKDD Conference on Knowledge Discovery and Data Mining</FONT></p>
        
        </div>
        
        <p class="little_split"></p>
        <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
        <p class="pub_title"><i>Universal Domain Adaptation via Compressive Attention Matching</i></p>
        <p class="pub_detail">Didi Zhu<sup>&#10035</sup>, Yincuan Li<sup>&#10035</sup>, <b><FONT color=#202020>Junkun Yuan</FONT></b>, Zexi Li, Kun Kuang, and Chao Wu<sup>&#9993</sup></p>
        <p class="pub_detail">Apr. 2023  &nbsp;&nbsp;|&nbsp;&nbsp; <b><a href="https://arxiv.org/pdf/2304.11862"><FONT color=#202020>ICCV 2023</FONT></a></b> &nbsp; <FONT color=#B0B0B0>International Conference on Computer Vision</FONT></p>
        
        </div>
        
        <p class="little_split"></p>
        <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
        <p class="pub_title"><i>Task-Oriented Multi-Modal Mutual Leaning for Vision-Language Models</i></p>
        <p class="pub_detail">Sifan Long<sup>&#10035</sup>, Zhen Zhao<sup>&#10035</sup>, <b><FONT color=#202020>Junkun Yuan</FONT></b><sup>&#10035</sup>, Zichang Tan, Jiangjiang Liu, Luping Zhou, Shengsheng Wang<sup>&#9993</sup>, and Jingdong Wang<sup>&#9993</sup></p>
        <p class="pub_detail">Mar. 2023  &nbsp;&nbsp;|&nbsp;&nbsp; <b><a href="https://arxiv.org/pdf/2303.17169"><FONT color=#202020>ICCV 2023</FONT></a></b> &nbsp; <FONT color=#B0B0B0>International Conference on Computer Vision</FONT></p>
        
        </div>
        
        <p class="little_split"></p>
        <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
        <p class="pub_title"><i>CAE v2: Context Autoencoder with CLIP Target</i></p>
        <p class="pub_detail">Xinyu Zhang<sup>&#10035</sup>, Jiahui Chen<sup>&#10035</sup>, <b><FONT color=#202020>Junkun Yuan</FONT></b>, Qiang Chen, Jian Wang, Xiaodi Wang, Shumin Han, Xiaokang Chen, Jimin Pi, Kun Yao, Junyu Han, Errui Ding, and Jingdong Wang<sup>&#9993</sup></p>
        <p class="pub_detail">Nov. 2022 &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/Atten4Vis/CAE">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <b><a href="https://arxiv.org/pdf/2211.09799"><FONT color=#202020>TMLR 2023</FONT></a></b> &nbsp; <FONT color=#B0B0B0>Transactions on Machine Learning Research</FONT></p>
        
        </div>
        
        <p class="little_split"></p>
        <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
        <p class="pub_title"><i>Label-Efficient Domain Generalization via Collaborative Exploration and Generalization</i></p>
        <p class="pub_detail"><b><FONT color=#202020>Junkun Yuan</FONT></b><sup>&#10035</sup>, Xu Ma<sup>&#10035</sup>, Defang Chen, Kun Kuang<sup>&#9993</sup>, Fei Wu, and Lanfen Lin</p>
        <p class="pub_detail">Aug. 2022 &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/junkunyuan/CEG">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <b><a href="https://arxiv.org/pdf/2208.03644"><FONT color=#202020>MM 2022</FONT></a></b> &nbsp; <FONT color=#B0B0B0>International Conference on Multimedia</FONT></p>
        
        </div>
        
        <p class="little_split"></p>
        <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
        <p class="pub_title"><i>Domain-Specific Bias Filtering for Single Labeled Domain Generalization</i></p>
        <p class="pub_detail"><b><FONT color=#202020>Junkun Yuan</FONT></b><sup>&#10035</sup>, Xu Ma<sup>&#10035</sup>, Defang Chen, Kun Kuang<sup>&#9993</sup>, Fei Wu, and Lanfen Lin</p>
        <p class="pub_detail">Oc. 2021 &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/junkunyuan/DSBF">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <b><a href="https://arxiv.org/pdf/2110.00726"><FONT color=#202020>IJCV 2022</FONT></a></b> &nbsp; <FONT color=#B0B0B0>International Journal of Computer Vision</FONT></p>
        
        </div>
        
        <p class="little_split"></p>
        <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
        <p class="pub_title"><i>Collaborative Semantic Aggregation and Calibration for Federated Domain Generalization</i></p>
        <p class="pub_detail"><b><FONT color=#202020>Junkun Yuan</FONT></b><sup>&#10035</sup>, Xu Ma<sup>&#10035</sup>, Defang Chen, Fei Wu, Lanfen Lin, and Kun Kuang<sup>&#9993</sup></p>
        <p class="pub_detail">Oct. 2021 &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/junkunyuan/CSAC">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <b><a href="https://arxiv.org/pdf/2110.06736"><FONT color=#202020>TKDE 2023</FONT></a></b> &nbsp; <FONT color=#B0B0B0>IEEE Transactions on Knowledge and Data Engineering</FONT></p>
        
        </div>
        
</body>
</html>
