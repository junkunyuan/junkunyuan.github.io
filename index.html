
<!DOCTYPE html>
<html>
<head>
    <link rel="stylesheet" href="resource/index.css" type="text/css">
    <link rel="shortcut icon" href="resource/my_photo.jpg">
    <title>Junkun Yuan</title>
    <meta name="description" content="Junkun Yuan">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <div id="layout-content" style="margin-top:25px">
    <table>
        <tbody>
            <tr>
                <td width="870">
                    <h1>Junkun Yuan &nbsp; 袁俊坤</h1>
                    <p>Research Scientist, &nbsp;<a href="https://hunyuan.tencent.com/">Hunyuan Multimodal Generation Team</a>&nbsp;&nbsp;@&nbsp;&nbsp;<a href="https://www.tencent.com/">Tencent</a></p>
                    <p>yuanjk0921@outlook.com</p>
                    <p>work and live in Shenzhen, China</p>
                    <p><font color=#B0B0B0>Last updated on June 29, 2025 at 14:08 (UTC+8)</font></p>
                </td>
                <td style="padding-right: 120px; padding-top: 10px;">
                    <img src="resource/my_photo.jpg" width="160">
                </td>
            </tr>
        </tbody>
    </table>
<body>

<h2>Biography</h2>
<p>
  I am a research scientist in <a href="https://hunyuan.tencent.com/">Hunyuan Multimodal Generation Team</a> at <a href="https://www.tencent.com/">Tencent</a>, working on multimodal generative foundation models.
  <br><br>

  I previously worked/interned in <a href="https://hunyuan.tencent.com/">Hunyuan Multimodal Generation Team</a> at <a href="https://www.tencent.com/">Tencent</a> from 2023 to 2025 (working with <a href="https://scholar.google.com/citations?user=AjxoEpIAAAAJ">Wei Liu</a>), and in <a href="http://vis.baidu.com/">Computer Vision Group</a> at <a href="https://home.baidu.com/">Baidu</a> from 2022 to 2023 (working with <a href="https://scholar.google.com/citations?user=PSzJxD8AAAAJ">Xinyu Zhang</a> and <a href="https://scholar.google.com/citations?user=z5SPCmgAAAAJ">Jingdong Wang</a>).<br><br>

  I received my PhD degree from <a href="http://www.zju.edu.cn/">Zhejiang University</a> in 2024, co-supervised by professors of <a href="https://scholar.google.com/citations?user=FOsNiMQAAAAJ">Kun Kuang</a>, <a href="https://person.zju.edu.cn/0096005">Lanfen Lin</a>, and 
  <a href="https://scholar.google.com/citations?user=XJLn4MYAAAAJ">Fei Wu</a>.<br><br>
</p>

    <h2>Publications</h2>
    <p><a href="https://scholar.google.com/citations?user=j3iFVPsAAAAJ">Google Scholar Profile</a></p>
    
        <p class="little_split"></p>
        <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Hunyuan-Game: Industrial-grade Intelligent Game Creation Model</i></p>
        <p class="paper_detail">Hunyuan Multimodal Generation Team at Tencent (<b><font color=#202020>as a team member</font></b>)</p>
        <p class="paper_detail">May 20, 2025  &nbsp;&nbsp;|&nbsp;&nbsp; <b><a href="https://arxiv.org/pdf/2505.14135"><font color=#202020>arXiv 2025</font></a></b> &nbsp; <font color=#B0B0B0></font></p>
        <p class="paper_detail"><font color=#FF000></font></p>
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div style="border-left: 8px solid #FFD6AD; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>HunyuanVideo: A Systematic Framework For Large Video Generative Models</i></p>
        <p class="paper_detail">Hunyuan Multimodal Generation Team at Tencent (<b><font color=#202020>as a team member</font></b>)</p>
        <p class="paper_detail">Dec 03, 2024 &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/Tencent-Hunyuan/HunyuanVideo">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <b><a href="https://arxiv.org/pdf/2412.03603"><font color=#202020>arXiv 2024</font></a></b> &nbsp; <font color=#B0B0B0></font></p>
        <p class="paper_detail"><font color=#FF000>It is the first open-sourced large-scale video generation model with 13B parameters. It has 200+ citations and 10K+ GitHub stars (as of June 2025).</font></p>
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div style="border-left: 8px solid #FFD6AD; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Follow-Your-Canvas: Higher-Resolution Video Outpainting with Extensive Content Generation</i></p>
        <p class="paper_detail">Qihua Chen<sup>&#10035</sup>, Yue Ma<sup>&#10035</sup>, Hongfa Wang<sup>&#10035</sup>, <b><font color=#202020>Junkun Yuan</font></b><sup>&#10035</sup><sup>&#9993</sup>, Wenzhe Zhao, Qi Tian, Hongmei Wang, Shaobo Min, Qifeng Chen<sup>&#9993</sup>, and Wei Liu</p>
        <p class="paper_detail">Sep 02, 2024 &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/mayuelala/FollowYourCanvas">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <b><a href="https://arxiv.org/pdf/2409.01055"><font color=#202020>AAAI 2025</font></a></b> &nbsp; <font color=#B0B0B0>AAAI Conference on Artificial Intelligence</font></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div style="border-left: 8px solid #FFD6AD; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Follow-Your-Emoji: Fine-Controllable and Expressive Freestyle Portrait Animation</i></p>
        <p class="paper_detail">Yue Ma<sup>&#10035</sup>, Hongyu Liu<sup>&#10035</sup>, Hongfa Wang<sup>&#10035</sup>, Heng Pan<sup>&#10035</sup>, Yingqing He, <b><font color=#202020>Junkun Yuan</font></b>, Ailing Zeng, Chengfei Cai, Heung-Yeung Shum, Wei Liu<sup>&#9993</sup>, and Qifeng Chen<sup>&#9993</sup></p>
        <p class="paper_detail">Jun 04, 2024 &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/mayuelala/FollowYourEmoji">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <b><a href="https://arxiv.org/pdf/2406.01900"><font color=#202020>SIGGRAPH-Asia 2024</font></a></b> &nbsp; <font color=#B0B0B0>ACM SIGGRAPH Annual Conference in Asia</font></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div style="border-left: 8px solid #FFD6AD; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Hunyuan-DiT: A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding</i></p>
        <p class="paper_detail">Hunyuan Multimodal Generation Team at Tencent (<b><font color=#202020>as an intern</font></b>)</p>
        <p class="paper_detail">May 14, 2024 &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/Tencent/HunyuanDiT">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <b><a href="https://arxiv.org/pdf/2405.08748"><font color=#202020>arXiv 2024</font></a></b> &nbsp; <font color=#B0B0B0></font></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div style="border-left: 8px solid #B2EEC8; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>HAP: Structure-Aware Masked Image Modeling for Human-Centric Perception</i></p>
        <p class="paper_detail"><b><font color=#202020>Junkun Yuan</font></b><sup>&#10035</sup>, Xinyu Zhang<sup>&#10035</sup><sup>&#9993</sup>, Hao Zhou, Jian Wang, Zhongwei Qiu, Zhiyin Shao, Shaofeng Zhang, Sifan Long, Kun Kuang<sup>&#9993</sup>, Kun Yao, Junyu Han, Errui Ding, Lanfen Lin, Fei Wu, and Jingdong Wang<sup>&#9993</sup></p>
        <p class="paper_detail">Oct 31, 2023 &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/junkunyuan/HAP">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <b><a href="https://arxiv.org/pdf/2310.20695"><font color=#202020>NeurIPS 2023</font></a></b> &nbsp; <font color=#B0B0B0>Advances in Neural Information Processing Systems</font></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div style="border-left: 8px solid #B2EEC8; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>MAP: Towards Balanced Generalization of IID and OOD through Model-Agnostic Adapters</i></p>
        <p class="paper_detail">Min Zhang, <b><font color=#202020>Junkun Yuan</font></b>, Yue He, Wenbin Li, Zhengyu Chen, and Kun Kuang<sup>&#9993</sup></p>
        <p class="paper_detail">Oct 02, 2023  &nbsp;&nbsp;|&nbsp;&nbsp; <b><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_MAP_Towards_Balanced_Generalization_of_IID_and_OOD_through_Model-Agnostic_ICCV_2023_paper.pdf"><font color=#202020>ICCV 2023</font></a></b> &nbsp; <font color=#B0B0B0>International Conference on Computer Vision</font></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div style="border-left: 8px solid #B2EEC8; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Neural Collapse Anchored Prompt Tuning for Generalizable Vision-Language Models</i></p>
        <p class="paper_detail">Didi Zhu, Zexi Li, Min Zhang, <b><font color=#202020>Junkun Yuan</font></b>, Yunfeng Shao, Jiashuo Liu, Kun Kuang<sup>&#9993</sup>, Yinchuan Li, and Chao Wu</p>
        <p class="paper_detail">Jun 28, 2023  &nbsp;&nbsp;|&nbsp;&nbsp; <b><a href="https://arxiv.org/pdf/2306.15955"><font color=#202020>KDD 2024</font></a></b> &nbsp; <font color=#B0B0B0>ACM SIGKDD Conference on Knowledge Discovery and Data Mining</font></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div style="border-left: 8px solid #B2EEC8; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Quantitatively Measuring and Contrastively Exploring Heterogeneity for Domain Generalization</i></p>
        <p class="paper_detail">Yunze Tong, <b><font color=#202020>Junkun Yuan</font></b>, Min Zhang, Didi Zhu, Keli Zhang, Fei Wu, and Kun Kuang<sup>&#9993</sup></p>
        <p class="paper_detail">May 25, 2023 &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/YunzeTong/HTCL">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <b><a href="https://arxiv.org/pdf/2305.15889"><font color=#202020>KDD 2023</font></a></b> &nbsp; <font color=#B0B0B0>ACM SIGKDD Conference on Knowledge Discovery and Data Mining</font></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div style="border-left: 8px solid #B2EEC8; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Universal Domain Adaptation via Compressive Attention Matching</i></p>
        <p class="paper_detail">Didi Zhu<sup>&#10035</sup>, Yincuan Li<sup>&#10035</sup>, <b><font color=#202020>Junkun Yuan</font></b>, Zexi Li, Kun Kuang, and Chao Wu<sup>&#9993</sup></p>
        <p class="paper_detail">Apr 24, 2023  &nbsp;&nbsp;|&nbsp;&nbsp; <b><a href="https://arxiv.org/pdf/2304.11862"><font color=#202020>ICCV 2023</font></a></b> &nbsp; <font color=#B0B0B0>International Conference on Computer Vision</font></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div style="border-left: 8px solid #B2EEC8; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Mutual Prompt Leaning for Vision Language Models</i></p>
        <p class="paper_detail">Sifan Long<sup>&#10035</sup>, Zhen Zhao<sup>&#10035</sup>, <b><font color=#202020>Junkun Yuan</font></b><sup>&#10035</sup>, Zichang Tan<sup>&#10035</sup>, Jiangjiang Liu, Jingyuan Feng, Shengsheng Wang<sup>&#9993</sup>, and Jingdong Wang</p>
        <p class="paper_detail">Mar 30, 2023  &nbsp;&nbsp;|&nbsp;&nbsp; <b><a href="https://arxiv.org/pdf/2303.17169"><font color=#202020>IJCV 2024</font></a></b> &nbsp; <font color=#B0B0B0>International Journal of Computer Vision</font></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div style="border-left: 8px solid #B2EEC8; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Task-Oriented Multi-Modal Mutual Leaning for Vision-Language Models</i></p>
        <p class="paper_detail">Sifan Long<sup>&#10035</sup>, Zhen Zhao<sup>&#10035</sup>, <b><font color=#202020>Junkun Yuan</font></b><sup>&#10035</sup>, Zichang Tan, Jiangjiang Liu, Luping Zhou, Shengsheng Wang<sup>&#9993</sup>, and Jingdong Wang<sup>&#9993</sup></p>
        <p class="paper_detail">Mar 30, 2023  &nbsp;&nbsp;|&nbsp;&nbsp; <b><a href="https://arxiv.org/pdf/2303.17169"><font color=#202020>ICCV 2023</font></a></b> &nbsp; <font color=#B0B0B0>International Conference on Computer Vision</font></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div style="border-left: 8px solid #FFBBCC; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>CAE v2: Context Autoencoder with CLIP Target</i></p>
        <p class="paper_detail">Xinyu Zhang<sup>&#10035</sup>, Jiahui Chen<sup>&#10035</sup>, <b><font color=#202020>Junkun Yuan</font></b>, Qiang Chen, Jian Wang, Xiaodi Wang, Shumin Han, Xiaokang Chen, Jimin Pi, Kun Yao, Junyu Han, Errui Ding, and Jingdong Wang<sup>&#9993</sup></p>
        <p class="paper_detail">Nov 17, 2022 &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/Atten4Vis/CAE">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <b><a href="https://arxiv.org/pdf/2211.09799"><font color=#202020>TMLR 2023</font></a></b> &nbsp; <font color=#B0B0B0>Transactions on Machine Learning Research</font></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div style="border-left: 8px solid #FFBBCC; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Label-Efficient Domain Generalization via Collaborative Exploration and Generalization</i></p>
        <p class="paper_detail"><b><font color=#202020>Junkun Yuan</font></b><sup>&#10035</sup>, Xu Ma<sup>&#10035</sup>, Defang Chen, Kun Kuang<sup>&#9993</sup>, Fei Wu, and Lanfen Lin</p>
        <p class="paper_detail">Aug 07, 2022 &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/junkunyuan/CEG">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <b><a href="https://arxiv.org/pdf/2208.03644"><font color=#202020>MM 2022</font></a></b> &nbsp; <font color=#B0B0B0>International Conference on Multimedia</font></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Domain-Specific Bias Filtering for Single Labeled Domain Generalization</i></p>
        <p class="paper_detail"><b><font color=#202020>Junkun Yuan</font></b><sup>&#10035</sup>, Xu Ma<sup>&#10035</sup>, Defang Chen, Kun Kuang<sup>&#9993</sup>, Fei Wu, and Lanfen Lin</p>
        <p class="paper_detail">Oct 02, 2021 &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/junkunyuan/DSBF">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <b><a href="https://arxiv.org/pdf/2110.00726"><font color=#202020>IJCV 2022</font></a></b> &nbsp; <font color=#B0B0B0>International Journal of Computer Vision</font></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
        <p class="little_split"></p>
        <div style="border-left: 8px solid #ADDEFF; padding-left: 10px">
        <div style="height: 0.3em;"></div>
        <p class="paper_title"><i>Collaborative Semantic Aggregation and Calibration for Federated Domain Generalization</i></p>
        <p class="paper_detail"><b><font color=#202020>Junkun Yuan</font></b><sup>&#10035</sup>, Xu Ma<sup>&#10035</sup>, Defang Chen, Fei Wu, Lanfen Lin, and Kun Kuang<sup>&#9993</sup></p>
        <p class="paper_detail">Oct 13, 2021 &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://github.com/junkunyuan/CSAC">code</a> &nbsp;&nbsp;|&nbsp;&nbsp; <b><a href="https://arxiv.org/pdf/2110.06736"><font color=#202020>TKDE 2023</font></a></b> &nbsp; <font color=#B0B0B0>IEEE Transactions on Knowledge and Data Engineering</font></p>
        
        <div style="height: 0.05em;"></div>
        </div>
        <p class="little_split"></p>
        
</body>
</html>
